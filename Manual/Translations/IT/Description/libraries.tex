\chapter{Librerie}\label{HOLlibraries}

% LaTeX macros in HOL manuals
%
% \holtxt{..}     for typewriter text that is HOL types or terms.  To
%                 produce backslashes, for /\, \/ and \x. x + 1, use \bs
% \ml{..}         for typewriter text that is ML input, including the
%                 names of HOL API functions, such as mk_const
% \theoryimp{..}  for names of HOL theories.

% text inside \begin{verbatim} should be indented three spaces, unless
% the verbatim is in turn inside a \begin{session}, in which case it
% should be flush with the left margin.


\newcommand{\simpset}{simpset}
\newcommand{\Simpset}{Simpset}

 \newcommand{\term}      {\mbox{\it term}}
 \newcommand{\vstr}      {\mbox{\it vstr}}

Una \emph{libreria} è un'astrazione intesa fornire un livello più alto
di organizzazione per applicazioni \HOL{}. In generale, una libreria può
contenere una collezione di teorie, procedure di dimostrazione, e materiale
di supporto, come la documentazione. Alcune librerie forniscono semplicemente
procedure di dimostrazione, come \ml{simpLib}, mentre altre forniscono teorie e
procedure di dimostrazioni, tale che \ml{intLib}. Le librerie possono includere altre
librerie.

Nel sistema \HOL{}, le librerie sono tipicamente rappresentate da strutture
\ML{} nominate seguendo la convenzione che la libreria \emph{x}
si troverà nella struttura \ML{} \ml{xLib}. Caricare questa struttura
dovrebbe caricare tutte le sotto-componenti rilevanti della libreria e settare
qualunque parametro di sistema che sia appropriato per l'uso della libreria.

Quando il sistema \HOL{} è invocato nella sua configurazione normale, alcune
utili librerie sono caricate automaticamente. La libreria \HOL{}
più di base è \ml{boolLib}, che supporta le definizioni della logica
\HOL{}, che si trova nella teoria \theoryimp{bool}, e fornisce un'utile
suite di strumenti di definizione e ragionamento.

Un'altra libreria usata in modo pervasivo si trova nella struttura \ml{Parse}
(il lettore può vedere che non siamo strettamente fedeli alla nostra
convenzione circa le denominazioni delle librerie). La libreria parser fornisce supporto
per il parsing e il `pretty-printing' dei tipi, i termini, e
i teoremi \HOL{}.

La libreria \ml{boss} fornisce una collezione base di teorie
standard e di procedure di dimostrazione di alto livello, e serve come una piattaforma
standard su cui lavorare. Essa è pre-caricata e aperta quando il sistema
\HOL{} si avvia. Essa include \ml{boolLib} e
\ml{Parse}. Le teorie fornite includono \theoryimp{pair},
\theoryimp{sum}, \theoryimp{option}; le teorie aritmetiche
\theoryimp{num}, \theoryimp{prim\_rec}, \theoryimp{arithmetic},
e \theoryimp{numeral}; e \theoryimp{list}. Altre librerie
incluse in \ml{bossLib} sono \ml{goalstackLib}, che fornisce
un gestore di dimostrazione per dimostrazioni basate su tattiche; \ml{simpLib}, che fornisce
una varietà di semplificatori; \ml{numLib}, che fornisce una procedura
di decisione per l'aritmetica; \ml{Datatype}, che fornisce
supporto di alto-livello per definire tipi di dato algebrici; e \ml{tflLib},
che fornisce supporto per definire funzioni ricorsive.


\section{Parsing e Prettyprinting}
\label{sec:parsing-printing}

Ogni tipo e termine in \HOL{} è in definitiva costruito per applicazione dei
costruttori (astratti) primitivi per i tipi e i termini. Tuttavia, al
fine di ospitare un'ampia varietà di espressioni matematiche, \HOL{}
fornisce un'infrastruttura flessibile per il parsing e il prettyprinting dei tipi
e dei termini attraverso la struttura \ml{Parse}.

Il parser dei termini supporta l'inferenza di tipo, l'overloading, i binders, e
varie dichiarazioni di fixity (infisso, prefisso, suffisso, e
combinazioni). Ci sono anche dei flag per controllare il comportamento
del parser. Inoltre, la struttura del parser è esposta così che
possano essere costruiti rapidamente nuovi parser per supportare applicazioni utente.

Il parser è parametrizzato da grammatiche per tipi e termini. Il
comportamento del parser e del prettyprinter di conseguenza è di solito alterato
da manipolazioni di grammatica.
%
\index{parsing, della logica HOL@parsing, della logica \HOL{}!grammatiche per}
%
Queste possono essere di due generi: \emph{temporanee} o \emph{permanenti}.
I cambiamenti temporanei dovrebbero essere usati nelle implementazioni di librerie, o nei
file di script per quei cambiamenti che l'utente non vuole far
persistere nelle teorie discendenti da quella attuale. I cambiamenti permanenti
sono appropriati per l'uso in file di script, e saranno forzati in tutte
le teorie discendenti. Le funzioni che fanno cambiamenti temporanei sono denotate
da un prefisso \ml{temp\_} nei loro nomi.

\subsection{Parsing dei tipi}
\index{types, nella logica HOL@types, nella logica \HOL{}!parsing of|(}

Il linguaggio dei tipi è semplice. Una grammatica astratta per il
linguaggio è presentata nella Figura~\ref{fig:abstract-type-grammar}. La
grammatica attuale (con i valori concreti per i simboli infissi e gli operatori
di tipo) può essere ispezionata usando la funzione \ml{type\_grammar}.
\begin{figure}[tbhp]
\newcommand{\nt}[1]{\mathit{#1}}
\newcommand{\tok}[1]{\texttt{\bfseries #1}}
\renewcommand{\bar}{\;\;|\;\;}
\[
\begin{array}{lcl}
\tau &::=& \tau \odot \tau \bar \nt{vtype} \bar \nt{tyop} \bar
           \tok{(} \;\nt{tylist}\;\tok{)} \;\nt{tyop}\bar \tau \;\nt{tyop}
           \bar \tok{(}\;\tau\;\tok{)} \bar \tau\tok{[}\tau\tok{]}\\
\odot &::=& \tok{->} \bar \tok{\#} \bar \tok{+} \bar \cdots\\
\nt{vtype} &::=& \tok{'a} \bar \tok{'b} \bar \tok{'c} \bar \cdots\\
\nt{tylist} &::=& \tau \bar \tau \;\tok{,}\;\nt{tylist}\\
\nt{tyop} &::=& \tok{bool} \bar \tok{list} \bar \tok{num} \bar
           \tok{fun} \bar \cdots
\end{array}
\]
\caption{Una grammatica astratta per i tipi \HOL{} ($\tau$).  Gli infissi ($\odot$)
  legano sempre più debolmente dei gli operatori di tipo~($\nt{tyop}$) (e
  e dei sottoscritti di tipo~($\tau\tok{[}\tau\tok{]}$)), così che
  $\tau_1 \,\odot\, \tau_2 \,\nt{tyop}$ è sempre parsato come $\tau_1\, \odot\,
  (\tau_2 \,\nt{tyop})$.  Infissi differenti possono avere priorità
	differenti, e gli infissi a diversi livelli di priorità possono associare
	in modo differente (alla sinistra, alla destra, o non associare del tutto). Gli utenti possono
	estendere le categorie $\odot$ e $\nt{tyop}$ facendo nuove definizioni
	di tipo, e manipolando la grammatica direttamente.}
\label{fig:abstract-type-grammar}
\end{figure}

\paragraph{Infissi di tipo}
Gli infissi possono essere introdotti con la funzione \ml{add\_infix\_type}.
Questa imposta un mapping da un simbolo infisso (come \texttt{->}) al
nome di un operatore di tipo esistente (come \texttt{fun}). E' necessario
dare al simbolo binario un livello di precedenza e
un'associatività. Si veda \REFERENCE{} per maggiori dettagli.

\paragraph{Abbreviazioni di tipo}
\index{abbreviazioni di tipo}

Gli utenti possono abbreviare pattern di tipo comuni con delle \emph{abbreviazioni}.
Questo è fatto con la funzione \ML{} \ml{type\_abbrev}:
\begin{hol}
\begin{verbatim}
   type_abbrev : string * hol_type -> unit
\end{verbatim}
\end{hol}
Un'abbreviazione è un nuovo operatore di tipo, di qualsiasi numero di argomenti,
che espande in un tipo esistente. Per esempio, si potrebbe sviluppare una
teoria leggera di numeri estesi con un infinito, in cui
tipo rappresentante fosse \holtxt{num option} (\holtxt{NONE}
rappresenterebbe il valore infinito). Si potrebbe impostare un'abbreviazione
\holtxt{infnum} che si espanderebbe a questo tipo sottostante.
Sono supportati anche i pattern polimorfici. Per esempio, come descritto
nella Sezione~\ref{sec:theory-of-sets}, l'abbreviazione \holtxt{set}, di
un unico argomento, è tale che \holtxt{:'a set} espande nel tipo
\holtxt{:'a -> bool}, per qualsiasi tipo \holtxt{:'a}.

Quando i tipi devono essere stampati, l'espansione delle abbreviazioni fatta dal parser è invertita.
Per maggiori informazioni, si veda la voce \ml{type\_abbrev} in \REFERENCE.
\index{tipi, nella logica HOL@tipi, nella logica \HOL{}!parsing dei|)}

\subsection{Parsing dei termini}

Il parser dei termini fornisce un'infrastruttura basata sulla grammatica per supportare
la sintassi concreta per le formalizzazioni. Di solito, la grammatica \HOL{} viene
estesa quando viene fatta una nuova definizione o una specifica di costante. (L'introduzione
di una nuova costante è discussa
nelle Sezioni~\ref{sec:constant-definitions} e \ref{conspec}.) Tuttavia,
qualsiasi identificatore può avere assegnato ad esso in ogni momento uno stato di parsing.
In quello che segue, esploriamo alcune delle capacità del
parser dei termini \HOL{}.


\subsubsection{Architettura del parser}
\label{sec:parser:architecture}

Il parser trasforma le stringhe in termini. Fa questo nella seguente
serie di fasi, ciascuna delle quali è influenzata dalla grammatica fornita.
Di solito questa grammatica è la grammatica globale di default, ma gli utenti possono
organizzarsi per usare grammatiche differenti se lo desiderano. %
\index{parsing, della logica HOL@parsing, della logica \HOL{}!grammatiche per}
%
Correttamente, il parsing avviene dopo che il lexing ha diviso l'input in una
serie di token. Per maggiori informazioni sul lexing, si veda la Sezione~\ref{HOL-lex}.
\begin{description}
\item[Sintassi Concreta:] Caratteristiche come gli infissi, i binder e le forme
	mix-fix sono tradotte, creando una forma di ``sintassi
	astratta'' intermedia (tipo ML \ml{Absyn}). Le fixity possibili sono
	discusse nella Sezione~\ref{sec:parseprint:fixities} di sotto. Le forme
	di sintassi concreta sono aggiunte alla grammatica con funzioni come
	\ml{add\_rule} e \ml{set\_fixity} (per le quali, si veda \REFERENCE).
	L'azione di questa fase di parsing è incarnata nella funzione
	\ml{Absyn}.

 Il data type \ml{Absyn} è costruito usando i costruttori \ml{AQ}
	(un antiquote, si veda la Sezione~\ref{sec:quotation-antiquotation}); %
%
  \index{antiquotation, nei termini della logica HOL@antiquotation, nei termini della logica \HOL{}}%
%
  \ml{IDENT} (un identificatore); \ml{QIDENT} (un identificatore qualificato,
	dato come \holtxt{thy\$ident}); \ml{APP} (un'applicazione di una forma
	a un'altra); \ml{LA} (un'astrazione di una variabile su un corpo),
	e \ml{TYPED} (una forma accompagnata da un vincolo di tipo\footnote{I
		tipi nei vincoli \ml{Absyn} non sono tipi HOL completi, ma valori
		da un'altro tipo intermedio \ml{Pretype}.}, si veda
	la Sezione~\ref{sec:parseprint-type-constraints}). A questo stadio della
	traduzione, non viene fatta alcuna distinzione tra costanti e
	variabili: benché le forme \ml{QIDENT} debbano essere delle costanti, gli utente sono
	anche in grado di riferirsi alle costanti dando loro dei nomi nudi.

  E' possibile che i nomi che occorrono nel valore \ml{Absyn} siano
	differenti da qualsiasi token che appariva nell'input
	originale. Per esempio, l'input
\begin{verbatim}
   ``if P then Q else R``
\end{verbatim} si trasformerà in
\begin{verbatim}
   APP (APP (APP (IDENT "COND", IDENT "P"), IDENT "Q"), IDENT "R")
\end{verbatim}
  (Questo è un output leggermente semplificato: i vari costruttori per
	\ml{Absyn}, incluso \ml{APP}, prendono anche parametri di localizzazione.)

  La grammatica standard include una regola che associa la speciale
	forma miz-fix per l'espressioni if-then-else con il sottostante
	``nome'' \holtxt{COND}. E' \holtxt{COND}. che sarà alla fine
	risolto come la costante \holtxt{bool\dol{}COND}.

  Se si usa la sintassi di ``quotation'' con un dollaro nudo,%
%
\index{ escape, nel parser della logica HOL@\ml{\$} (escape, nel parser della logica \HOL{})}%
\index{token!sopprimere il comportamento di parsing dei|(}
%
allora questa fase del parser non tratterà le stringhe come parte di una
forma speciale. Per esempio, \holtxt{\holquote{\dol{}if~P}} si trasforma
nella forma \ml{Absyn}
\begin{verbatim}
   APP(IDENT "if", IDENT "P")
\end{verbatim}
  \emph{non} in una forma che coinvolge \holtxt{COND}.

  Più tipicamente, spesso si scrive qualcosa come
	\holtxt{\holquote{\$+~x}}, che genera la sintassi astratta
\begin{verbatim}
   APP(IDENT "+", IDENT "x")
\end{verbatim}
  Senza il segno di dollaro, il parser della sintassi concreta si lamenterebbe
	del fatto che l'infisso più non ha un argomento
	sinistro. Quando il risultato di successo del parsing è passato alla
	fase successiva, il fatto che ci sia una costante chiamata \holtxt{+}
	darà all'input il suo significato desiderato.

  Si può eseguire l'``escape'' dei simboli racchiudendoli in parentesi.
	Così, quello di sopra si potrebbe scrivere \holtxt{\holquote{(+)~x}} per avere
	lo stesso effetto.
\index{token!sopprimere il comportamento di parsing dei|)}

  L'utente può inserire a questo punto nel processo di parsing delle funzioni
	di trasformazione intermedia sviluppate per proprio conto . Questo è fatto
	con la funzione
\begin{verbatim}
   add_absyn_postprocessor
\end{verbatim}
  La funzione dell'utente sarà di tipo \ml{Absyn~->~Absyn} e potrà
	eseguire qualunque cambiamento sia appropriato. Come tutti gli altri aspetti del
	parsing, queste funzioni sono parte di una grammatica: se l'utente non vuole
	vedere usata una particolare funzione, può organizzarsi in modo che il parsing
	sia fatto rispetto a una grammatica differente.

\item[Risoluzione dei Nomi:] Le forme nude \ml{IDENT} nel valore \ml{Absyn}
	sono risolte come variabili libere, nomi legati o costanti.
	Questo processo risulta in un valore del tipo di dati \ml{Preterm}, che
	ha costruttori simili a quelli in \ml{Absyn} eccetto che con le forme
	per le costanti. %
	\index{parsing, della logica HOL@parsing, della logica \HOL{}!preterms}%
	Una stringa può essere convertita direttamente in un \ml{Preterm} per mezzo della
	funzione \ml{Preterm}.

  Un nome legato è il primo argomento a un costruttore \ml{LAM}, un
	identificatore che occorro sul lato sinistro di una freccia di
	una espressione case, o un identificatore che occorre all'interno di un pattern
	di comprensione d'insieme. Una costante è una stringa che è presente nel dominio
	dell'``overload map'' della grammatica. Le variabili libere sono tutti gli altri identificatori.
	Variabili libere dello stesso nome in un termine avranno lo stesso
	tipo. Gli identificatori sono testati per vedere se sono legati, e poi per
	vedere se sono costanti. Così è possibile scrivere
\begin{verbatim}
   \SUC. SUC + 3
\end{verbatim}
  ed avere la stringa \holtxt{SUC} trattata come un numero nel
	contesto dell'astrazione data, piuttosto che come la costante
	successore.

\index{parsing, della logica HOL@parsing, della logica \HOL{}!overloading}
\index{overloading|see{parsing, of \HOL{} logic, overloading}}
  L'``overloap map'' è una mappa da stringhe a liste di termini. I
	termini di solito sono solo costanti, ma possono essere termini arbitrari (dando
	origine a ``macro sintattiche'' o ``pattern'').\index{macro sintattiche}
	Questa infrastruttura è usata per permettere ad un nome come \holtxt{+} di mappare a
	costanti di addizione differenti in teorie come
	\theoryimp{arithmetic}, \theoryimp{integer}, e
  \theoryimp{words}. In questo modo i nomi ``reali'' delle costanti si possono
	slegare da ciò che l'utente digita. Nel caso dell'addizione, il
	più sui numeri naturali è di fatto chiamato \holtxt{+} (strettamente,
	\holtxt{arithmetic\$+}); ma sugli interi, è
	\holtxt{int\_add}, e su word è \holtxt{word\_add}. (Si noti
	che poiché ciascuna costante arriva da una teoria differente e così da un
	namespace differenti, esse potrebbero avere tutte il nome \holtxt{+}.)

  \index{inferenza di tipo!nel parser HOL@nel parser \HOL{}}
  Quando la risoluzione dei nomi determina che un identificatore dovrebbe essere trattato
	come una costante, esso è mappato a una forma pretermine che elenca tutte le
	possibilità per quella stringa. Successivamente, poiché i termini nel
	range della mappa di overload tipicamente avranno tipi differenti,
	l'inferenza di tipo spesso eliminerà le possibilità dalla lista. Se
	rimangono più possibilità dopo che l'inferenza di tipo è stata
	eseguita, allora sarà stampato un warning, e sarà scelta una
	delle possibilità. (Gli utenti possono controllare quali termini sono
	selezionati quando si presenta questa situazione.)

  Quando un termine nella mappa di overload è scelto come l'opzione migliore, è
	sostituito nel termini alla posizione appropriata. Se il termine è una
	lambda astrazione, allora sono fatte tante $\beta$-riduzioni quante
	possibili, usando qualsiasi argomento a cui il termine sia stato applicato. E'
	in questo modo che un pattern sintattico può elaborare gli argomenti. (Si veda
	anche la Sezione~\ref{sec:parser:syntactic-patterns} per maggiori informazioni sui
	pattern sintattici.)
\item[Inferenza di Tipo:] %
  \index{inferenza di tipo!nel parser HOL@nel parser \HOL{}}%
  Tutti i termini nella logica \HOL{} sono ben tipizzati. Il kernel rafforza
	questo attraverso le API per il tipo di dato \ml{term}. (In particolare,
	la funzione \ml{mk\_comb} %
	\index{mk_comb@\ml{mk\_comb}}%
	controlla che il tipo del primo argomento sia una funzione il cui
	dominio è uguale al tipo del secondo argomento.) Il lavoro del
	parser è di trasformare le stringhe fornite dall'utente in termini. Per convenienza,
	è vitale che l'utente non debba fornire tipi per tutti gli
	identificatori che digita. (Si veda la Sezione~\ref{sec:parser:type-inference}
	di sotto.)

  In presenza di identificatori sottoposti a overload, l'inferenza di tipo può non essere
	in grado di assegnare un tipo unico a tutte le costanti. Se esistono
	più possibilità, ne sarà scelto uno quando il \ml{Preterm} è infine
	convertito in un termine genuino.
\item[Conversione a un Termine:]%
  Quando è stato completato il controllo di tipo di un \ml{Preterm}, la conversione finale da
	quel tipo al tipo del \ml{term} è per lo più semplice. L'utente
	può pure inserire ulteriore elaborazione a questo punto, così che una
	funzione fornita dall'utente modifichi il risultato prima che il parser
	lo restituisca.
\end{description}

\subsubsection{Caratteri unicode}
\label{sec:parser:unicode-characters}

\index{Unicode}\index{UTF-8}
\index{parsing, della logica HOL@parsing, della logica \HOL{}!Caratteri unicode|(}
E' possibile fare in modo che l'infrastruttura \HOL{} di parsing e stampa
utilizzi caratteri Unicode (scritti nella codifica UTF-8). Questo rende
possibile scrivere e leggere termini come
\begin{alltt}
   \(\forall\)x. P x \(\land\) Q x
\end{alltt}
piuttosto che
\begin{alltt}
   !x. P x /\bs{} Q x
\end{alltt}
Se lo desiderano, gli utenti possono semplicemente definire costanti che hanno caratteri
Unicode nei loro nomi, e lasciare le cose come stanno. Il problema con
questo approccio è che gli strumenti standard probabilmente creeranno file
di teoria che includono binding \ML{} (illegali) come \ml{val $\rightarrow$\_def =
  \dots}. Il risultato saranno file \ml{...Theory.sig} e
\ml{...Theory.sml} che falliranno di compilare, anche se la chiamata a
\ml{export\_theory} può avere successo. Questo problema può essere manovrato attraverso
l'uso di funzioni come \ml{set\_MLname}, probabilmente è una pratica
migliore usare solamente caratteri alfanumerici nei nomi delle costanti, e
usare poi funzioni come \ml{overload\_on} e \ml{add\_rule} per creare
la sintassi unicode per la costante sottostante.

Se gli utenti hanno a disposizione dei font con il repertorio di caratteri appropriati per
mostrare la loro sintassi, e confidano nel fatto che anche ognuno degli utenti delle loro
teorie li hanno, allora questo è perfettamente ragionevole. Tuttavia, se
gli utenti vogliono mantenere la retro-compatibilità con la pura sintassi
ASCII, possono farlo definendo prima una sintassi ASCII pura. Dopo aver fatto
questo, possono creare una versione Unicode della sintassi con la
funzione \ml{Unicode.unicode\_version}. Quindi, quando la variabile
di traccia \ml{"Unicode"}  è $0$, la sintassi ASCII sarà usata per
il parsing e la stampa. Se la traccia è impostata a $1$, allora funzionerà
anche la sintassi unicode nel parse, e il pretty-priunter
la preferirà quando i termini sono stampati.

Per esempio, in \ml{boolScript.sml}, il carattere Unicode per l'and
logico (\texttt{$\land$}), è impostato come un'alternativa unicode per
\texttt{/\bs} con la chiamata
\begin{verbatim}
   val _ = unicode_version {u = UChar.conj, tmnm = "/\\"};
\end{verbatim}
(In questo contesto, è stata aperta la struttura \ml{Unicode},
dando accesso anche alla struttura \ml{UChar} che contiene i binding
per l'alfabeto Greco, e alcuni altri simboli matematici.)

L'argomento a \ml{unicode\_version} è un record con campi \ml{u}
e \ml{tmnm}. Entrambe sono stringhe. Il campo \ml{tmnm} può essere o
il nome di una costante, o un token che appare in una regola di sintassi concreta
(che possibilmente mappa a qualche altro nome). Se il \ml{tmnm} è solo il
nome di una costante, allora, con la variabili di traccia abilitata, la stringa
\ml{u} sarà sottoposta a overloading con lo stesso nome. Se il \ml{tmnm} è lo
stesso di un token di una regola di sintassi concreta, allora il comportamento è di
creare una nuova regola che mappa allo stesso nome, ma con la stringa \ml{u}
usata come il token.

\paragraph{Regole di lexing con caratteri Unicode}
%
\index{token!Caratteri unicode}
\index{identificatori, nella logica HOL@identificatori, nella logica \HOL{}!caratteri che non associano}
%
In parole povere, \HOL{} considera i caratteri divisi in tre
classi: alfanumerici, simboli che non associano e simboli. Questo
influenza il comportamento del lexer quando incontra delle stringhe di
caratteri. A meno che ci sia già nella grammatica uno specifico token ``misto'',
i token si dividono quando la classe di caratteri cambia. Così, nella
stringa
\begin{verbatim}
   ++a
\end{verbatim}
il lexer vedrà due token, \holtxt{++} e \holtxt{a}, perché
\holtxt{+} è un simbolo e \holtxt{a} è un alfanumerico. La
classificazione dei caratteri Unicode extra è molto
semplicistica: tutt le lettere greche eccetto \holtxt{$\lambda$} sono alfanumerici;
il simbolo di negazione logica \holtxt{$\neg$} non associa; e
ogni altra cosa è simbolica. (L'eccezzione per \holtxt{$\lambda$} è per
permettere a stringhe come \holtxt{$\lambda$x.x} di essere divise dal lexer in \emph{quattro} token.)

\index{parsing, della logica HOL@parsing, della logica \HOL{}!Caratteri unicode|)}

\subsubsection{Pattern sintattici (``macro'')}
\label{sec:parser:syntactic-patterns}
\index{parsing, della logica HOL@parsing, della logica \HOL{}!overloading}
\index{parsing, della logica HOL@parsing, della logica \HOL{}!pattern sintattici|(}

La ``mappa di overload'' menzionata in precedenza è di fatto una combinazione di
mappe, una per il parsing, e una per la stampa. La mappa di parsing è da
nomi a liste di termini, e determina come i nomi che appaiono in un
\ml{Preterm} saranno tradotti in termini. In sostanza, i nomi legati sono tradotti
in variabili legate, nomi non legati che non sono nel dominio della mappa sono tradotti
in variabili libere, e nomi non legati nel dominio della mappa sono tradotti
in uno degli elementi dell'insieme associato con il nome dato.
Ogni termine nell'insieme delle possibilità può avere un tipo differente, così
l'inferenza di tipo sceglierà da quelli che hanno tipi coerenti con
il resto del termine dato. Se la lista risultante contiene più di
un elemento, allora il termine che appare prima nella lista sarà
scelto.

Il caso d'uso più comune per la mappa di overload è di avere nomi mappati a
costanti. In questo modo, per esempio, le varie teorie aritmetiche possono
mappare la stringa \ml{"+"} alla nozione rilevante di addizione, ognuna delle
quali è una costante differente. Tuttavia, il sistema ha una flessibilità
extra perché i nomi possono mappare termini arbitrari. Per esempio, è
possibile mappare a istanze di costanti con specifici tipi. Così, la
stringa \ml{"<=>"} mappa l'eguaglianza, ma dove gli argomenti sono forzati
essere di tipo \holquote{:bool}.

Inoltre, se il termine mappato è una lambda-astrazione (cioè, della
forma $\lambda x.\;M$), allora il parser eseguirà tutte le $\beta$-riduzioni
possibili per quel termine e gli argomenti che lo accompagnano. per
esempio, in \theoryimp{boolTheory} e nei suoi discendenti, la stringa
\ml{"<>"} è sottoposta a overload rispetto al termine \holquote{\bs{}x~y.~\td{}(x~=~y)}.
Inoltre, \ml{"<>"} è impostato come un infisso al livello della sintassi
concreta. Quando l'utente inserisce \holquote{x~\lt\gt~y}, il valore
\ml{Absyn} risultante è
\begin{verbatim}
   APP(APP(IDENT "<>", IDENT "x"), IDENT "x")
\end{verbatim}
The \ml{"x"}  and \ml{"y"} identifiers will map to free variables, but
the \ml{"<>"} identifier maps to a list containing
\holquote{\bs{}x~y.~\td(x~=~y)}. This term has type
\begin{verbatim}
   :'a -> 'a -> bool
\end{verbatim}
e le variabili polimorfiche sono generalizzabili, permettendo all'inferenza
di tipo di dare i tipi (identici) appropriati a \ml{x} e \ml{y}.
Assumendo che questa opzione sia l'unico oveloading per il \ml{"<>"} lasciato
dopo l'inferenza di tipo, allora il termine risultante sarà
\holtxt{\td(x~=~y)}. Meglio, benché questa sarà la struttura
sottostante del termine in memoria, esso sarà di fatto stampato come
\holquote{x~\lt\gt~y}.

Se il termine mappato nella mappa di overload contiene qualsiasi variabili libere,
queste variabili non saranno istanziate in alcun modo. In particolare,
se queste variabili hanno tipi polimorfici, allora le variabili di tipo in
questi tipi saranno costanti: non soggette a istanziazione dall'inferenza
di tipo.

\paragraph{Il pretty-printing e i pattern sintattici} La seconda parte della ``mappa di overload'' è una mappa da termini a stringhe, che specifica come
i termini dovrebbero essere trasformati indietro in identificatori. (Benché di fatto
non costruisca un valore \ml{Absyn}, questo processo inverte la fase del parsing
di risoluzione dei nomi, producendo qualcosa che è poi stampato
secondo la parte di sintassi concreta della grammatica data.)

Poiché il parsing può mappare nomi singoli in complicate strutture di termine,
la stampa deve essere in grado di ricondurre una complicata struttura di termine a
un nome singolo. Esso fa questo eseguendo
un term matching.%
\index{matching!nel pretty-printing dei termini}%
%
\footnote{Il matching eseguito è del primo ordine; per contro il matching di ordine superiore
	è fatto nel semplificatore.}
Se più pattern matchano con lo stesso termine, allora il printer sceglie il
match più specifico (quello che richiede l'istanziazione minima delle
variabili del pattern.) Se questo risulta ancora in più possibilità, ugualmente
specifiche, ha la precedenza il pattern aggiunto
più recentemente. (Gli utenti possono così manipolare le preferenze del printer
eseguendo delle chiamate altrimenti ridondanti alla funzione \ml{overload\_on}.)

Nell'esempio di sopra dell'operatore non-uguale-a, il pattern sarà
\holtxt{\~{}(?x = ?y)}, dove i punti interrogativi, indicano variabili del pattern
istanziabili. Se un pattern include delle variabili libere (si ricordi che
la \ml{x} e la \ml{y} in questo esempio erano legate da un'astrazione),
allora queste non saranno istanziabili.

Non c'è alcun'altra finezza nell'uso di questa infrastruttura: matching
``più grandi'', che coprono più di un termine hanno la precedenza. La difficoltà
che questo può causare è illustrata nel pattern \holtxt{IS\_PREFIX} dalla teoria
\theoryimp{rich\_listTheory}. Per questioni di retro-compatibilità
questo identificatore mappa a
\begin{verbatim}
   \x y. isPREFIX y x
\end{verbatim}
dove \holtxt{isPREFIX} è una costante da \theoryimp{listTheory}.
(La questione è che \holtxt{IS\_PREFIX} si aspetta i suoi argomenti in
ordine inverso a quello che si aspetta \holtxt{isPREFIX}.) Ora, quando questa
macro è impostata la mappa di overload contiene già un mapping dalla
stringa \holtxt{"isPREFIX"} alla costante \holtxt{isPREFIX} (questo
accade con ogni definizione di costante). Ma dopo la chiamata
che stabilisce il nuovo pattern per \holtxt{IS\_PREFIX}, la
forma \holtxt{isPREFIX} non sarà più stampata. Né è sufficiente,
ripetere la chiamata
\begin{verbatim}
   overload_on("isPREFIX", ``isPREFIX``)
\end{verbatim}
Piuttosto (supponendo che \holtxt{isPREFIX} sia di fatto la forma
di stampa preferita), la chiamata deve essere
\begin{verbatim}
   overload_on("isPREFIX", ``\x y. isPREFIX x y``)
\end{verbatim}
così che il pattern di \ml{isPREFIX} è lungo quanto quella di \ml{IS\_PREFIX}.
\index{parsing, della logica HOL@parsing, della logica \HOL{}!pattern sintattici|)}


\subsubsection{Vincoli di tipo}
\label{sec:parseprint-type-constraints}

\index{vincoli di tipo!nel parser HOL@nel parser \HOL{}}
Un termine può essere vincolato ad essere un certo tipo. Per esempio,
\holtxt{X:bool} vincola la variabile \holtxt{X} ad avere il tipo
\holtxt{bool}. Un tentativo di vincolare un
termine in modo inappropriato solleverà un'eccezione: per esempio,
\begin{hol}
\begin{verbatim}
   if T then (X:ind) else (Y:bool)
\end{verbatim}
\end{hol}
fallirà perché entrambi i rami di un condizionale dovono essere dello stesso
tipo. I vincoli di tipo possono essere visti come un suffisso che lega più
strettamente di qualunque altra cosa eccetto l'applicazione di funzione. Così $\term\
\ldots\ \term \ : \type$ è uguale a $(\term\ \ldots\ \term)\ :
\type$, ma $x < y:\holtxt{num}$ è un vincolo legittimo sulla sola
variabile $y$.


L'inclusione di \holtxt{:} negli identificatori simbolici significa che può
essere necessario separare qualche vincolo con degli spazi vuoti. Per esempio,
\begin{hol}
\begin{verbatim}
   $=:bool->bool->bool
\end{verbatim}
\end{hol}
sarà suddiviso dal lexer \HOL{} come
\begin{hol}
\begin{verbatim}
   $=: bool -> bool -> bool
\end{verbatim}
\end{hol}
ed elaborato dal parser come un'applicazione del'identificatore simbolico \holtxt{\$=:} alla
lista di termini argomento [\holtxt{bool}, \holtxt{->}, \holtxt{bool},
\holtxt{->}, \holtxt{bool}]. Uno spazio messo al punto giusto eviterà questo problema:
\begin{hol}
\begin{verbatim}
   $= :bool->bool->bool
\end{verbatim}
\end{hol}
è elaborato da parser come l'identificatore simbolico ``='' vincolato a un tipo.
Al posto del \holtxt{\$}, si possono anche usare le parentesi per rimuovere dai lessemi
un comportamento di parsing speciale:
\begin{hol}
\begin{verbatim}
   (=):bool->bool->bool
\end{verbatim}
\end{hol}

\subsubsection{Inferenza di tipo}
\label{sec:parser:type-inference}

\index{inferenza di tipo!nel parser HOL@nel parser \HOL{}|(}
Si consideri il termine \holtxt{x = T}: esso (e tutti i suoi sottotermini)
ha un tipo nella logica \HOL{}. Ora, \holtxt{T} ha il tipo \holtxt{bool}. Questo
significa che la costante \holtxt{=} ha tipo \holtxt{xty -> bool -> bool},
per qualche tipo \holtxt{xty}. Dal momento che lo schema di tipo per \holtxt{=} è
\holtxt{'a -> 'a -> bool}, sappiamo che \holtxt{xty} di fatto deve essere
\holtxt{bool} perché l'istanza di tipo sia ben formata. Sapendo
questo, possiamo dedurre che il tipo di \holtxt{x} deve essere \holtxt{bool}.

Trascurando il gergo (``schema'' e ``istanza'') nel precedente
paragrafo, abbiamo condotto un assegnamento di tipo alla struttura di termine,
finendo con un termine ben formato. Sarebbe molto noioso per gli utenti
condurre un tale assegnamento a mano per ciascun termine immesso ad \HOL{}.
Così, \HOL{} usa un adattamento dell'algoritmo d'inferenza di tipo di Milner
per l'\ML{} quando si costruiscono dei termini attraverso il parsing. Alla fine dell'inferenza
di tipo, alle variabili di tipo non vincolate sono assegnati dei nomi da parte del sistema.
Di solito, questa assegnazione fa la cosa giusta. Tuttavia, a volte, il
tipo più generale non è ciò che si desidera e l'utente deve aggiungere dei vincoli
di tipo ai sotto termini rilevanti. Per situazioni complicate, si può assegnare la
variabili globale \ml{show\_types}. Quando è impostato questo flag,
i prettyprinter per i termini e i teoremi mostreranno come i tipi
sono stati assegnati ai sottotermini. Se non si vuole che il sistema assegni
delle variabili di tipo per proprio conto, si può impostare la variabile globale
\ml{guessing\_tyvars} a \ml{false}, nel qual caso
l'esistenza delle variabili di tipo non assegnate alla fine dell'inferenza di tipo
solleveranno un'eccezzione.
\index{type inference!nel parser HOL@nel parser \HOL{}|)}


\subsubsection{Overloading}
\label{sec:parsing:overloading}

\index{parsing, della logica HOL@parsing, della logica \HOL{}!overloading|(}
Una misura limitata di risoluzione di overloading è eseguita dal parser
del termine. Per esempio, il simbolo `tilde' (\holtxt{\~{}})
denota la negazione booleana nella teoria iniziale di \HOL, e denota anche
l'invero additivo nelle teorie \ml{integer} e
\ml{real}. Se carichiamo la teoria \ml{integer}
e immettiamo un termine ambiguo con \holtxt{\~{}}, il
sistema ci informerà che è stata eseguita la risoluzione dell'overloading.

\setcounter{sessioncount}{0}
\begin{session}
\begin{verbatim}
- load "integerTheory";
> val it = () : unit

- Term `~~x`;
<<HOL message: more than one resolution of overloading was possible.>>
> val it = `~~x` : term

- type_of it;
> val it = `:bool` : hol_type
\end{verbatim}
\end{session}

Per risolvere più scelte possibili è usato un meccanismo di priorità.
Nell'esempio, \holtxt{\~{}} potrebbe essere coerentemente scelto avere il tipo
\holtxt{:bool -> bool} o \holtxt{:int -> int}, e il
meccanismo ha scelto il primo. Per un controllo più fine, si possono usare dei
vincoli di tipo espliciti. Nella seguente sessione, il
\holtxt{\~{}\~{}x} nella prima quotation ha il tipo \holtxt{:bool},
mentre nella seconda, un vincolo di tipo assicura che \holtxt{\~{}\~{}x} ha
il tipo \holtxt{:int}.

\begin{session}
\begin{verbatim}
- show_types := true;
> val it = () : unit

- Term `~(x = ~~x)`;
<<HOL message: more than one resolution of overloading was possible.>>
> val it = `~((x :bool) = ~~x)` : term

- Term `~(x:int = ~~x)`;
> val it = `~((x :int) = ~~x)` : term
\end{verbatim}
\end{session}

Si noti che il simbolo \holtxt{\~{}} sta per due costanti differenti nella
seconda quotation; la sua prima occorrenza è la negazione booleana, mentre
le altre due occorrenze sono l'operazione d'inverso additivo per
gli interi.
\index{parsing, della logica HOL@parsing, della logica \HOL{}!overloading|)}

\subsubsection{Le fixity}
\label{sec:parseprint:fixities}

Al fine di fornire qualche flessibilità notazionale, le costanti sono disponibili in varie forme o {\it fixity}: oltre a essere costanti ordinarie (senza alcuna fixity), le costanti possono essere anche dei {\it binder}, {\it prefissi}, {\it suffissi}, {\it infissi}, o {\it closefix}.
Più in generale, i termini possono anche essere rappresentati usando specifiche {\it mixfix} ragionevolmente arbitrarie.
Il grado in cui i termini legano i loro argomenti associati è conosciuto come precedenza.
Più grande è questo numero, più stretto il binding.
Per esempio, nel momento in cui è introdotto, \verb-+- ha una precedenza di 500,  mentre il più stretto binder moltiplicazione (\verb+*+) ha una precedenza di 600.

\paragraph{I binder}

Un binder è un costrutto che lega una variabile; per esempio, il
quantificatore universale. In \HOL, questo è rappresentato usando un trucco che
risale ad Alnozo Church: un binder è una costante che prende una lambda
astrazione come suo argomento. Il binding lambda è usato per implementare
il binding del costrutto. Questa è una soluzione elegante ed uniforme.
Così la sintassi concreta \verb+!v. M+ è rappresentata
dall'applicazione della costante \verb+!+ all'astrazione \verb+(\v. M)+.

I binder più comuni sono \verb+!+, \verb+?+, \verb+?!+, e
\verb+@+. A volte si vogliono iterare applicazione dello stesso
binder, ad esempio,
\begin{alltt}
   !x. !y. ?p. ?q. ?r. \term.
\end{alltt}
Questo può invece essere reso come
\begin{alltt}
   !x y. ?p q r. \term.
\end{alltt}

\paragraph{Infissi}

Le costanti infisse possono associare in tre modi differenti: a destra,
a sinistra o non associare del tutto. (Se \holtxt{+} fosse non-associativo, allora
il parser non riuscirebbe ad elaborare \holtxt{3 + 4 + 5}; si dovrebbe scrivere
\holtxt{(3 + 4) + 5} o \holtxt{3 + (4 + 5)} a seconda del significato
desiderato.) L'ordine di precedenza per l'insieme iniziale di infissi è
\holtxt{/\bs}, \holtxt{\bs/}, \holtxt{==>}, \holtxt{=},
\begin{Large}\holtxt{,}\end{Large} (la virgola\footnote{Quando
	è stata caricata \theoryimp{pairTheory}.}). Inoltre, tutte queste
costanti sono associative a destra. Così
\begin{hol}
\begin{verbatim}
   X /\ Y ==> C \/ D, P = E, Q
\end{verbatim}
\end{hol}
%
è uguale a
%
\begin{hol}
\begin{verbatim}
   ((X /\ Y) ==> (C \/ D)), ((P = E), Q).
\end{verbatim}
\end{hol}
%
\noindent Un'espressione
\[
\term \; \holtxt{<infix>}\; \term
\]
è rappresentata internamente come
\[
((\holtxt{<infix>}\; \term)\; \term)
\]

\paragraph{Prefissi}

Mentre gli infissi appaiono tra i loro argomenti, i prefissi appaiono prima di essi.
Questo potrebbe inizialmente apparire la stessa cosa di quanto accade con la normale applicazione di funzione dove il simbolo alla sinistra semplicemente non ha alcuna fixity: $f$ in $f(x)$ non si comporta forse come un prefisso?
Di fatto tuttavia, in un termine come $f(x)$, dove $f$ e $x$ non hanno fixity, la sintassi è trattata come se ci fosse una invisibile applicazione di funzione infissa tra i due token: $f\cdot{}x$.
Questo operatore infisso lega più strettamente, così che quando si scrive $f\,x + y$, il risultato del parser è $(f\cdot{}x) + y$\footnote{Ci sono operatori infissi che legano più strettamente, il punto nella selezione di campo fa sì che $f\,x.fld$ sia elaborato dal parser come $f\cdot(x.fld)$.\index{tipi record!notazione di selezione del campo}}.
E' quindi utile permettere dei prefissi genuini così che gli operatori possano vivere a livelli di precedenza differenti rispetto all'applicazione di funzione.
Un esempio di questo è \verb+~+, la negazione logica.
Questo è un prefisso con una precedenza più bassa dell'applicazione di funzione.
Normalmente
\[
   f\;x\; y\qquad \mbox{è elaborata dal parser come}\qquad (f\; x)\; y
\] ma \[
  \holtxt{\~{}}\; x\; y\qquad\mbox{è elaborata dal parser come}\qquad
  \holtxt{\~{}}\; (x\; y)
\]
poiché la precedenza di \verb+~+ è più bassa di quella dell'applicazione di funzione.
Il simbolo unario di negazione sarebbe anche tipicamente definito come un prefisso, se non altro per permettere di scrivere \[
  {\it negop}\,{\it negop}\,3
\]
(qualunque cosa sia {\it negop}) senza bisogno di parentesi extra.

Dall'altro lato, la sintassi \holtxt{univ} per l'insieme universale (si veda la Sezione~\ref{sec:theory-of-sets}\index{universal set}) è un esempio di un operatore prefisso che lega più strettamente dell'applicazione.
Questo significa che \holtxt{f\,univ(:'a)} è elaborato dal parser come \holtxt{f(univ(:'a))}, non come \holtxt{(f univ)(:'a)} (su cui il parser fallirebbe il controllo di tipo).

\paragraph{Suffissi}

I suffissi appaiono dopo il loro argomenti. Non ci sono suffissi
introdotti nelle teorie standard disponibili in \HOL{}, ma gli utenti
sono sempre in grado di introdurli per loro conto se lo scelgono. I suffissi sono
associati con una precedenza esattamente come lo sono gli infissi e i prefissi.
Se \holtxt{p} è un prefisso, \holtxt{i} un infisso, e \holtxt{s} un
suffisso, allora ci sono sei ordinamenti possibili per i tre differenti
operatori, sulla base delle loro precedenze, dando cinque risultati per il parsing di
$\holtxt{p}\; t_1\; \holtxt{i}\; t_2\; \holtxt{s}$ a seconda delle
relative precedenze:
\[
\begin{array}{cl}
\mbox{\begin{tabular}{c}Precedenze\\(dalla più bassa alla più alta)\end{tabular}} &
\multicolumn{1}{c}{\mbox{Risultato del parsing}}\\
\hline
p,\;i,\;s & \holtxt{p}\;(t_1\;\holtxt{i}\;(t_2\;\holtxt{s}))\\
p,\;s,\;i & \holtxt{p}\;((t_1\;\holtxt{i}\;t_2)\;\holtxt{s})\\
i,\;p,\;s & (\holtxt{p}\;t_1)\;\holtxt{i}\;(t_2\;\holtxt{s})\\
i,\;s,\;p & (\holtxt{p}\;t_1)\;\holtxt{i}\;(t_2\;\holtxt{s})\\
s,\;p,\;i & (\holtxt{p}\;(t_1\;\holtxt{i}\;t_2))\;\holtxt{s}\\
s,\;i,\;p & ((\holtxt{p}\;t_1)\;\holtxt{i}\;t_2)\;\holtxt{s}\\
\end{array}
\]

\paragraph{I closefix}

I termini closefix sono operatori che racchiudono completamente gli argomenti.
Un esempio che si potrebbe usare nello sviluppo di una teoria della
semantica denotazionale sono le parentesi semantiche. Così, le infrastrutture di parsing
di \HOL{} possono essere configurate in modo da permettere di scrivere \holtxt{denotation x}
come \holtxt{[| x |]}. I closefix non sono associati con delle precedenze
perché non possono competere per gli argomenti con altri operatori.


\subsubsection{Trucchi e magia del parser}

Qui descriviamo come ottenere alcuni effetti utili con il
parser in \HOL{}.

\begin{description}

\item[Aliasing] Se si vuole che una sintassi speciale sia un ``alias'' per una
	forma \HOL{} normale, questo è facile da ottenere; entrambi gli esempi fatti finora
	di fatto hanno fatto proprio questo. Tuttavia, se si vuole avere soltanto una
	normale sostituzione uno-a-uno di una stringa per un'altra, non si può
	usare la fase grammatica/sintassi per parsing per fare questo. Piuttosto, si
	può usare il meccanismo di overloading. Per esempio, sia
	\texttt{MEM} un alias per \texttt{IS\_EL}. Abbiamo bisogno della funzione
	\texttt{overload\_on} per impostare l'overload della constante originale sul nuovo
	nome:
\begin{verbatim}
   val _ = overload_on ("MEM", Term`IS_EL`);
\end{verbatim}

\item[Rendere l'addizione associativa a destra] Se si ha un numero di vecchi
	script che assumono che l'addizione sia associativa a destra perché questo è
	come era una volta \HOL{}, potrebbe essere troppo penoso convertire tutto. Il trucco
	è di rimuovere tutte le regole al livello dato della grammatica, e
	rimetterle come infissi che associano sulla destra. il modo più semplice per riconoscere
	quali regole sono nella grammatica è per ispezione (usando
	\ml{term\_grammar()}). Con la sola \ml{arithmeticTheory}
	caricata, gli unici infissi al livello 500 sono \holtxt{+} and
  \holtxt{-}. Così, rimuoviamo le loro regole:
\begin{verbatim}
   val _ = app temp_remove_rules_for_term ["+", "-"];
\end{verbatim}
  \noindent E poi le rimettiamo con l'associatività
	appropriata:
\begin{verbatim}
   val _ = app (fn s => temp_add_infix(s, 500, RIGHT)) ["+", "-"];
\end{verbatim}
\noindent Si noti che usiamo le versioni \ml{temp\_} di queste due
funzioni così che altre teorie che dipendono da questa non saranno
influenzate. Si noti inoltre che non possiamo avere due infissi allo stesso
livello di precedenza con differenti associatività, così dobbiamo
rimuovere entrambi gli operatori, non solo l'addizione.

\item[Sintassi mix-fix per {\it if-then-else}:]
\index{condizionali, nella logica HOL@condizionali, nella logica \HOL{}!stampa dei}
%
Il primo passo per andare in questa direzione è di guardare all'aspetto generale
delle espressioni di questa forma. In questo caso, sarà:
%
\[
  \holtxt{if}\;\; \dots \;\;\holtxt{then}\;\;\dots\;\;
  \holtxt{else}\;\;\dots
  \]
%
 Dal momento che ci deve essere un termine ``a penzoloni'' sulla destra, la
	fixity appropriata è \ml{Prefix}. Sapendo che il termine costante
	sottostante è chiamato \holtxt{COND}, il modo più semplice per ottenere
	la sintassi desiderata è:
\begin{verbatim}
val _ = add_rule
   {term_name = "COND", fixity = Prefix 70,
    pp_elements = [TOK "if", BreakSpace(1,0), TM, BreakSpace(1,0),
                   TOK "then", BreakSpace(1,0), TM, BreakSpace(1,0),
                   TOK "else", BreakSpace(1,0)],
    paren_style = Always,
    block_style = (AroundEachPhrase, (PP.CONSISTENT, 0))};
\end{verbatim}
\noindent La regola effettiva è leggermente un pò più complicata, e
si può trovare nei sorgenti della teoria \theoryimp{bool}.

\item[Sintassi mix-fix sintassi per la sostituzione di termini:]

Qui ciò che si desidera è di essere in grado di scrivere qualcosa come:
\[
  \mbox{\texttt{[}}\,t_1\,\mbox{\texttt{/}}\,t_2\,\mbox{\texttt{]}}\,t_3
\]
denotando la sostituzione di $t_1$ per $t_2$ in $t_3$, magari
traducendolo in \holtxt{SUB $t_1$ $t_2$ $t_3$}. Questo sembra
come ci dovesse essere un altro \ml{Prefix}, ma la scelta delle
parentesi quadre (\holtxt{[} e \holtxt{]}) come delimitatori sarebbe
in conflitto con la sintassi concreta per i letterali lista se si facesse questo.
Dato che i letterali lista sono di fatto della classe
\ml{CloseFix}, la nuova sintassi deve essere della stessa classe. Questo è abbastanza semplice
da fare: impostiamo la sintassi
\[
\holtxt{[}\,t_1\,\holtxt{/}\,t_2\,\holtxt{]}
\]
in modo che mappi a \holtxt{SUB $t_1$ $t_2$}, un valore di un tipo
funzionale, che quando applicato a un terzo argomento apparirà
corretto\footnote{Si noti che facendo la stessa cosa per
	l'esempio \textit{if-then-else} di sopra sarebbe
	inappropriato, dal momento che permetterebbe di scrivere
\[ \holtxt{if}\;P\;\holtxt{then}\;Q\;\holtxt{else} \]
senza l'argomento finale}.
La regola per questo è così:
\begin{verbatim}
  val _ = add_rule
           {term_name = "SUB", fixity = Closefix,
            pp_elements = [TOK "[", TM, TOK "/", TM, TOK "]"],
            paren_style = OnlyIfNecessary,
            block_style = (AroundEachPhrase, (PP.INCONSISTENT, 2))};
\end{verbatim}

\end{description}

\subsubsection{Nascondere le costanti}
\label{hidden}

\index{parsing, della logica HOL@parsing, della logica \HOL{}!nascondere lo status di costante|(}
\index{sistema HOL@sistema \HOL{}!nascondere le costanti nel|(}
\index{costanti, nella logica HOL@costanti, nella logica \HOL{}!nascondere lo status delle}
\index{parsing, della logica HOL@parsing, della logica \HOL{}!overloading}
%
La seguente funzione può essere usata per nascondere lo status di costante di un
nome dal parser delle quotation.

\begin{holboxed}
\index{hide@\ml{hide}|pin}
\begin{verbatim}
  val hide   : string -> ({Name : string, Thy : string} list *
                          {Name : string, Thy : string} list)
\end{verbatim}
\end{holboxed}

\noindent La valutazione di \ml{hide "$x$"}
fa sì che il parser delle quotation tratti $x$ come una variabile (purché le regole
lessicali lo permettano), anche se $x$ è il nome di una costante nella teoria attuale
(le costanti e le variabili possono avere lo stesso nome).
Questo è utile se si vogliono usare delle variabili
%
\index{variabili, nella logica HOL@variabili, nella logica \HOL{}!con nomi di costante}
%
con lo stesso nome di costanti dichiarate in precedenza (o incorporate)
(ad esempio \ml{o}, \ml{I}, \ml{S} \etc). Il nome $x$ è ancora una costante
per i costruttori, le teorie, ecc; \ml{hide} influisce solo sul parsing e
la stampa rimuovendo il nome dato dalla ``mappa di overload'' descritta
di sopra nella Sezione~\ref{sec:parser:architecture}. Si noti che l'effetto
di \ml{hide} è \emph{temporaneo}; i suoi effetti non persistono nelle
teorie discendenti da quella attuale. Si veda la voce \ml{hide} in
\REFERENCE{} per maggiori dettagli, inclusa una spiegazione del tipo
restituito.

La funzione

\begin{holboxed}
\index{reveal@\ml{reveal}|pin}
\begin{verbatim}
   reveal : string -> unit
\end{verbatim}
\end{holboxed}

\noindent annulla il nascondimento.

La funzione

\begin{holboxed}
\index{hidden@\ml{hidden}|pin}
\begin{verbatim}
   hidden : string -> bool
\end{verbatim}
\end{holboxed}

\noindent controlla se una stringa è il nome di una costante nascosta.
\index{sistema HOL@sistema \HOL{}!adattamento dell'interfaccia utente del}
\index{sistema HOL@sistema \HOL{}!nascondere le costanti nel|)}
\index{parsing, della logica HOL@parsing, della logica \HOL{}!nascondere lo status di costante nel|)}

\subsubsection{Adattare la profondità del pretty-print}
\index{stampa, nella logica HOL@stampa, nella logica \HOL{}!adattamento della profondità strutturale nella}

La seguente reference \ML{} può essere usata per impostare la profondità massima
della stampa

\begin{holboxed}
\index{max_print_depth@\ml{max\_print\_depth}|pin}
\begin{verbatim}
   max_print_depth : int ref
\end{verbatim}
\end{holboxed}

\index{profondità di stampa di default, per la logica HOL@profondità di stampa di default, per la logica \HOL{}|(}

\noindent La profondità di default della stampa è $-1$ che è intesa significare
nessun massimo. I sotto termini annidati più profondamente della profondità
massima di stampa sono stampati come \holtxt{...}. Per esempio:

\setcounter{sessioncount}{0}
\begin{session}
\begin{verbatim}
- ADD_CLAUSES;
> val it =
    |- (0 + m = m) /\ (m + 0 = m) /\ (SUC m + n = SUC (m + n)) /\
       (m + SUC n = SUC (m + n)) : thm

- max_print_depth := 3;
> val it = () : unit
- ADD_CLAUSES;
> val it = |- (... + ... = m) /\ (... = ...) /\ ... /\ ... : thm
\end{verbatim}
\end{session}
\index{profondità di stampa di default, per la logica HOL@profondità di stampa di default, per la logica \HOL{}|)}

\subsection{Quotation e antiquotation}
\label{sec:quotation-antiquotation}

\index{quotation, nella logica HOL@quotation, nella logica \HOL{}!parser per}
\index{parsing, della logica HOL@parsing, della logica \HOL{}!della sintassi di quotation|(}
La sintassi correlata alla logica nel sistema HOL è tipicamente passato al
parser in forme speciali conosciute come \emph{quotation}. Una quotation di base
è delimitata da singoli accenti grave (cioè, \ml{`}, carattere ASCII~96). Quando
i valori quotation sono stampati dal loop interattivo ML, appaiono
piuttosto brutti a causa  dello speciale filtro che è fatto di questi
valori ancor prima che l'interprete li veda:
\setcounter{sessioncount}{0}
\begin{session}
\begin{verbatim}
- val q = `f x = 3`;
> val 'a q = [QUOTE " (*#loc 1 11*)f x = 3"] : 'a frag list
\end{verbatim}
\end{session}
Quotations (Moscow ML prints the type as \ml{'a frag list}) are the
raw input form expected by the various HOL parsers.  They are also
polymorphic (to be explained below).  Thus the function
\ml{Parse.Term} function takes a (term) quotation and returns a term,
and is thus of type \[ \ml{term quotation -> term}
\]

I parser dei termini e dei tipi possono essere chiamati anche implicitamente usando
i doppi accenti acuti come delimitatori. Per il parser dei tipi, il primo
carattere non spazio dopo il delimitatore principale deve essere un segno di deu punti.
Così:
\begin{session}
\begin{verbatim}
- val t = ``p /\ q``;
> val t = ``p /\ q`` : term

- val ty = ``:'a -> bool``;
> val ty = ``:'a -> bool`` : hol_type
\end{verbatim}
\end{session}

L'espressione legata alla variabile ML \ml{t} di sopra di fatto è espansa
a un'applicazione della funzione \ml{Parse.Term} alla quotation
argomento \ml{`p /\bs{} q`}. Analogamente, la seconda espressione si espande
in un'applicazione di \ml{Parse.Type} alla quotation \ml{`:'a -> bool`}.

Il vantaggio importante delle quotation rispetto a normali stringhe \ML{} è
che esse possono includere caratteri di nuova riga e backslash senza
richiedere caratteri speciali di escape. Le nuove righe occorrono ogni volta che i termini vanno oltre
una dimensione banale, mentre i backslash occorrono non solo nella
rappresentazione di $\lambda$, ma anche nella sintassi per la congiunzione e
la disgiunzione.

Se una quotation deve includere un carattere di accento grave, allora questo dovrebbe
essere fatto usando il carattere di escape proprio della sintassi della quotation, il
caret (\ml{\^}, carattere ASCII~94). Per avere un semplice caret, le cose diventano
leggermente più complicate. Se una sequenza di caret è seguita dallo
spazio vuoto (incluso un carattere di nuova riga), allora quella sequenza di caret è
passata al parser di HOL senza modifiche. Altrimenti, un singolo caret si può
ottenere scrivendone due in una riga. (L'ultima regola è analoga al
modo in cui in \ML{} la sintassi delle stringhe tratta il backslash.) Così:
\begin{session}
\begin{verbatim}
- ``f ^` x ``;
<<HOL message: inventing new type variable names: 'a, 'b, 'c>>
> val it = ``f ` x`` : term

- ``f ^ x``;
<<HOL message: inventing new type variable names: 'a, 'b, 'c>>
> val it = ``f ^ x`` : term
\end{verbatim}
\end{session}

La regola per i caret non seguiti da uno spazio vuoto è illustrata qui,
includendo un esempio che accade quando non si segue la regola
per il quoting:
\begin{session}
\begin{verbatim}
- ``f ^^+ x``;
<<HOL message: inventing new type variable names: 'a, 'b, 'c>>
> val it = ``f ^+ x`` : term

- ``f ^+ x``;
! Toplevel input:
! (Parse.Term [QUOTE " (*#loc 2 3*)f ", ANTIQUOTE (+),
!              QUOTE " (*#loc 2 7*) x"]);
!                                                  ^
! Ill-formed infix expression
\end{verbatim}
\end{session}

L'uso principale del caret è d'introdurre le \emph{quntiquotation} (come
suggerito nell'ultimo esempio di sopra). All'interno di una quotation, l'espressioni
della forma {\small\verb+^(+}$t${\small\verb+)+}
%
\index{ antiquotation, nella logica HOL@{\small\verb+^+} (antiquotation, nella logica \HOL{})}
%
(dove $t$ è un'espressione \ML\ di tipo
%
\index{controllo di tipo, nella logica HOL@controllo di tipo, nella logica \HOL{}!antiquotation nel}
%
\ml{term} o \ml{type}) sono chiamate antiquotation.
%
\index{termini, nella logica HOL@termini, nella logica \HOL{}!antiquotation}
\index{antiquotation, nei termini della logica HOL@antiquotation, nei termini della logica \HOL{}}
%
Una quotation \holtxt{\^{}($t$)} è valutata al
valore \ML{} di $t$. Per esempio, {\small\verb+``x \/ ^(mk_conj(``y:bool``, ``z:bool``))``+}
è valutata allo stesso termine di {\small\verb+``x \/ (y /\ z)``+}. L'uso
più comune dell'antiquotation è quando il termine $t$ è legato a una variabile
\ML\ $x$. In questo caso {\small\verb+^(+}$x${\small\verb+)+} può essere
abbreviato da {\small\verb+^+}$x$.

La seguente sessione illustra l'antiquotation.

\setcounter{sessioncount}{0}
\begin{session}
\begin{verbatim}
- val y = ``x+1``;
> val y = ``x + 1`` : term

val z = ``y = ^y``;
> val z = ``y = x + 1`` : term

- ``!x:num.?y:num.^z``;
> val it = ``!x. ?y. y = x + 1`` : term
\end{verbatim}
\end{session}

\noindent Anche i tipi possono essere sottoposti all'antiquotation:

\begin{session}
\begin{verbatim}
- val pred = ``:'a -> bool``;
> val pred = ``:'a -> bool`` : hol_type

- ``:^pred -> bool``;
> val it = ``:('a -> bool) -> bool`` : hol_type
\end{verbatim}
\end{session}

\noindent Le quotation sono polimorfiche, e la variabile di tipo di una
quotation corrisponde al tipo dell'entità che può essere sottoposta all'antiquotation
in quella quotation. Dal moemnto che il parser dei termini si aspetta solo termini
sottoposti ad antiquotation, l'antiquotation di un tipo all'interno di una quotation di termine richiede l'uso di
\holtxt{ty\_antiq}. Per esempio,%
%
\index{ty_antiq@\ml{ty\_antiq}}

\begin{session}
\begin{verbatim}
- ``!P:^pred. P x ==> Q x``;

! Toplevel input:
! Term `!P:^pred. P x ==> Q x`;
!           ^^^^
! Type clash: expression of type
!   hol_type
! cannot have type
!   term

- ``!P:^(ty_antiq pred). P x ==> Q x``;
> val it = `!P. P x ==> Q x` : term
\end{verbatim}
\end{session}
%
\index{parsing, della logica HOL@parsing, della logica \HOL{}!della sintassi delle quotation|)}



\subsection{Retro-compatibilità della sintassi}

Questa sezione del manuale documenta il cambiamento (esteso) fatto al
parsing di \HOL{} dei termini e dei tipi nella release Taupo (una delle
release HOL3) e al di là del punto di vista di un utente che non
vuole sapere come usare le nuove strutture, ma vuole essere sicuro
che il proprio vecchio codice continui a funzionare in modo pulito.

I cambiamenti che possono far sì che i vecchi termini falliscano il parsing sono:
\begin{itemize}
\newcommand\condexp{\holtxt{$p$ => $q$ | $r$}}
\item La precedenza delle annotazioni di tipo è completamente cambiata. Ora
	è un suffisso molto stretto (benché con una precedenza più debole di quella
	associata con l'applicazione di funzione), invece di uno debole.
	Questo significa che \mbox{\tt (x,y:bool \# bool)} ora dovrebbe essere scritto
	come \mbox{\tt (x,y):bool \# bool}. La forma precedente sarà ora
	parsata come un'annotazione di tipo che si applica solo a \verb+y+. Questo
	cambiamento porta la sintassi delle logica più vicina a quella dell'SML e
	dovrebbe rendere in genere più facile annotare le tuple, dal momento che ora
	si può scrivere \[ (x\,:\,\tau_1,\;y\,:\,\tau_2,\dots z\,:\,\tau_n)
  \] al posto di \[
  (x\,:\,\tau_1, \;(y\,:\,\tau_2, \dots (z\,:\,\tau_n)))
  \] dove le parentesi extra si sono dovute aggiungere solo per permettere di
	scrivere una forma di vincolo che occorre frequentemente.
\item La maggior parte degli operatori aritmetici ora sono associativi a sinistra piuttosto che
	a destra. In particolare, $+$, $-$, $*$ e {\tt DIV} sono
	associativi a sinistra. In modo simile, gli analoghi operatori nelle altre
	teoria aritmetiche come {\tt integer} e {\tt real} sono anche associativi
	a sinistra. Questo porta il parser di \HOL{} in linea con la pratica
	matematica standard.
\item Il binding dell'eguaglianza nell'espressioni {\tt let} è trattata esattamente
	nello stesso modo dell'eguaglianze negli altri contesti. Nelle versioni precedenti
	di \HOL, l'eguaglianze in questo contesto hanno una precedenza di bindig differente
	più debole.
\item La vecchia sintassi per l'espressioni condizionali è stata
	rimossa. Così la stringa \holquote{\condexp} ora deve essere
	scritta
	$\holquote{\texttt{if}\;p\;\texttt{then}\;q\;\texttt{else}\;r}$.
\item Alcune categorie lessicali sono sorvegliate più strettamente. I letterali
	stringa (le stringhe all'interno dei doppi apici) e quelli numerici non possono essere usati
	a meno che le teorie rilevanti non siano state caricate. Inoltre questi
	letterali non possono essere usati come variabili all'interno di scopi di binding.
\end{itemize}


\section{Un Semplice Gestore di Dimostrazione Interattivo}\label{sec:goalstack}

Il \emph{goal stack} fornisce una semplice interfaccia di dimostrazione interattiva
basata sulle tattiche. Quando si vogliono usare le tattiche per decomporre una dimostrazione, sorgono
molti stati intermedi; il goalstack si prende cura del necessario mantenimento
di queste informazioni. L'implementazione dei goalstack qui riportati è un
ridisegno della concezione originale di Larry Paulson.

La libreria goalstack è caricata automaticamente quando \HOL{} si avvia.

I tipi astratti \ml{goalstack} e \ml{proofs} sono il
punto focale delle operazioni di dimostrazione all'indietro. il tipo \ml{proofs} può essere
considerato come una lista di goalstack indipendenti. La maggior parte delle operazioni agiscono sulla
testa della lista dei goalstack; ci sono anche operazioni così che il
punto focale può essere cambiato.

\subsection{Avviare un goalstack di dimostrazione}

\begin{hol}
\begin{verbatim}
   g        : term quotation -> proofs
   set_goal : goal -> proofs
\end{verbatim}
\end{hol}

Si ricordi che il tipo \ml{goal} è un'abbreviazione per
\ml{term list * term}. Per partire su un nuovo goal, si da a
\ml{set\_goal} un goal. Questa crea un nuovo goalstack e lo rende il
punto focale di ulteriori operazioni.

Un'abbreviazione per \ml{set\_goal} è la funzione \ml{g}: essa
invoca il parser automaticamente, e non permette al goal di
avere alcuna assunzione.

La chiamata a \ml{set\_goal}, o \ml{g}, aggiunge un nuovo tentativo di dimostrazione a
quelli esistenti, \textit{cioè}, al posto di sovrascrivere il tentativo
di dimostrazione attuale, il nuovo tentativo è impilato in cima.

\subsection{Applicare una tattica a un goal}

\begin{hol}
\begin{verbatim}
   expandf : tactic -> goalstack
   expand  : tactic -> goalstack
   e       : tactic -> goalstack
\end{verbatim}
\end{hol}

Come si fa dunque di fatto a fare una dimostrazione goalstack? Nella maggior parte dei casi,
l'applicazione delle tattiche al goal attuale è fatto con la funzione
\verb+expand+. Nel raro caso in cui si voglia applicare una
tattica {\it invalida\/}, allora è usata \verb+expandf+. (Per una
spiegazione delle tattiche invalide, si veda il Capitolo 24 di \& Melham.)
Per espandere una tattica si può anche usare l'abbreviazione \verb+e+.


\subsection{Undo}

\begin{hol}
\begin{verbatim}
   b          : unit -> goalstack
   rd         : unit -> goalstack
   drop       : unit -> proofs
   dropn      : int  -> proofs
   backup     : unit -> goalstack
   redo       : unit -> goalstack
   restart    : unit -> goalstack
   set_backup : int  -> unit
\end{verbatim}
\end{hol}

Spesso (siamo tentati di dire {\it di solito}!) si prende una strada sbagliata
nel fare una dimostrazione, o si fa un errore nell'impostare un goal. Per annullare un passo
nel goalstack, sono usate la funzione \ml{backup} e la sua abbreviazione
\ml{b}. Questo ripristinerà il goalstack al suo stato precedente.
Per rifare un passaggio nel goalstack, si usa la funzione \ml{redo},
abbreviata come \ml{rd}.


Per eseguire il backup completo al goal originale, può essere usata
la funzione \ml{restart}. Ovviamente, è anche importante liberarsi
dei tentativi di dimostrazione che sono sbagliati; per questo c'è \ml{drop},
che si sbarazza del tentativo di dimostrazione corrente, e \ml{dropn}, che
elimina i primi $n$ tentativi di dimostrazione.


Ogni tentativo di dimostrazione ha la sua \emph{lista-di-annullamento} degli stati
precedenti. La lista di annullamento per ciascun tentativo è di dimensione fissata (inzialmente
12). Se si vuole impostare questo valore per il tentativo corrente di dimostrazione, si può
usare la funzione \ml{set\_backup}. Se la dimensione della lista di
backup è impostata essere più piccola di quanto sia attualmente, la lista di annullamente sarà
immediatamente troncata. Non si può annullare un'operazione ``proofs-level'', come
\ml{set\_goal} o \ml{drop}.

\subsection{Visualizzare lo stato del proof manager}

\begin{hol}
\begin{verbatim}
   p            : unit -> goalstack
   status       : unit -> proofs
   top_goal     : unit -> goal
   top_goals    : unit -> goal list
   initial_goal : unit -> goal
   top_thm      : unit -> thm
\end{verbatim}
\end{hol}

Per visualizzare lo stato del proof manager in qualsiasi momento, si possono
usare le funzioni \ml{p} e \ml{status}. La prima mostra solo
i subgoal in cima al goalstack corrente, mentre la seconda da una
sintesi di ogni tentativo di dimostrazione.

To get the top goal or goals of a proof attempt, use \ml{top\_goal}
and \ml{top\_goals}. To get the original goal of a proof attempt,
use \ml{initial\_goal}.

Per ottenere il o i top goal di un tentativo di dimostrazione, si usi \ml{top\_goal}
e \ml{top\_goals}. Per ottenere il goal originale di un tentativo di dimostrazione,
si usi \ml{initial\_goal}.

Una volta che un teorema è stato dimostrato il goalstack che è stato usato per derivarlo
continua ad esistere (e anche la sua lista-di-annullamento): il suo compito principale ora è quello di
mantenere il teorema. Questo teorema può essere estratto con
\ml{top\_thm}.

\subsection{Spostare il fuoco su un differente subgoal o tentativo di dimostrazione}

\begin{hol}
\begin{verbatim}
   r             : int -> goalstack
   R             : int -> proofs
   rotate        : int -> goalstack
   rotate_proofs : int -> proofs
\end{verbatim}
\end{hol}

Spesso vogliamo spostare la nostra attenzione a un differente goal nella dimostrazione
attuale, o a una dimostrazione differente. Le funzioni che fanno questo sono
\ml{rotate} e \ml{rotate\_proofs}, rispettivamente. Le abbreviazioni
\ml{r} e \ml{R} sono più semplici da digitare.

\section{Dimostrazione di Alto Livello---\texttt{bossLib}}
% would use \ml{boss} above but it puts LaTeX into fits
\label{sec:bossLib}
\newcommand\bossLib{\ml{bossLib}}

\index{bossLib@\ml{bossLib}}
La libreria \bossLib\ introduce alcuni degli strumenti di dimostrazione di teoremi
più ampiamente utilizzati in \HOL{} e li fornisce di un'interfaccia conveniente
per l'interazione. La libreria attualmente si concentra su tre cose:
definizione di datatype e funzioni; operazioni interattive di dimostrazione
di alto livello, e composizione di ragionatori automatici. Il caricamento di \bossLib\
impegna a lavorare in un contesto che fornisce già le teorie
dei booleani, le coppie, le somme, il tipo option, l'aritmetica, e le liste.


\subsection{Supporto per passi di dimostrazione di alto livello}
\label{sec:high-level-proof-steps}

Le seguenti funzioni usano informazione nel database per facilitare
l'applicazione delle funzionalità di \HOL{} sottostanti.

\index{Induct_on (tattica ML d'induzione)@\ml{Induct\_on} (tattica \ML{} d'induzione)}
\index{Cases_on (tattica ML di case-split)@\ml{Cases\_on} (tattica \ML{} di case-split)}
\begin{verbatim}
   type_rws     : hol_type -> thm list
   Induct       : tactic
   Cases        : tactic
   Cases_on     : term quotation -> tactic
   Induct_on    : term quotation -> tactic
\end{verbatim}

\index{type_rws@\ml{type\_rws}}
\index{TypeBase@\ml{TypeBase}}
%
La funzione \ml{type\_rws} cercherà per il tipo dato nel
database sottostante \ml{TypeBase} e restituirà utili regole di riscrittura per
quel tipo. Le regole di riscrittura del datatype sono costruite a partire dai
teoremi di iniettività e distinzione, insieme con la definizione di costante
case. Le tattiche di semplificazione \ml{RW\_TAC}, \ml{SRW\_TAC},
e il \simpset{} \ml{(srw\_ss())} includono automaticamente questi
teoremi. Altre tattiche usate con altri \simpset{} avranno bisogno di questi
teoremi per essere aggiunte manualmente.

\index{teoremi d'induzione, nella logica HOL@teoremi d'induzione, nella logica \HOL{}!per tipi di dato algebrici}
%
La tattica \ml{Induct} rende conveniente invocare l'induzione. Quando
è applicata a un goal, è esaminato il quantificatore universale principale;
se il suo tipo è quello di un datatype conosciuto, è estratta e applicata
l'appropriata tattica d'induzione strutturale.

The \ml{Cases} tactic makes it convenient to invoke case
analysis. The leading universal quantifier in the goal is examined; if
its type is that of a known datatype, the appropriate structural
case analysis theorem is extracted and applied.

La tattica \ml{Cases\_on} prende una quotation, che è
parsata a un termine $M$, e poi in $M$ viene effettuata una ricerca per il goal. Se $M$
è una variabile, allora si cerca per una variabile con lo stesso nome. Una volta
che si conosce il termine su cui effettuare lo split, il suo tipo e i fatti associati sono
ottenuti dal database sottostante e usati per eseguire il case
split. Se alcune delle variabili libere di $M$ sono legate nel goal, è fatto un tentativo
per rimuovere i quantificatori (universali) così che il case split abbia
vigore. Infine, $M$ non ha bisogno di apparire nel goal, benché dovrebbe almeno
contenere alcune delle variabili libere che compaiono già nel goal. Si noti
che la tattica \ml{Cases\_on} è più generale di \ml{Cases}, ma
richiede che gli sia dato un termine esplicito.

\index{Induct_on (tattica ML d'induzione)@\ml{Induct\_on} (tattica \ML{} d'induzione)}
La tattica \ml{Induct\_on} prende una quotation, che è parsata in un
termine $M$, e poi si cerca in $M$ il goal. Se $M$ è una
variabile, allora si cerca per una variabile con lo stesso nome. Una volta che il
termine su cui effettuare l'induzione è conosciuto, il suo tipo e i fatti associati sono
ottenuti dal database sottostante e usati per eseguire
l'induzione. Se $M$ non è una variabile, è creato una nuova variabile $v$
che non occorre già nel goal, ed è usata per costruire un termine $v = M$
a cui viene subordinato il goal prima che sia eseguita
l'induzione. Prima tuttavia, tutti i termini che contengono variabili libere da $M$
sono spostate dalle assunzioni alla conclusione del goal, e tutte
le variabili libere dei $M$ sono quantificate universalmente. \ml{Induct\_on} è
più generale di \ml{Induct}, ma richiede che le venga dato un termine
esplicito.

Sono stati forniti tre entry-point supplementari per induzioni più
esotiche:
\begin{description}
\item [\ml{completeInduct\_on}] esegue un'induzione completa sul
	termine denotato dalla quotazione data. L'induzione completa permette
	un'ipotesi d'induzione apparentemente\footnote{L'induzione completa e l'induzione
		matematica ordinaria sono entrambe derivabili l'una dall'altra.} più forte
	rispetto all'induzione matematica ordinaria: vale a dire, quando
	si esegue l'induzione su $n$, è permesso assumere che la proprietà valga per
	\emph{tutti} gli $m$ più piccoli di $n$. Formalmente: $\forall P.\ (\forall x.\
  (\forall y.\ y < x \supset P\, y) \supset P\,x) \supset \forall x.\
  P\,x$. Questo permette di usare l'ipotesi d'induzione più di
	una volta, e permette anche d'istanziare l'ipotesi d'induzione
	in modo diverso dal predecessore.

\item [\ml{measureInduct\_on}] prende una quotation, e la suddivide
	per trovare un termine e una funzione misura con cui indurre.
	Per esempio, se si volesse fare un'induzione sulla lunghezza di una lista
	\holtxt{L}, l'invocazione \ml{measureInduct\_on~`LENGTH L`}
	sarebbe appropriata.

\item [\ml{recInduct}] prende un teorema d'induzione generato da
	\ml{Define} o \ml{Hol\_defn} e lo applica al goal attuale.

\end{description}


\subsection{Ragionatori Automatici}
\label{sec:automated-reasoners}

\ml{bossLib} riunisce i più potenti ragionatori in \HOL{} e
prova a rendere facile comporli in un modo semplice. Prendiamo i nostri ragionatori
base da \ml{mesonLib}, \ml{simpLib}, e \ml{numLib},
ma il punto di \ml{bossLib} è di fornire un livello di astrazione così
che l'utente debba sapere solo pochi entry-point\footnote{Nella metà degli anni 1980
	Graham Birtwistle ha sostenuto un tale approccio, chiamandolo `HOL in Dieci
	Tattiche}. (Quest librerie sottostanti, e altre che forniscono strumenti analogamente
potenti sono descritte nel dettaglio nelle sezioni di sotto.)
\begin{hol}
\begin{verbatim}
   PROVE      : thm list -> term -> thm
   PROVE_TAC  : thm list -> tactic

   METIS_TAC  : thm list -> tactic
   METIS_PROVE: thm list -> term -> thm

   DECIDE     : term quotation -> thm
   DECIDE_TAC : tactic
\end{verbatim}
\end{hol}
La regola d'inferenza \texttt{PROVE} (e la tattica corrispondente
\texttt{PROVE\_TAC}) prende una lista di teoremi e un termine, e tenta
di dimostrare il termine usando un ragionatore al primo ordine. Le due funzioni
\ml{METIS} eseguono la stessa funzionalità ma usano un metodo di dimostrazione
sottostante differente. Gli entry-point \texttt{PROVE} si riferiscono alla
libreria \texttt{meson}, che è ulteriormente descritta nella
Sezione~\ref{sec:mesonLib} di sotto. Il sistema \ml{METIS} è descritto
nella Sezione~\ref{sec:metisLib}. La regola d'inferenza \texttt{DECIDE}
(e la tattica corrispondente \texttt{DECIDE\_TAC}) applica una procedura
di decisione che (al meno) gestisce enunciati dell'aritmetica lineare.

\begin{hol}
\begin{verbatim}
   RW_TAC   : simpset -> thm list -> tactic
   SRW_TAC  : ssfrag list -> thm list -> tactic
   &&       : simpset * thm list -> simpset  (* infix *)
   std_ss   : simpset
   arith_ss : simpset
   list_ss  : simpset
   srw_ss   : unit -> simpset
\end{verbatim}
\end{hol}
%
\index{RW_TAC@\ml{RW\_TAC}} La tattica di riscrittura \ml{RW\_TAC} lavora
prima aggiungendo i teoremi dati nel \simpset dato; poi
semplifica il goal quanto più possibile; quindi esegue dei case split
sull'espressioni condizionali nel goal; poi ripetutamente (1)
elimina tutte le ipotesi della forma $v = M$ o $M = v$ dove $v$ è
una variabile che non occorre in $M$, (2) rompe qualsiasi equazione tra
termini costruttore ovunque nel goal. Infine,
\ml{RW\_TAC} solleva le espressioni-\holtxt{let} all'interno del goal così che
l'equazioni binding appaiono come
abbreviazioni\index{abbreviazioni!dimostrazione basata-su-tattche} nelle
assunzioni.

\index{SRW_TAC@\ml{SRW\_TAC}} La tattica \ml{SRW\_TAC} è analoga a
\ml{RW\_TAC}, ma lavora rispetto a un \simpset{} sottostante
(accessibile attraverso la funzione \ml{srw\_ss}) che viene aggiornato ogni volta che viene caricato
un nuovo contesto. Questo \simpset{} può essere aumentato attraverso
l'aggiornamento di ``frammenti \simpset{} '' (valori \ml{ssfrag}) e
teoremi. Nelle situazioni in cui ci sono grandi tipi archiviati nel
sistema, le performance di \ml{RW\_TAC} ne possono risentire perché
aggiunge ripetutamente tutti i teoremi di riscrittura per i tipi conosciuti in un
\simpset{} prima di attaccare il goal. Dall'altro lato,
\ml{SRW\_TAC} carica le riscritture nel \simpset{} al di sotto di
\ml{srw\_ss()} una volta sola, rendendo l'operazione più veloce in questa
situazione.

\ml{bossLib} fornisce un numero d'insiemi di semplificazione. Il
simpset per la logica pura, le somme, le coppie e il tipo \ml{option} è
chiamato \ml{std\_ss}. Il simpset per l'aritmetica è chiamato
\ml{arith\_ss}, e il simpset per le liste è chiamato \ml{list\_ss}.
I simpset forniti da \bossLib{} aumentano strettamente di forza:
\ml{std\_ss} è contenuto in \ml{arith\_ss}, e \ml{arith\_ss} è
contenuto in \ml{list\_ss}. Il combinatore infisso \ml{\&\&} è usato
per costruire un nuovo \simpset{} da un \simpset{} e una lista di
teoremi dati. La tecnologia di semplificazione di \HOL{} è ulteriormente descritta nella
Sezione~\ref{sec:simpLib} di sotto e nelle \REFERENCE.

\begin{hol}
\begin{verbatim}
   by : term quotation * tactic -> tactic (* infix 8 *)
   SPOSE_NOT_THEN : (thm -> tactic) -> tactic
\end{verbatim}
\end{hol}
La funzione \ml{by} è un operatore infisso che prende una quotation
e una tattica $tac$. La quotation è parsata in un termine $M$. Quando
l'invocazione ``\ml{$M$ by $\mathit{tac}$}'' è applicata a un goal
$(A,g)$, è creato un nuovo subgoal $(A,M)$ e ad esso è applicata la tattica $tac$.
Se il goal è dimostrato, il teorema risultante è de-costruito e aggiunto
alle assunzioni del goal originale; così la dimostrazione procede con
il goal $((M::A), g)$. (Si noti tuttavia, che avverrà uno split dei casi
se la de-costruzione di $\ \vdash M$ espone delle disgiunzioni.) Così
\ml{by} permette di mischiare un utile stile di ragionamento `asserzionale' o `Mizar-like'
all'ordinaria dimostrazione basata sulle tattiche\footnote{Le dimostrazioni nel
	sistema Mizar sono documenti leggibili, diversamente dalla maggior parte
	delle dimostrazioni basate su tattiche.}

L'entry-point \ml{SPOSE\_NOT\_THEN} inizia una dimostrazione per
contraddizione assumendo la negazione del goal e spostando la
negazione all'interno dei quantificatori. Essa fornisce il teorema
risultante come un argomento della funzione fornite, che userà il
teorema per costruire e applicare una tattica.

\section{Dimostrazione al Primo Ordine---\texttt{mesonLib} e \texttt{metisLib}}
\label{sec:first-order-proof}
\index{procedure di decisione!logica del primo ordine}

La dimostrazione del primo ordine è una potente tecnica di dimostrazione di teoremi che può
sbrigare goal complicati. Diversamente da strumenti come il semplificatore, o
dimostra un goal completamente, o fallisce. Non può trasformare un goal
in una forma differente (e più utile).

\subsection{Model elimination---\texttt{mesonLib}}
\label{sec:mesonLib}

\index{meson (model elimination) procedura@\ml{meson} (model elimination) procedura}
\index{metodo model elimination per la logica del primo ordine}

La libreria \ml{meson} è un'implementazione del
metodo model-elimination per trovare dimostrazioni di goal nella logica
del primo ordine. Ci sono tre entry-point principali:
\begin{hol}
\begin{verbatim}
   MESON_TAC     : thm list -> tactic
   ASM_MESON_TAC : thm list -> tactic
   GEN_MESON_TAC : int -> int -> int -> thm list -> tactic
\end{verbatim}
\end{hol}

Ciascuna di quest tattiche tenta di dimostrare il goal. Esse o avranno
successo nel fare questo, o falliranno con un eccezione ``depth exceeded''. Se
il fattore di ramificazione nello spazio di ricerca è alto, le tattiche
\texttt{meson} possono richiedere anche molto tempo per raggiungere la profondità massima.

Tutte le tattiche \texttt{meson} prendono una lista di teoremi. Questi
fatti extra sono usati dalla procedura di decisione per aiutare a dimostrare il goal.
\texttt{MESON\_TAC} ignora le assunzioni del goal; gli altri due
entry-point includono le assunzioni come pare del sequente da
dimostrare.

I parametri extra a \ml{GEN\_MESON\_TAC} forniscono un controllo extra del
comportamento dell'aumentare iterativo della profondità che è al centro della
ricerca per una dimostrazione. In ogni iterazione data, l'algoritmo ricerca
per una dimostrazione di profondità non più alta di un parametro $d$. Il
comportamento di default per \ml{MESON\_TAC} e \ml{ASM\_MESON\_TAC} è di iniziare $d$
a 0, incrementarlo di uno ogni volta che una ricerca fallisce, e di fallire se
$d$ eccede il valore archiviato nel valore della reference
\ml{mesonLib.max\_depth}. Per contro,
\ml{GEN\_MESON\_TAC~min~max~step} inizia $d$ a \ml{min}, lo incrementa
di \ml{step}, e rinuncia quando $d$ eccede \ml{max}.

La funzione \ml{PROVE\_TAC} da \ml{bossLib} esegue qualche
normalizzazione, prima di passare un goal e le sue assunzioni a
\ml{ASM\_MESON\_TAC}. A causa di questa normalizzazione, nella maggior parte
delle circostanze, si dovrebbe preferire \ml{PROVE\_TAC}
a \ml{ASM\_MESON\_TAC}.

\subsection{Risoluzione---\texttt{metisLib}}
\label{sec:metisLib}

\index{procedura metis (risoluzione)@procedura \ml{metis} (risoluzione)}
\index{metodo di risoluzione per la logica del primo ordine}

La libreria \ml{metis} è un'implementazione del metodo di risoluzione
per trovare dimostrazioni di goal nella logica del primo ordine. Ci sono due
entry-point principali:

\begin{hol}
\begin{verbatim}
   METIS_TAC   : thm list -> tactic
   METIS_PROVE : thm list -> term -> thm
\end{verbatim}
\end{hol}

Entrambe le funzioni prendono una lista di teoremi, e questi sono usati come lemmi
nella dimostrazione. \texttt{METIS\_TAC} è una tattica, e o avrà successo
nel dimostrare il goal, o se non ha successo o fallirà o continuerà a ciclare
all'infinito. \texttt{METIS\_PROVE} prende un termine $t$ e prova a dimostrare un
teorema con conclusione $t$: se ha successo, è restituito il teorema
$\vdash t$. Come per \texttt{METIS\_TAC}, potrebbe fallire o ciclare all'infinito se
la ricerca della dimostrazione non ha successo.

La famiglia \texttt{metisLib} di strumenti di dimostrazione implementano la risoluzione
ordinata e il calcolo di paramodulazione ordinata per la logica del primo ordine,
che di solito li rende più adatti a goal che richiedono ragionamenti di eguaglianza
non banali rispetto alle tattiche in \texttt{mesonLib}.


\section{Semplificazione---\texttt{simpLib}}
\label{sec:simpLib}
\index{semplificazione|(}

Il semplificatore è il motore di riscrittura più sofisticato in \HOL{}. E'
raccomandato come un cavallo di battaglia di scopo generale durante la dimostrazione di teoremi
interattiva. Come strumenti di riscrittura, il ruolo generale del semplificatore
è di applicare teoremi della forma generale
\[
\vdash l = r
\]
a termini, rimpiazzando le istanze di $l$ nel termine con $r$. Così, la
ruotine base di semplificazione è una \emph{conversione}, che prende un termine
$t$, e restituisce un teorema $\vdash t = t'$, o l'eccezione
\ml{UNCHANGED}.

La conversione di base è
\begin{hol}
\begin{verbatim}
   simpLib.SIMP_CONV : simpLib.simpset -> thm list -> term -> thm
\end{verbatim}
\end{hol}
Il primo argomento, un \simpset, è il modo standard di fornire una
collezione di regole di (e altri dati, che saranno spiegati di sotto) al
semplificatore. Ci sono dei \simpset{} che accompagnano le teorie principali
di \HOL{}. Per esempio, il \simpset{} \ml{bool\_ss}
in \ml{boolSimps} incorpora tutti i teoremi di riscrittura usuali desiderabili su formule
booleane:
\setcounter{sessioncount}{0}
\begin{session}
\begin{verbatim}
- SIMP_CONV bool_ss [] ``p /\ T \/ ~(q /\ r)``;
> val it = |- p /\ T \/ ~(q /\ r) = p \/ ~q \/ ~r : thm
\end{verbatim}
\end{session}
Oltre alla riscrittura con i teoremi ovvi, \ml{bool\_ss} è
anche capace di eseguire semplificazioni che non sono esprimibili come
teoremi semplici:
\begin{session}
\begin{verbatim}
- SIMP_CONV bool_ss [] ``?x. (\y. P (f y)) x /\ (x = z)``;
> val it = |- (?x. (\y. P (f y)) x /\ (x = z)) = P (f z) : thm
\end{verbatim}
\end{session}
In questo esempio, il semplificatore ha eseguito una $\beta$-riduzione nel
primo congiunto sotto il quantificatore esistenziale, e poi ha fatto una
riduzione ``unwinding'' o ``one-point'', riconoscendo che l'unico
valore possibile per la variabile quantificata \holtxt{x} era il valore
\holtxt{z}.

Il secondo argomento a \ml{SIMP\_CONV} è una lista di teoremi da
aggiungere al \simpset fornito, e da aggiungere come regole di riscrittura addizionali.
In questo modo, gli utenti possono aumentare temporaneamente i \simpset{} con
le loro proprie riscritture. Se un particolare insieme di teoremi è usato spesso come
un tale argomento, allora è possibile costruire un valore \simpset{} per
incorporare queste nuove riscritture.

Per esempio, la riscrittura \ml{arithmeticTheory.LEFT\_ADD\_DISTRIB}, che
afferma che $p(m + n) = pm + pn$, non fa parte di alcun \simpset{} standard
di \HOL{}. Questo perché può causare un aumento poco attraente nella
dimensione del termine (ci sono due occorrenze di $p$ al lato destro
del teorema). Ciò nonostante, è chiaro che questo teorema può può
essere appropriato occasionalmente:
\begin{session}
\begin{verbatim}
- SIMP_CONV bossLib.arith_ss [LEFT_ADD_DISTRIB] ``p * (n + 1)``;
> val it = |- p * (n + 1) = p + n * p : thm
\end{verbatim}
\end{session}
Si noti come il \simpset{} \ml{arith\_ss} non ha solamente semplificato il
termine intermedio \ml{(p * 1)}, ma ha anche riordinato l'addizione per
mettere il termine più semplice sulla sinistra, e ordinato gli argomenti
della moltiplicazione.


\subsection{Tattiche di semplificazione}
\label{sec:simplification-tactics}
\index{semplificazione!tattiche}

Il semplificatore è implementato intorno alla conversione \ml{SIMP\_CONV},
che è una funzione per `convertire' i termini in teoremi. Per applicare
il semplificatore ai goal (alternativamente, per eseguire dimostrazioni basate su tattiche
con il semplificatore), \HOL{} fornisce cinque tattiche, ognuna delle quali è
disponibile in \ml{bossLib}.

\subsubsection{\ml{SIMP\_TAC : simpset -> thm list -> tactic}}
\index{SIMP_TAC@\ml{SIMP\_TAC}}

\ml{SIMP\_TAC} è la tattica di semplificazione più semplice: essa tenta di
semplificare il goal attuale (ignorando le assunzioni) usando il \simpset{}
dato e i teoremi aggiuntivi. Non è niente di più che il
sollevamento della sottostante conversione \ml{SIMP\_CONV} al livello
di tattica attraverso l'uso della funzione standard \ml{CONV\_TAC}.

\subsubsection{\ml{ASM\_SIMP\_TAC : simpset -> thm list -> tactic}}
\index{ASM_SIMP_TAC@\ml{ASM\_SIMP\_TAC}}

Come \ml{SIMP\_TAC}, \ml{ASM\_SIMP\_TAC} semplifica il goal attuale
(lasciando le assunzioni intatte), ma include le assunzioni
del goal come regole di riscrittura extra. Così:
\begin{session}
\begin{verbatim}
1 subgoal:
> val it =
    P x
    ------------------------------------
      x = 3
     : goalstack

- e (ASM_SIMP_TAC bool_ss []);
OK..
1 subgoal:
> val it =
    P 3
    ------------------------------------
      x = 3
     : goalstack
\end{verbatim}
\end{session}
\noindent
In questo esempio, \ml{ASM\_SIMP\_TAC} ha usato \holtxt{x = 3} come una
regola di riscrittura addizionale, e ha sostituito la \holtxt{x} di \holtxt{P x}
con \holtxt{3}. Quando un'assunzione è usata da \ml{ASM\_SIMP\_TAC} essa
è convertita in regole di riscrittura nello stesso modo dei teoremi passati nella
lista data come secondo argomento della tattica. Per esempio,
un'assunzione \holtxt{\~{}P} sarà trattata come la riscrittura \holtxt{|- P = F}.

\subsubsection{\ml{FULL\_SIMP\_TAC : simpset -> thm list -> tactic}}
\index{FULL_SIMP_TAC@\ml{FULL\_SIMP\_TAC}}

\noindent
La tattica \ml{FULL\_SIMP\_TAC} semplifica non solo la conclusione di
un goal ma anche le sue assunzioni. Essa procede semplificando
ciascuna assunzione una alla volta, usando inoltre le assunzioni precedenti nella
semplificazione delle assunzioni successive. Dopo essere stata semplificata, ciascuna
assunzione è ri-aggiunta alla lista di assunzioni del goal con la
tattica \ml{STRIP\_ASSUME\_TAC}. Questo significa che le assunzioni che
diventano congiunzioni avranno ciascuno dei congiunti assunti separatamente.
Le assunzioni che diventano disgiunzioni faranno sì che un nuovo sotto goal sia
creato per ciascun disgiunto. Se un'assunzione è semplificata a falso,
questo risolverà il goal.

\ml{FULL\_SIMP\_TAC} attacca le assunzioni nell'ordine in cui
appaiono nella lista dei termini che rappresentano le assunzioni
del goal. Tipicamente quindi, la prima assunzione da semplificare
sarà l'assunzione aggiunta più di recente. Vista alla luce della
stampa dei goal di \ml{goalstackLib}, \ml{FULL\_SIMP\_TAC} si fa
strada lungo l'elenco delle assunzioni, dal basso verso l'alto.

La seguente sessione dimostra un uso semplice di \ml{FULL\_SIMP\_TAC}:
\begin{session}
\begin{verbatim}
    x + y < z
    ------------------------------------
      0.  f x < 10
      1.  x = 4
     : goalstack

- e (FULL_SIMP_TAC bool_ss []);
OK..
1 subgoal:
> val it =
    4 + y < z
    ------------------------------------
      0.  f 4 < 10
      1.  x = 4
     : goalstack
\end{verbatim}
\end{session}
In questo esempio, l'assunzione \holtxt{x = 4} ha fatto sì che la \holtxt{x}
nell'assunzione \holtxt{f x < 10} sia stata rimpiazzata da \holtxt{4}. La
\holtxt{x} nel goal è stata sostituita in modo analogo. Se le assunzioni fossero
apparse nell'ordine opposto, solo la \holtxt{x} del goal sarebbe
cambiata.

La prossima sessione dimostra un comportamento ancora più interessante.
\begin{session}
\begin{verbatim}
> val it =
    f x + 1 < 10
    ------------------------------------
      x <= 4
     : goalstack

- e (FULL_SIMP_TAC bool_ss [arithmeticTheory.LESS_OR_EQ]);
OK..
2 subgoals:
> val it =
    f 4 + 1 < 10
    ------------------------------------
      x = 4

    f x + 1 < 10
    ------------------------------------
      x < 4
     : goalstack
\end{verbatim}
\end{session}
In questo esempio, il goal è stato riscritto con il teorema che afferma
\[
\vdash x \leq y \equiv x < y \lor x = y
\]
Sostituendo l'assunzione con una disgiunzione che risulta in due sotto goal.
Nel secondo di questi, l'assunzione \holtxt{x = 4} ha ulteriormente
semplificato il resto del goal.

\subsubsection{\ml{RW\_TAC : simpset -> thm list -> tactic}}
\index{RW_TAC@\ml{RW\_TAC}}

Nonostante il suo tipo sia lo stesso delle tattiche di semplificazioni già
descritte, \ml{RW\_TAC} è una tattica ``aumentata''. Essa è aumentata in
due modi:
\begin{itemize}
\item Quando si semplifica il goal, il \simpset{} fornito è aumentato
	non solo con i teoremi passati esplicitamente nel secondo argomento,
	ma anche con tutte le regole di riscrittura dalla \ml{TypeBase}, e
	anche con le assunzioni del goal.
%
  \index{TypeBase@\ml{TypeBase}}
\item \ml{RW\_TAC} also does more than just perform simplification.
  It also repeatedly ``strips'' the goal.  For example, it moves the
  antecedents of implications into the assumptions, splits
  conjunctions, and case-splits on conditional expressions.  This
  behaviour can rapidly remove a lot of syntactic complexity from
  goals, revealing the kernel of the problem.  On the other hand, this
  aggressive splitting can also result in a large number of
  sub-goals.  \ml{RW\_TAC}'s augmented behaviours are intertwined with
  phases of simplification in a way that is difficult to describe.
\end{itemize}

\subsubsection{\ml{SRW\_TAC : ssfrag list -> thm list -> tactic}}
\index{SRW_TAC@\ml{SRW\_TAC}}

La tattica \ml{SRW\_TAC} ha un tipo differente dalle altre
tattiche di semplificazione. Non prende un \simpset{} come un argomento.
Piuttosto la sua operazione si fonda sempre sul \simpset{} incorporato
\ml{srw\_ss()} (ulteriormente descritto nella Sezione~\ref{sec:srw_ss}). I
teoremi forniti come il secondo argomento di \ml{SRW\_TAC} sono trattati nello
stesso modo delle altre tattiche di semplificazione.  Infine, la
lista dei frammenti \simpset{} sono incorporati nel \simpset{}
sottostante, permettendo all'utente di fondere capacità di semplificazione
aggiuntive se lo desidera.

Per esempio, per includere la procedura di decisione Presburger, si potrebbe
scrivere
\begin{hol}
\begin{verbatim}
   SRW_TAC [ARITH_ss][]
\end{verbatim}
\end{hol}
I frammenti \Simpset{} sono descritti di sotto nella
Sezione~\ref{sec:simpset-fragments}.

La tattica \ml{SRW\_TAC} esegue la stessa combinazione di semplificazione e
goal-splitting che fa \ml{RW\_TAC}. Le principali differenze tra le
due tattiche risiedono nel fatto che la seconda può essere inefficiente quando
si lavora con una grande \ml{TypeBase}, e nel fatto che lavorare con
\ml{SRW\_TAC} risparmia dal dover costruire esplicitamente
dei \simpset{} che includano tutte le riscritture ``appropriate'' del contesto
attuale. Il secondo ``vantaggio'' è basato sull'assunzione che
\ml{(srw\_ss())} non include mai riscritture inappropriate. La presenza
di riscritture non utilizzate non è mai un problema: la presenza di riscritture che
fanno la cosa sbagliata può essere causa di maggiore irritazione.

\subsection{I \simpset{} standard}
\label{sec:standard-simpsets}

\HOL{} è fornito con un numero di \simpset{} standard. Ognuno di questi è
accessibile dall'interno di \ml{bossLib}, benché alcuni si originano in altre
strutture.

\subsubsection{\ml{pure\_ss} and \ml{bool\_ss}}
\label{sec:purebool-ss}
%
\index{pure_ss@\ml{pure\_ss}}
%
Il \simpset{} \ml{pure\_ss} (definito nella struttura \ml{pureSimps})
non contiene del tutto teoremi di riscrittura, e gioca il ruolo di una tabula
rasa all'interno dello spazio dei \simpset{} possibili. Quando si costruisce un
\simpset{} completamente nuovo, \ml{pure\_ss} è un punto di partenza possibile.
Il \simpset{} \ml{pure\_ss} ha solo due componenti: regole di congruenza
per specificare come traversare i termini. e una funzione che trasforma
i teoremi in regole di riscrittura. Le regole di congruenza sono ulteriormente descritte
nella Sezione~\ref{sec:advanced-simplifier}; la generazione di regole
di riscrittura da teoremi è descritta nella
Sezione~\ref{sec:simplifier-rewriting}.

\index{bool_ss (insieme di semplificazione)@\ml{bool\_ss} (insieme di semplificazione)}
%
Il \simpset{} \ml{bool\_ss} (definito nella struttura \ml{boolSimps}) è
spesso usato quando altri \simpset{} potrebbero essere troppo. Esso contiene
regole di riscritture per i connettivi booleani, e poco altro. Esso
contiene tutti i teoremi di de~Morgan per spostare le negazioni tra i
connettivi (congiunzione, disgiunzione, implicazione e espressioni
condizionali), incluse le regole dei quantificatori che hanno $\neg(\forall
x.\,P(x))$ e $\neg(\exists x.\,P (x))$ sui loro lati sinistri. Esso
contiene anche le regole che specificano il comportamento dei connettivi
quando le costanti \holtxt{T} e \holtxt{F} appaiono come loro
argomenti. (Una di queste regole è \holtxt{|- T /\bs{} p = p}.)

Come nell'esempio di sopra, \ml{bool\_ss} esegue anche
delle $\beta$-riduzioni e svolgimenti di un solo punto. Questi ultimi trasformano termini
della forma \[
\exists x.\;P(x)\land\dots (x = e) \dots\land Q(x)
\]
in
\[
P(e) \land \dots \land Q(e)
\]
Analogamente, lo svolgimento trasformerà $\forall x.\;(x = e)
\Rightarrow P(x)$ in $P(e)$.

Infine, \ml{bool\_ss} include anche regole di congruenza che permettono
al semplificatore di fare delle assunzioni aggiuntive quando sono semplificate
implicazioni ed espressioni condizionali. Questa caratteristica è spiegata
ulteriormente nella Sezione~\ref{sec:simplifier-rewriting} di sotto, ma può essere
illustrata da qualche esempio (il primo dimostra anche lo svolgimento
sotto un quantificatore universale):
\begin{session}
\begin{verbatim}
- SIMP_CONV bool_ss [] ``!x. (x = 3) /\ P x ==> Q x /\ P 3``;
> val it = |- (!x. (x = 3) /\ P x ==> Q x /\ P 3) = P 3 ==> Q 3 : thm

- SIMP_CONV bool_ss [] ``if ~(x = 3) then P x else Q x``;
> val it = |- (if ~(x = 3) then P x else Q x) =
              (if ~(x = 3) then P x else Q 3) : thm
\end{verbatim}
\end{session}

\subsubsection{\ml{std\_ss}}
%
\index{std_ss (insieme di semplificazione)@\ml{std\_ss} (insieme di semplificazione)}
%
Il \simpset{} \ml{std\_ss} è definito in \ml{bossLib}, e aggiunge
regole di riscrittura pertinenti ai tipi di somme, coppie, option e
numeri naturali a \ml{bool\_ss}.
\begin{session}
\begin{verbatim}
- SIMP_CONV std_ss [] ``FST (x,y) + OUTR (INR z)``;
<<HOL message: inventing new type variable names: 'a, 'b>>
> val it = |- FST (x,y) + OUTR (INR z) = x + z : thm

- SIMP_CONV std_ss [] ``case SOME x of NONE => P | SOME y => f y``;
> val it = |- (case SOME x of NONE => P | SOME v => f v) = f x : thm
\end{verbatim}
\end{session}

Con i numeri naturali, il \simpset{} \ml{std\_ss} può calcolare
con valori ground, e anche includere una suite di ``riscritture ovvie''
per formule che includono variabili.
\begin{session}
\begin{verbatim}
- SIMP_CONV std_ss [] ``P (0 <= x) /\ Q (y + x - y)``;
> val it = |- P (0 <= x) /\ Q (y + x - y) = P T /\ Q x : thm

- SIMP_CONV std_ss [] ``23 * 6 + 7 ** 2 - 31 DIV 3``;
> val it = |- 23 * 6 + 7 ** 2 - 31 DIV 3 = 177 : thm
\end{verbatim}
\end{session}

\subsubsection{\ml{arith\_ss}}
%
\index{arith_ss (insieme di semplificazione)@\ml{arith\_ss} (insieme di semplificazione)}
%
Il \simpset{} \ml{arith\_ss} (definito in \ml{bossLib}) estende
\ml{std\_ss} aggiungendo la capacità di decidere formule dell'aritmetica
Presburger, e per normalizzare espressioni aritmetiche (raccogliendo
coefficienti , e ri-ordinazione di addendi). La sottostante procedura di decisione
per i numeri naturali è quella descritta nella
Sezione~\ref{sec:numLib} di sotto.

Questi due aspetti del \simpset{} \ml{arith\_ss} sono dimostrati
qui:
\begin{session}
\begin{verbatim}
- SIMP_CONV arith_ss [] ``x < 3 /\ P x ==> x < 20 DIV 2``;
> val it = |- x < 3 /\ P x ==> x < 20 DIV 2 = T : thm

- SIMP_CONV arith_ss [] ``2 * x + y - x + y``;
> val it = |- 2 * x + y - x + y = x + 2 * y : thm
\end{verbatim}
\end{session}
Si noti che la sottrazione su numeri naturali funziona in modi che possono
sembrare non intuitivi. In particolare, la normalizzazione del coefficiente non può
occorrere quando atteso prima:
\begin{session}
\begin{verbatim}
- SIMP_CONV arith_ss [] ``2 * x + y - z + y``;
! Uncaught exception:
! UNCHANGED
\end{verbatim}
\end{session}
Sui numeri naturali, l'espressione $2 x + y - z + y$  non è
uguale a $2 x + 2 y - z$. In particolare, queste espressioni non sono
uguali quando  $2x + y < z$.

\subsubsection{\ml{list\_ss}}
%
\index{list_ss (insieme di semplificazione)@\ml{list\_ss} (insieme di semplificazione)}
%
L'ultimo valore \simpset{} puro in \ml{bossLib}, \ml{list\_ss} aggiunge
teoremi di riscrittura circa il tipo delle liste a \ml{arith\_ss}. Queste
riscritture includono i fatti ovvi circa i costruttori del tipo lista
\holtxt{NIL} e \holtxt{CONS}, come il fatto che \holtxt{CONS} è
iniettivo:
\begin{hol}
\begin{verbatim}
   (h1 :: t1 = h2 :: t2) = (h1 = h2) /\ (t1 = t2)
\end{verbatim}
\end{hol}
Opportunamente, \ml{list\_ss} include anche delle riscritture per le funzioni
definite per ricorsione primitiva sulle liste. Esempi includono
\holtxt{MAP}, \holtxt{FILTER} e \holtxt{LENGTH}. Così:
\begin{session}
\begin{verbatim}
- SIMP_CONV list_ss [] ``MAP (\x. x + 1) [1;2;3;4]``;
> val it = |- MAP (\x. x + 1) [1; 2; 3; 4] = [2; 3; 4; 5] : thm

- SIMP_CONV list_ss [] ``FILTER (\x. x < 4) [1;2;y + 4]``;
> val it = |- FILTER (\x. x < 4) [1; 2; y + 4] = [1; 2] : thm

- SIMP_CONV list_ss [] ``LENGTH (FILTER ODD [1;2;3;4;5])``;
> val it = |- LENGTH (FILTER ODD [1; 2; 3; 4; 5]) = 3 : thm
\end{verbatim}
\end{session}
Questi esempi dimostrano come il semplificatore può essere usato come un valutatore
simbolico di scopo generale per termini che assomigliano in grande misura a quelli
che appaiono in un linguaggio di programmazione funzionale. Si noti che
questa funzionalità è fornita anche da \ml{computeLib} (si veda
la Sezione~\ref{sec:computeLib} di sotto); \ml{computeLib} è più
efficiente, ma meno generale del semplificatore. Per esempio:
\begin{session}
\begin{verbatim}
- EVAL ``FILTER (\x. x < 4) [1;2;y + 4]``;
> val it =
    |- FILTER (\x. x < 4) [1; 2; y + 4] =
       1::2::(if y + 4 < 4 then [y + 4] else []) : thm
\end{verbatim}
\end{session}

\subsubsection{Il \simpset{} ``stateful''---\ml{srw\_ss()}}
\label{sec:srw_ss}
\index{srw_ss (insieme di semplificazione)@\ml{srw\_ss} (insieme di semplificazione)}

L'ultimo \simpset{} esportato da \ml{bossLib} è nascosto dietro una
funzione. Il valore \ml{srw\_ss} ha il tipo \ml{unit -> simpset}, così
che si deve digitare \ml{srw\_ss()}  per ottenere un valore \simpset{}.
Questo uso di un tipo funzione permette al \simpset{} sottostante di essere
archiviato in una reference \ML{}, e permette ad esso di essere aggiornato
dinamicamente. In questo modo, la trasparenza referenziale è deliberatamente
spezzata. Tutti gli altri \simpset{} si comporteranno sempre in modo identico:
\ml{SIMP\_CONV~bool\_ss} è la stessa routine di semplificazione ovunque
e ogni volta che è chiamata.

Per contro, \ml{srw\_ss} è progettata per essere aggiornata. Quando una teoria è
caricata, quando un nuovo tipo è definito. il valore dietro \ml{srw\_ss()}
cambia, e il comportamento di \ml{SIMP\_CONV} applicato a
\ml{(srw\_ss())} cambia con esso. La filosofia di sviluppo dietro
\ml{srw\_ss} è che essa dovrebbe essere sempre una ragionevole prima scelta in
tutte le situazioni dove il semplificatore è utilizzato.

Questa versatilità è illustrata nel seguente esempio:
\begin{session}
\begin{verbatim}
- Hol_datatype `tree = Leaf | Node of num => tree => tree`;
<<HOL message: Defined type: "tree">>
> val it = () : unit

- SIMP_CONV (srw_ss()) [] ``Node x Leaf Leaf = Node 3 t1 t2``;
<<HOL message: Initialising SRW simpset ... done>>
> val it =
    |- (Node x Leaf Leaf = Node 3 t1 t2) =
       (x = 3) /\ (Leaf = t1) /\ (Leaf = t2) : thm

- load "pred_setTheory";
> val it = () : unit

- SIMP_CONV (srw_ss()) [] ``x IN { y | y < 6}``;
> val it = |- x IN {y | y < 6} = x < 6 : thm
\end{verbatim}
\end{session}
%
Gli utenti possono aumentare il \simpset{} stateful da soli con la funzione
%
\begin{holboxed}
\index{export_rewrites@\ml{export\_rewrites}}
\begin{verbatim}
   BasicProvers.export_rewrites : string list -> unit
\end{verbatim}
\end{holboxed}
Le stringhe passate a \ml{export\_rewrites} sono i nomi di teoremi
nell'attuale segmento di teoria (quelli che saranno esportati quando
\ml{export\_theory} è chiamata). Non solo questi teoremi sono aggiunti
al \simpset{} sottostante nella sessione attuale, ma essi saranno
aggiunti nelle sessioni future quando la teoria è ricaricata.
\begin{session}
\begin{verbatim}
- val tsize_def = Define`
  (tsize Leaf = 0) /\
  (tsize (Node n t1 t2) = n + tsize t1 + tsize t2)
`;
Definition has been stored under "tsize_def".
> val tsize_def =
    |- (tsize Leaf = 0) /\
       !n t1 t2. tsize (Node n t1 t2) = n + tsize t1 + tsize t2 : thm

- val _ = BasicProvers.export_rewrites ["tsize_def"];

- SIMP_CONV (srw_ss()) [] ``tsize (Node 4 (Node 6 Leaf Leaf) Leaf)``;
> val it = |- tsize (Node 4 (Node 6 Leaf Leaf) Leaf) = 10 : thm
\end{verbatim}
\end{session}

Come regola generale, \ml{(srw\_ss())} include tutte le ``riscritture ovvie''
del suo contesto, così come codice per fare calcoli standard
(come l'aritmetica esegita nell'esempio di sopra). Non
include procedure di decisione che possono esibire performance occasionalmente
povere, così i frammenti \simpset{} che contengono queste procedure
dovrebbero essere aggiunte manualmente a quelle invocazioni di semplificazione che ne
hanno bisogno.

\subsection{Frammenti \simpset{}}
\label{sec:simpset-fragments}
\index{semplificazione!frammenti simpset}

Il frammento \simpset{} è il blocco base di costruzione usato per
costruire valori \simpset{}. C'è una funzione base che
esegue questa costruzione:
\begin{hol}
\begin{verbatim}
   op ++  : simpset * ssfrag -> simpset
\end{verbatim}
\end{hol}
dove \ml{++} è un infisso. In generale, è meglio costruire sopra
il \simpset{} \ml{pure\_ss} o uno dei sui discendenti al fine di
selezionare la funzione ``filtro'' di default per convertire teoremi in
regole di riscrittura. (Questo processo di filtro è descritto di sotto nella
Sezione~\ref{sec:generating-rewrite-rules}.)

Per le teorie principali (o gruppi di esse), una collezione di
frammenti \simpset{} rilevanti si trova di solito nel modulo \ml{<thy>Simps},
dove \ml{<thy>} è il nome della teoria. Per esempio, i frammenti
\simpset{}  per la teoria dei numeri naturali si trovano in
\ml{numSimps}, e i frammenti per le liste si trovano in \ml{listSimps}.

Alcuni frammenti \simpset{} standard della distribuzione sono descritti
nella Tabella~\ref{table:ssfrags}. Questi ed altri frammenti \simpset{}
sono descritti in maggior dettaglio nelle \REFERENCE.

\begin{table}[htbp]
\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lp{0.65\textwidth}}
\ml{BOOL\_ss} &
Riscritture standard per gli operatori booleani
(congiunzione, negazione \&c), così come una conversione per eseguire
$\beta$-riduzioni.  (In \ml{boolSimps}.)
\\
\ml{CONG\_ss} & Regole di congruenza per l'implicazione e le espressioni
condizionali. (In \ml{boolSimps}.)
\\
\ml{ARITH\_ss} &
La procedura di decisione sui numeri naturali
per l'aritmetica universale Presburger. (In \ml{numSimps}.)
\\
\ml{PRED\_SET\_AC\_ss} & Normalizzazione-AC per unioni e intersezioni
su insiemi. (In \ml{pred\_setSimps}.)
\end{tabular}
\end{center}
\caption{Alcuni dei frammenti \simpset{} standard di \HOL{}}
\label{table:ssfrags}
\end{table}

I frammenti \simpset{} in definitiva sono costruiti con il costruttore
\ml{SSFRAG}:
\begin{hol}
\begin{verbatim}
   SSFRAG : {
     convs  : convdata list,
     rewrs  : thm list,
     ac     : (thm * thm) list,
     filter : (controlled_thm -> controlled_thm list) option,
     dprocs : Traverse.reducer list,
     congs  : thm list,
     name   : string option
   } -> ssfrag
\end{verbatim}
\end{hol}
Una descrizione completa per i vari campi del record passato a
\ml{SSFRAG}, e il loro significato è dato in \REFERENCE. La
funzione \ml{rewrites} fornisce una strada semplice per costruire un
frammento che include solo una lista di riscritture:
\begin{hol}
\begin{verbatim}
   rewrites : thm list -> ssfrag
\end{verbatim}
\end{hol}

\subsection{Riscrittura con il semplificatore}
\label{sec:simplifier-rewriting}

La riscrittura è l'``operazione core'' del semplificatore. Questa sezione
descrive l'azione di riscrittura in maggior dettaglio.


\subsubsection{Riscrittura di base}
\label{sec:basic-rewriting}

Data una regola di riscrittura della forma \[
\vdash \ell = r
\]
il semplificatore eseguirà una scansione dall'alto verso il basso del termine di input $t$,
cercando per dei \emph{match}~(si veda la Sezione~\ref{sec:simp-homatch} di sotto)
di $\ell$ all'interno di $t$. Questo match occorrerà in un sotto termine di $t$
(chiamiamolo $t_0$) e restituirà un'istanziazione. Quando questa
istanziazione è applicata alla regola di riscrittura, il risultato sarà una nuova
equazione della forma \[
\vdash t_0 = r'
\]
Poiché il sistema a quel punto ha un teorema che esprime un'equivalenza per
$t_0$ può creare la nuova equazione \[
  \vdash \underbrace{(\dots t_0\dots)}_t = (\dots r' \dots)
\]
L'attraversamento del termine da semplificare è ripetuto fino a quando non si trova
alcun match ulteriore per le regole di riscrittura del semplificatore. La
strategia di attraversamento è
\begin{enumerate}
\item \label{enum:simp-traverse-toplevel}%
  Mentre c'è un qualsiasi match per le regole di riscrittura archiviate a questo livello,
	si continui ad applicarle. \emph{Non} si può fare affidamento sull'ordine in cui le regole
	di riscrittura sono applicate, eccetto quando un \simpset{}
	include due riscritture con esattamente gli stessi lati sinistri. la
	riscrittura aggiunta per ultima sarà quella preferita per match. (Questo permette una
	certa misura di overloading della riscrittura nella costruzione dei
	\simpset{}.)
%% may not wish to own up to above detail
\item \label{enum:simp-traverse-recurse}%
  Eseguire una ricorsione sui sotto termini del termine. Il modo in cui i termini sono
	attraversati a questo passo può essere controllato da \emph{regole di
		congruenza}~(una caratteristica avanzata, si veda la Sezione~\ref{sec:simp-congruences}
	di sotto)
\item Se il passo~\ref{enum:simp-traverse-recurse} ha cambiato il termine
	completamente, si provi un'altra fase di riscrittura a questo livello. Se questo fallisce,
	o se non c'è stato alcun cambiamento dall'attraversamento dei sotto-termin, si provi
	qualsiasi procedura di decisione incorporata (si veda
	la Sezione~\ref{sec:simp-embedding-code}). Se la fase di riscrittura o
	qualsiasi delle procedure di decisione ha alterato il termine, ri ritorni al
	passo~\ref{enum:simp-traverse-toplevel}. Altrimenti di termini.
\end{enumerate}

\subsubsection{Riscrittura condizionale}
\index{semplificazione!riscrittura condizionale}

La descrizione di sopra è una leggera semplificazione dello stato reale delle
cose. Una caratteristica particolarmente potente del semplificatore è che
essa realmente usa regole di riscrittura \emph{condizionali}. Questi sono teoremi
della forma
\[
\vdash P \Rightarrow (\ell = r)
\]
Quando il semplificatore trova un match per il termine $\ell$ durante il suo attraversamento
del termine, tenta di scaricare la condizione $P$. Se il
semplificatore può semplificare il termine $P$ a vero, allora l'istanza di
$\ell$ nel termine attraversato può essereo sostituito dall'appropriata
istanziazione di $r$.

Quando si semplifica $P$ (un termine che non necessariamente nemmeno occorre
nell'originale), il semplificatore si può trovare ad applicare un'altra
regola di riscrittura condizionale. Per fermare eccessive applicazioni
ricorsive, il semplificatore tiene traccia di uno stack di tutte le
condizioni collaterali su cui sta lavorando. Il semplificatore smette la dimostrazione
su una condizione collaterale se nota una ripetizione in questo stack.
C'è anche una variabile accessibile dall'utente, \ml{Cond\_rewr.stack\_limit}
che specifica la dimensione massima dello stack che il semplificatore può
di usare.

Le riscritture condizionali possono essere estremamente utili. Per esempio, i teoremi
circa la divisione e il modulo sono spesso accompagnate da condizioni
che richiedono che il divisore non sia zero. Il semplificatore può spesso
scaricare queste, in particolare se include una procedura di decisione
aritmetica. Per esempio, il teorema \ml{MOD\_MOD} dalla teoria
\ml{arithmetic} afferma
\[
\vdash 0 < n \;\Rightarrow \; (k\,\textsf{MOD}\,n)\,\textsf{MOD}\,n = k
\,\textsf{MOD}\,n
\]
Il semplificatore (in modo specifico, \ml{SIMP\_CONV~arith\_ss~[MOD\_MOD]})
può usare questo teorema per semplificare il termine
\holtxt{(k~MOD~(x~+~1))~MOD~(x~+~1)}: la procedura di decisione
aritmetica può dimostrare che \holtxt{0 < x + 1}, giustificando la riscrittura.

Benché le regole riscrittura condizionali siano potenti, non ogni teorema della
forma descritta di sopra è una scelta appropriata. Una riscrittura scelta male
può causare un considerevole degrado delle performance del semplificatore, dal momento
che perde tempo nel tentare di dimostrare condizioni collaterali impossibili. Per
esempio, il semplificatore non è molto bravo a trovare testimoni
esistenziali. Questo significa che la riscrittura condizionale \[
\vdash x < y \land y < z \Rightarrow (x < z = \top)
\]
non funzionerà come si potrebbe sperare. In generale, il semplificatore non è un
buon strumento per eseguire ragionamenti di transitività. (Si provi invece uno strumento
al primo ordine come \ml{PROVE\_TAC})

\subsubsection{Generare regole di riscrittura per i teoremi}
\label{sec:generating-rewrite-rules}
\index{teoremi equazionali, nella logica HOL@teoremi equazionali, nella logica \HOL{}!uso dei ... nel semplificatore}

Ci sono due strade per cui un teorema per la riscrittura può essere passato al
semplificatore: o come un argomento esplicito a una delle funzioni
\ML{} (\ml{SIMP\_CONV}, \ml{ASM\_SIMP\_TAC} ecc) che prendono liste di
teoremi come argomenti, o includendoli in un frammento \simpset{}
che è incorporato in un \simpset. In entrambi i casi, questi teoremi sono
trasformati prima di essere usati. Le trasformazioni applicate sono
progettate per rendere l'uso interattivo quanto più conveniente possibile.

In particolare, non è necessario passare al semplificatore teoremi
che sono esattamente della forma
\[
\vdash P \Rightarrow (\ell = r)
\]
Piuttosto, il semplificatore trasformerà i suoi teoremi di input per estrarre
le riscritture di questa stessa forma. L'esatta trasformazione eseguita è
dipendente dal \simpset{} utilizzato: ciascun \simpset{} contiene la sua
propriat funzione ``filtro'' che è applicata ai teoremi che sono aggiunti ad
esso. La maggior parte dei \simpset{} usano la funzione filtro dal \simpset{}
\ml{pure\_ss} (si veda la Sezione~\ref{sec:purebool-ss}). Tuttavia, quando un
frammento \simpset{} è aggiunto a un simpset completo, il frammento può
specificare un componente filtro addizionale. Se specificata, questa funzione
è di tipo \ml{controlled\_thm~->~controlled\_thm~list}, ed è applicata
ad ognuno dei teoremi prodotti dal filtro del \simpset{} esistente.
%
\index{semplificazione!garantire la terminazione}
(Un teorema ``controllato'' è uno che è accompagnato da un pezzo di
dati di ``controllo'' che esprimono il limite (se ne esiste uno) sul numero di volte
che può essere applicato. Si veda la Sezione~\ref{sec:simp-special-rewrite-forms}
per come gli utenti possono introdurre questi limiti. Il tipo di ``controllo''
appare nel modulo \ML{} \ml{BoundedRewrites}.)

Il filtro che produce riscritture in \ml{pure\_ss} elimina
le congiunzioni, le implicazioni e le quantificazioni universali fino a quando o ha
un teorema di eguaglianza, o qualche altra forma booleana. Per esempio,
il teorema \ml{ADD\_MODULUS} afferma
\[
\vdash
\begin{array}[t]{l}
(\forall n\;x.\;\;0 < n \Rightarrow ((x + n)\,\textsf{MOD}\,n =
 x\,\textsf{MOD}\,n)) \;\;\land\\
(\forall n\;x.\;\;0 < n \Rightarrow ((n + x)\,\textsf{MOD}\,n =
 x\,\textsf{MOD}\,n))
\end{array}
\]
Questo teorema si trasforma in due regole di riscrittura \[
\begin{array}{l}
\vdash 0 < n \Rightarrow ((x + n)\,\textsf{MOD}\,n = x\,\textsf{MOD}\,n)\\
\vdash 0 < n \Rightarrow ((n + x)\,\textsf{MOD}\,n = x\,\textsf{MOD}\,n)
\end{array}
\]

Se guardando a un'eguaglianza dove ci sono delle variabili sul
lato destro che non occorrono sul lato sinistro, il
semplificatore trasforma questo nella regola \[
\vdash (\ell = r) = \top
\]
Analogamente, se è una negazione booleana $\neg P$, diventa la regola \[
\vdash P = \bot
\]
e altre formule booleane $P$ diventano \[
\vdash P = \top
\]

Infine, se guardando a un'eguaglianza il cui lato sinistro è esso stesso
un'eguaglianza, e dove il lato destro non è a sua volta un'eguaglianza,
il semplificatore trasforma $(x = y) = P$ nelle due regole
\[
\begin{array}{l}
\vdash (x = y) = P\\
\vdash (y = x) = P
\end{array}
\]
Questo è generalmente utile. Per esempio, un teorema come
\[
\vdash \neg(\textsf{SUC}\,n = 0)
\]
farà sì che il semplificatore riscriva entrambi $(\textsf{SUC}\,n = 0)$ e
$(0 = \textsf{SUC}\,n)$ a falso.

La restrizione che il lato destro di una tale regola non sia esso stesso
un'eguaglianza è una semplifice euristica che previene alcune forme di looping.


\subsubsection{Regole di riscrittura di matching}
\label{sec:simp-homatch}

Dato un teorema di riscrittura $\vdash \ell = r$, il primo passo di
esecuzione di una riscrittura è determinare se $\ell$ può o meno essere
istanziata così da renderla uguale al termine che è
riscritto. Questo processo è conosciuto come matching. Per esempio, se $\ell$
è il termine $\textsf{SUC}(n)$, allora farne il matching verso il termine
$\textsf{SUC}(3)$ avrà successo, e troverà l'istanziazione $n\mapsto
3$. Contrariamente all'unificazione, il matching non è simmetrico: un
pattern $\textsf{SUC}(3)$ non matcherà il termine $\textsf{SUC}(n)$.

\index{matching di ordine superiore} \index{matching!di ordine superiore} Il semplificatore
usa una speciale forma di matching di ordine superiore. Se un pattern
include una variabile di qualche tipo funzione (diciamo $f$), e quella variabile
è applicata a un argomento $a$ che non include alcuna variabile eccetto quelle
che sono legate da un'atrazione ad uno spopo più alto. allora il termine
combinato $f(a)$ matcherà qualsiasi termine del tipo appropriato fino a quando le
sole occorrenze delle variabili legate in $a$ sono in sotto-termini
che matchano $a$.

Assumiamo per i seguenti esempi che la variabile $x$ sia legata a uno
scopo più alto. Allora, se $f(x)$ deve matchare $x + 4$, la variabile $f$
sarà istanziata a $(\lambda x.\; x + 4)$. Se $f(x)$ deve matchare
$3 + z$, allora $f$ sarà istanziata a $(\lambda x.\;3 + z)$.
Inoltre $f(x + 1)$ matcha $x + 1 < 7$, ma non matcha $x + 2 <
7$.

Un matching di ordine superiore di questo genere rende più facile esprimere i risultati
dei movimenti dei quantificatori come regole di riscrittura, e avere queste regole applicate dal
semplificatore. Per esempio, il teorema
\[
\vdash (\exists x. \;P(x)\lor Q(x)) = (\exists x.\;P(x)) \lor (\exists
x.\;Q(x))
\]
ha due variabili di un tipo funzione ($P$ e $Q$), ed entrambe sono
applicate alla variabile legata $x$. Questo significa che quando applicato
all'input \[
\exists z. \;z < 4 \lor z + x = 5 * z
\]
il matcher troverà l'istanziazione \[
\begin{array}{l}
P \mapsto (\lambda z.\;z < 4)\\
Q \mapsto (\lambda z.\;z + x = 5 * z)
\end{array}
\]

Eseguendo questa istanziazione, e poi facendo qualche $\beta$-riduzione
sulla regola di riscrittura, si produce il teorema \[
\vdash (\exists z. \;z < 4 \lor z + x = 5 * z) =
(\exists z. \;z < 4) \lor (\exists z.\;z + x = 5 * z)
\]
come richiesto.

Un altro esempio di una regola che il semplificatore userà con successo è
\[
\vdash f \circ (\lambda x.\; g(x)) = (\lambda x.\;f(g(x)))
\]
La presenza dell'astrazione sul lato sinistro della regola
richiede che un'astrazione appaia nel termine da matchare, così questa
regola può essere vista come un'implementazione di un metodo per muovere le astrazioni
al di sopra delle composizioni di funzione.

Un esempio di un possibile lato sinistro che \emph{non} matchare come
in generale si potrebbe desiderare è $(\exists x.\;P(x + y))$. Questo è
perché il predicato $P$ è applicato a un argomento che include la
variabile libera $y$.

\subsection{Caratteristiche avanzate}
\label{sec:advanced-simplifier}

Questa sezione descrive alcune delle caratteristiche avanzate del semplificatore.

\subsubsection{Regole di congruenza}
\label{sec:simp-congruences}
\index{semplificazione!regole di congruenza}
\index{regole di congruenza!nella semplificazione}
Le regole di congruenza controllano il modo in cui il semplificatore traversa un termine.
Esse forniscono anche un meccanismo per mezzo del quale possono essere
aggiunte al contesto del semplificatore assunzioni aggiuntive, che rappresentano informazione circa il
contesto contenitore. Le regole di congruenza più semplici sono incorporate nel
simpset \ml{pure\_ss}. Esse specificano come traversare termini applicazione e
astrazione. A questo livello fondamentale, queste regole di congruenza
sono poco di più che le regole d'inferenza \ml{ABS}
\[
\frac{\Gamma \turn t_1 = t_2}
{\Gamma \turn (\lquant{x}t_1) = (\lquant{x}t_2)}
\]
(dove $x\not\in\Gamma$) e \ml{MK\_COMB}
\[
\frac{\Gamma \turn f = g \qquad \qquad \Delta \turn x = y}
{\Gamma \cup \Delta \turn f(x) = g(y)}
\]
Quando si specifica l'azione del semplificatore, queste regole dovrebbero essere
lette verso l'alto. Con \ml{ABS}, per esempio, la regola dice ``quando
si semplifica un'astrazione, si semplifichi il corpo $t_1$ a qualche nuovo $t_2$,
e poi il risultato è formato ri-astraendo con la variabile
legata~$x$.''

Ulteriori regole di congruenza dovrebbero essere aggiunte al semplificatore nella forma
di teoremi, attraverso il campo \ml{congs} dei record passati al
costruttore \ml{SSFRAG}. Tali regole di congruenza dovrebbero essere della forma
\[
\mathit{cond_1} \Rightarrow \mathit{cond_2} \Rightarrow \dots (E_1 =
E_2)
\]
dove $E_1$ è la forma da riscrivere. Ciascun $\mathit{cond}_i$ può
essere o una formula booleana arbitraria (nel qual caso è trattata come
una condizione collaterale da scaricare) o un'equazione della forma generale
\[
\forall \vec{v}. \;\mathit{ctxt}_1 \Rightarrow \mathit{ctxt}_2
\Rightarrow \dots (V_1(\vec{v}) = V_2(\vec{v}))
\]
dove la variabile $V_2$ deve occorrere libera in $E_2$.

Per esempio, la forma teorema di \ml{MK\_COMB} sarebbe
\[
\vdash (f = g) \Rightarrow (x = y) \Rightarrow (f(x) = g(y))
\]
e la forma teorema di \ml{ABS} sarebbe
\[
\vdash (\forall x. \;f (x) = g (x)) \Rightarrow (\lambda x. \;f(x)) = (\lambda
x.\;g(x))
\]
La forma per \ml{ABS} dimostra come è possibile per le regole
di congruenza gestire variabili legate. Dal momento che le regole di congruenza sono
matchate con il match di ordine superiore della Sezione~\ref{sec:simp-homatch},
questa regola matcherà tutti i possibili termini astrazione.

Questi semplici esempi non hanno ancora dimostrato l'uso delle
condizioni $\mathit{ctxt}$ su sotto-equazioni. Un esempio di questo è
la regola di congruenza (che si trova in \ml{CONG\_ss}) per le implicazioni. Questa
afferma
\[
\vdash (P = P') \Rightarrow (P' \Rightarrow (Q = Q')) \Rightarrow
(P \Rightarrow Q = P' \Rightarrow Q')
\]
Questa regola dovrebbe essere letta: ``Quando si semplifica $P\Rightarrow Q$, prima
si semplifichi $P$ a $P'$. Poi si assuma $P'$, e si semplifichi $Q$ a $Q'$.
Poi il risultato è $P' \Rightarrow Q'$.''

La regola per espressioni condizionali è
\[
\vdash \begin{array}[t]{l}
  (P = P') \Rightarrow (P' \Rightarrow (x = x')) \Rightarrow
  (\neg P' \Rightarrow (y = y')) \;\Rightarrow\\
       (\textsf{if}\;P\;\textsf{then}\;x\;\textsf{else}\;y =
       \textsf{if}\;P'\;\textsf{then}\;x'\;\textsf{else}\;y')
\end{array}
\]
Questa regola permette di assumere la guardia quando si semplifica il
ramo-vero del condizionale, e di assumere la sua negazione quando si
semplifica il ramo-falso.

Le assunzioni contestuali da regole di congruenza sono trasformate in
riscritture usando il meccanismo descritto nella
Sezione~\ref{sec:generating-rewrite-rules}.

Le regole di congruenza possono essere usate per ottenere un numero di interessanti
effetti. Per esempio, una congruenza può specificare che i sotto-termini
\emph{non} siano semplificati se lo si desidera. Questo potrebbe essere usato per impedire
la semplificazione dei rami delle espressioni condizionali:
\[
\vdash (P = P') \Rightarrow
       (\textsf{if}\;P\;\textsf{then}\;x\;\textsf{else}\;y =
       \textsf{if}\;P'\;\textsf{then}\;x\;\textsf{else}\;y)
\]
Se aggiunta al semplificatore, questa regola prenderà la precedenza su qualsiasi
altra regola per le espressioni condizionali (mascherando quella sopra
\ml{CONG\_ss}, diciamo) e farà sì che il semplificatore di scendere solo
nella guardia. Con le riscritture standard (da \ml{BOOL\_ss}):
\[
\begin{array}{l}
\vdash \;\textsf{if}\;\top\;\textsf{then}\;x\;\textsf{else}\;y \,\;=\,\; x\\
\vdash \;\textsf{if}\;\bot\;\textsf{then}\;x\;\textsf{else}\;y \,\;=\,\; y
\end{array}
\]
gli utenti possono scegliere che il semplificatore ignori
i rami di un condizionali fino a quando la guarda di quel condizionale è semplificata
o a vero o a falso.


\subsubsection{Normalizzazione-AC}
\index{simplification!AC-normalisation}

The simplifier can be used to normalise terms involving associative
and commutative constants.  This process is known as
\emph{AC-normalisation}.  The simplifier will perform AC-normalisation
for those constants which have their associativity and commutativity
theorems provided in a constituent \simpset{} fragment's \ml{ac}
field.

For example, the following \simpset{} fragment will cause
AC-normalisation of disjunctions
\begin{hol}
\begin{verbatim}
   SSFRAG {ac = [(DISJ_ASSOC, DISJ_COMM)],
           rewrs = [], filter = NONE, convs = [],
           dprocs = [], congs = []}
\end{verbatim}
\end{hol}
The pair of provided theorems must state
\begin{eqnarray*}
x \oplus y &=& y \oplus x\\
x \oplus (y \oplus z) &=& (x \oplus y) \oplus z
\end{eqnarray*}
for a constant $\oplus$.  The theorems may be universally quantified,
and the associativity theorem may be oriented either way.  Further,
either the associativity theorem or the commutativity theorem may be
the first component of the pair.  Assuming the \simpset{} fragment
above is bound to the \ML{} identifier \ml{DISJ\_ss}, its behaviour is
demonstrated in the following example:
\begin{session}
\begin{verbatim}
- SIMP_CONV (bool_ss ++ DISJ_ss) [] ``p /\ q \/ r \/ P z``;
<<HOL message: inventing new type variable names: 'a>>
> val it = |- p /\ q \/ r \/ P z = r \/ P z \/ p /\ q : thm
\end{verbatim}
\end{session}

\index{arith_ss (insieme di semplificazione)@\ml{arith\_ss} (insieme di semplificazione)}
L'ordine degli operandi nella forma normale-AC in cui lavora la normalizzazione-AC
del semplificatore non è specificato. Tuttavia, la forma
normale è sempre associata a destra. Si noti inoltre che il \simpset{}
\ml{arith\_ss}, e il frammento \ml{ARITH\_ss} che ne è la base, ha
le sue procedure di normalizzazione su misura per l'addizione sui
numeri naturali. Combinare la normalizzazione-AC, come descritta qui, con
\ml{arith\_ss} può far sì che il semplificatore entri in un loop infinito.

I teoremi AC possono essere aggiunti ai \simpset{} anche attraverso la lista di teoremi parte
dell'interfaccia della tattica e della conversione, usando la forma speciale di riscrittura
\ml{AC}:
\begin{session}
\begin{verbatim}
- SIMP_CONV bool_ss [AC DISJ_ASSOC DISJ_COMM] ``p /\ q \/ r \/ P z``;
<<HOL message: inventing new type variable names: 'a>>
> val it = |- p /\ q \/ r \/ P z = r \/ P z \/ p /\ q : thm
\end{verbatim}
\end{session}
Si veda la Sezione~\ref{sec:simp-special-rewrite-forms} per maggiori informazioni su speciali
forme di riscrittura.

\subsubsection{Incorporate codice}
\label{sec:simp-embedding-code}

Il semplificatore dispone di due modi diversi in cui il codice dell'utente può essere
incorporato nel suo attraversamento e semplificazione dei termini di input. Incorporando
il loro proprio codice gli utenti possono personalizzare il comportamento del
semplificatore in una misura significativa.

\paragraph{Conversioni utente}
Il più semplice dei due metodi permente al semplificatore di includere
conversioni fornite dall'utente. Queste sono aggiunte ai \simpset{} nel
campo {convs} dei frammenti \simpset{}. Questo campo prende liste di
valori di tipo
\begin{hol}
\begin{verbatim}
   { name: string,
    trace: int,
      key: (term list * term) option,
     conv: (term list -> term -> thm) -> term list -> term -> thm}
\end{verbatim}
\end{hol}

I campo \ml{name} e \ml{trace} sono usati quando il tracing del semplificatore
è acceso. Se la conversione è applicata, e se il livello di traccia del
semplificatore è maggiore di o uguale al campo \ml{trace}, allora sarà
emesso un messagio circa l'applicazione della conversione
(che include il suo \ml{name}).

Il campo \ml{key} del record di sopra è usato per specificare i
sotto-termini a cui la conversione dovrebbe essere applicata. Se il valore è
\ml{NONE}, allora la conversione sarà provata ad ogni posizione.
Altrimenti, la conversione è applicata a posizioni di termine che matchano il
pattern fornito. Il primo componente del pattern è una lista di
variabili che dovrebbero essere trattate come costanti quando si cercano match
del pattern. Il secondo componente è il pattern di termine stesso. Il matching
verso questa componente \emph{non} è fatto dal match di ordine superiore della
Sezione~\ref{sec:simp-homatch}, ma da una ``term-net'' di ordine superiore.
Questa forma di matching non cerca di essere precisa; essa è usata per
eliminare in modo efficiente match chiaramente impossibili. Non controlla
i tipi, e non controlla binding multipli. Questo significa che la
conversione non sarà applicata a termini che sono match esatti
per il pattern fornito.

Infine, la conversione stessa. Molti usi di questa infrastruttura sono per aggiungere
normali conversioni \HOL{} (di tipo \ml{term->thm}), e questo può essere
fatto ignorando i primi due parametri del campo \ml{conv} Per una
conversione \ml{myconv}, l'idioma standard è di scrivere
\ml{K~(K~myconv)}. Se l'utente lo desidera, tuttavia, il suo codice
\emph{può} riferirsi ai primi due parametri. Il secondo parametro è
lo stack delle condizioni collaterali che sono state tentate fino ad ora. Il
primo permette al codice dell'utente di richiamare il semplificatore, passando
lo stack delle condizioni collaterali, e una nuova condizione per risolvere.
L'argomento \ml{term} deve essere di tipo \holtxt{:bool}, e la chiamata
ricorsiva lo semplificherà a vero (e richiamerà \ml{EQT\_ELIM} per trasformare un termine
$t$ nel teorema $\vdash t$). Questa restrizione può essere sollevata in una
futura versione di \HOL{} ma per come stanno le cose ora, la chiamata ricorsiva può
essere usata \emph{solo} per scaricare la condizione collaterale. Si noti anche che
è responsabilità dell'utente passare uno stack appropriatamente aggiornato di
condizioni collaterali all'invocazione ricorsiva del semplificatore.

Una conversione fornita dall'utente non dovrebbe mai restituire l'identità riflessiva
(un'istanza di $\vdash t = t$). Questo farà andare in loop il
semplificatore. Piuttosto che restituire un tale risultato, si sollevi un \ml{HOL\_ERR} o
un'eccezione \ml{Conv.UNCHANGED}. (Entrambe sono trattate allo stesso modo dal semplificatore.)

\paragraph{Procedure di decisione consapevoli del contesto}
Un altro metodo, più complicato, per incorporare codice utente nel
semplificatore è \emph{attraverso} il campo \ml{dprocs} della struttura
frammento \simpset{}. Questo metodo è più generale rispetto ad aggiungere
conversioni, e permette anche al codice utente di costruire e mantenere i suoi
propri contesti logici costruiti su misura.

Il campo \ml{dprocs} richiede liste di valori di tipo
\ml{Traverse.reducer}. Questi valori sono costruiti con il
costruttore \ml{REDUCER}:
\begin{hol}
\begin{verbatim}
   REDUCER : {initial : context,
              addcontext : context * thm list -> context,
              apply : {solver : term list -> term -> thm,
                       context : context,
                       stack : term list} -> term -> thm}
          -> reducer
\end{verbatim}
\end{hol}
Il tipo \ml{context} è un alias per il tipo \ML{} incorporato
\ml{exn}, quello delle eccezioni. Le eccezioni sono qui usate come un
``tipo universale'', capace di archiviare dati di qualsiasi tipo. Per esempio,
se il dato desiderato è una coppia di un intero e un booleano, allora si
può fare la seguente dichiarazione:
\begin{hol}
\begin{verbatim}
   exception my_data of int * bool
\end{verbatim}
\end{hol}
Non è necessario rendere questa dichiarazione visibile con uno scopo
ampio. Infatti, solo le funzioni che accedono e creano contesti di questa
forma hanno bisogno di vederla. Per esempio:
\begin{hol}
\begin{verbatim}
  fun get_data c = (raise c) handle my_data (i,b) => (i,b)
  fun mk_ctxt (i,b) = my_data(i,b)
\end{verbatim}
\end{hol}

Quando crea un valore di tipo \ml{reducer}, l'utente deve fornire un
contesto iniziale, e due funzioni. La prima, \ml{addcontext}, è
chiamata dal meccanismo di attraversamento del semplificatore per dare ad ogni
procedura di decisione incorporata accesso ai teoremi che rappresentano la nuova informazione
di contesto. Per esempio, questa funzione è chiamata con i teoremi dalle
assunzioni attuali in \ml{ASM\_SIMP\_TAC}, e con i teoremi
dagli argomenti lista di teoremi a tutt le varie funzioni di
semplificazione. Mentre un termine è attraversato, le regole di congruenza che governano
questo attraversamento possono fornire teoremi addizionali; questi saranno passati
anche alla funzione \ml{addcontext}. (Naturalmente, è del tutto a carico
della funzione \ml{addcontext} come questi teoremi saranno
gestiti; essi possono persino essere ignorati completamente.)

Quando un riduttore è applicato a un termine, è richiamata la funzione
\ml{apply} fornita. Oltre che al termine da trasformare, la
funzione \ml{apply} è passata anche a un record che contiene un
risolutore di condizione collaterale, l'attuale contesto della procedura di decisione. e
allo stack delle condizioni collaterali tentate fino ad ora. Lo stack e il risolutore
sono gli stessi degli argomenti addizionali dati alle conversioni fornite
dall'utente. La potenza dell'astrazione del riduttore è di avere accesso a
un contesto che può essere costruito in modo appropriato per ogni procedura di decisione.

Le procedure di decisione sono applicate per l'ultima volta quando un termine è incontrato dal
semplificatore. Inoltre, esse sono applicate \emph{dopo} che il semplificatore ha
giaà eseguito un ricorsione in ogni sotto-termine e ha tentato di fare quante più riscritture
possibili. Questo significa che benché la riscrittura del semplificatore avvenda in un maniera che va
dall'alto verso il basso, le procedure di decisione saranno applicate dal basso verso l'alto e
solo come ultima risorsa.

Come per le conversioni-utente, le procedure di decisione devono sollevare un'eccezione
piuttosto che restituire istanze di riflessività.

\subsubsection{Forme speciali di riscrittura}
\label{sec:simp-special-rewrite-forms}

Si può accedere ad alcune delle caratteristiche del semplificatore in un modo
relativamente semplice usando funzioni \ML{} per costruire speciali forme
di teorema. Questi teoremi speciali possono poi essere passati negli
argomenti lista-di-teoremi delle tattiche di semplificazione.

A due delle caratteristiche avanzate del semplificatore, la normalizzazione-AC e
le regole di congruenza si può accedere in questo modo. Piuttosto che costruire un
frammento \simpset{} custom che include le regole AC o di congruenza
richieste, l'utente può usare piuttosto le funzioni \ml{AC} o \ml{Cong}:
\begin{hol}
\begin{verbatim}
   AC : thm -> thm -> thm
   Cong : thm -> thm
\end{verbatim}
\end{hol}
Per esempio, se il valore teorema
\begin{hol}
\begin{verbatim}
   AC DISJ_ASSOC DISJ_COMM
\end{verbatim}
\end{hol}
appare tra i teoremi passati a una tattica di semplificazionie, allora
il semplificatore eseguirà una normalizzazione-AC di disgiunzioni. La
funzione \ml{Cong} fornisce un'interfaccia analoga per l'aggiunta di
nuove regole di congruenza.

\index{semplificazione!garantire la terminazione}
\index{Once (controllo delle applicazioni di riscrittura)@\ml{Once} (controllo delle applicazioni di riscrittura)|pin}
\index{Ntimes (controllo delle applicazioni di riscrittura)@\ml{Ntimes} (controllo delle applicazioni di riscrittura)|pin}
Altre due funzioni forniscono un meccanismo crudo per controllare il
numero di volte che una riscrittura individuale sarà applicata.
\begin{hol}
\begin{verbatim}
   Once : thm -> thm
   Ntimes : thm -> int -> thm
\end{verbatim}
\end{hol}
Un teorema ``avvolto'' nella funzione \ml{Once} sarà applicato
una sola volta quando il semplificatore è applicato a un termine dato. Un teorema
avvolto in \ml{Ntimes} sarà applicato tante volte quante sono date nel
parametro intero.

\paragraph{Semplificare a particolari sotto-termini}
\index{semplificazione!a particolari sotto-termini}
Abbiamo già visto (Sezione~\ref{sec:simp-congruences} di sopra) che
la tecnologia di congruenza del semplificatore può essere usata per forzare il
semplificatore ad ignorare termini particolari. L'esempio nella sezione
di sopra ha discusso come un regola di congruenza potrebbe essere usata per assicurare che
solo le guardie delle espressioni condizionali dovrebbero essere semplificate.

In molte dimostrazioni, è comune voler riscrivere solo un lato o
l'altro di un connettivo binario (spesso, questo connettivo è
un'eguaglianza). Per esempio, questo capita quando si riscrive con equazioni
da complicate definizioni ricorsive che non sono soltanto ricorsioni
strutturali. In tali definizioni, il lato sinistro dell'equazioni
avrà un simbolo funzione attaccato a una sequenza di variabili, ad esempio:
\begin{hol}
\begin{verbatim}
   |- f x y = ... f (g x y) z ...
\end{verbatim}
\end{hol}
Teoremi di una forma analoga sono anche restituiti come i teoremi
``casi'' dalle definizioni induttive.

Qualunque sia la loro origine, tali teoremi sono il classico esempio di
qualcosa a cui si vorrebbe attaccare il qualificatore \ml{Once}.
Tuttavia, questo può non essere sufficiente: si può desiderare di dimostrare un risultato
come
\begin{hol}
\begin{verbatim}
   f (constructor x) y = ... f (h x y) z ...
\end{verbatim}
\end{hol}
(Con le relazioni, il goal può spesso rappresentare un'implicazione al posto di
un'eguaglianza.) In questa situazioni, spesso si vuole semplicemente espandere
l'istanza di \holtxt{f} sulla sinistra, lasciando l'altra occorrenza
da sola. Usare \ml{Once} espanderà solo una di esse, ma senza
specificare quale deve essere espansa.

La soluzione a questo problema è di usare speciali regole di congruenza,
costruite come forme speciali che possono essere passate come teoremi come
\ml{Once}. Le funzioni
\begin{hol}
\begin{verbatim}
   SimpL : term -> thm
   SimpR : term -> thm
\end{verbatim}
\end{hol}
costruiscono regole di congruenza per forzare la riscritture alla sinistra o alla destra di
termini particolari. Per esempio, se \holtxt{opn} è un operatore binario,
\ml{SimpL~\holquote{(opn)}} restituisce \ml{Cong} applicato al teorema
\begin{hol}
\begin{verbatim}
   |- (x = x') ==> (opn x y = opn x' y)
\end{verbatim}
\end{hol}
\index{SimpLHS@\ml{SimpLHS}|pin}\index{SimpRHS@\ml{SimpRHS}|pin}
Dal momento che il caso eguaglianza è così comune, gli speciali valori
\ml{SimpLHS} e \ml{SimpRHS} sono forniti per forzare
la semplificazione sulla sinistra o sulla destra di un'eguaglianza rispettivamente.
Queste sono definite essere solo applicazioni di \ml{SimpL} e \ml{SimpR}
all'eguaglianza.

Si noti che queste regole si applicano per tutto un termine, non soltanto
all'occorrenza più alta di un operatore. Inoltre, l'operatore più alto nel
termine non ha bisogno di essere quello della regola di congruenza. Questo comportamento è
una conseguenza automatica dell'implementazione in termini di regole
di congruenza.

\subsubsection{Limitare la semplificazione}
\label{sec:limit-simpl}

\index{semplificazione!garantire la terminazione}
Oltre alle forme-di-teoremi \ml{Once} and \ml{Ntimes} appena
discusse, che limitano il numero di volte che una particolare riscrittura è
applicata, il semplificatore può anche essere limitato nel numero totale di
riscritture che esso esegue. La funzione \ml{limit} (in \ml{simpLib} e
\ml{bossLib})
\begin{hol}
\begin{verbatim}
   limit : int -> simpset -> simpset
\end{verbatim}
\end{hol}
registra un limite numerico in un \simpset{}. Quando un \simpset{} limitato
poi lavora su un termine, non applicherà mai più del numero dato
di riscritture a quel termine. Quando sono usate riscritture condizionali, la
riscrittura fatta nello scaricamento delle condizioni collaterali pesano negativamente sul
limite, finché la riscrittura è infine applicata. Anche l'applicazione
delle regole di congruenza, delle conversioni e delle procedure
di decisione fornite dall'utente pesano tutte negativamente sul limite.

Quando il semplificatore cede il controllo a una conversione o a una procedura di
decisione fornita dall'utente non può garantire che queste funzioni restituiranno
alla fine un risultato (e inoltre esse possono prendere un tempo arbitrariamente lungo per terminare, spesso una preoccupazione
con le procedure di decisione aritmetica), ma l'uso di \ml{limit} è
altrimenti un buon metodo per assicurare che la semplificazione termina.

\subsubsection{Riscrittura con pre-ordini arbitrari}
\label{sec:preorder-rewriting}
\index{semplificazione!con i pre-ordini}

Oltre a semplificare rispetto all'eguaglianza, è anche
possibile usare il semplificatore per ``riscrivere'' rispetto a una relazione
che è riflessiva e transitiva (un \emph{preordine}). Questo può essere un
modo molto potente di lavorare con relazioni di transizione nella semantica
operazionale.

{\newcommand{\bred}{\ensuremath{\rightarrow^*_\beta}}

  Si immagini, per esempio, che si debba impostare un ``profondo incorporamento'' del
	$\lambda$-calcolo. Questo implicherà la definizione di un nuovo tipo
	(diciamo, \texttt{lamterm}) all'interno della logica, così come le definizioni delle
	funzioni appropriate (ad esempio, la sostituzione) e le relazioni su
	\texttt{lamterm}. E' probabile che si lavori con la chiusura riflessiva
	e transitiva della $\beta$-riduzione (\bred). Questa relazione ha
	regole di congruenza come
\[
\begin{array}{c@{\qquad\qquad}c}
\infer{M_1 \,N\;\bred\;M_2\,N}{M_1 \;\bred\;M_2} &
\infer{M \,N_1\;\bred\;M\,N_2}{N_1 \;\bred\;N_2}\\[3mm]
\multicolumn{2}{c}{\infer{(\lambda v.M_1)\;\bred\;(\lambda v.M_2)}{M_1\;\bred M_2}}
\end{array}
\] e un'importante riscrittura
\[
\infer{(\lambda v. M)\,N \;\bred\; M[v := N]}{}
\]
Dovendo applicare queste regole manualmente mostrare che un
dato termine iniziale si può ridurre a una particolare destinazione è di solito
molto doloroso, coinvolgendo molte applicazioni, non solo quelle dei teoremi
di sopra, ma anche quelle dei teoremi che descrivono la chiusura riflessiva e
transitiva (si veda la Sezione~\ref{relation}).

Benché il $\lambda$-calcolo sia non-deterministico, esso è anche confluente, così
vale il seguente teorema:
\[
\infer{
  M_1 \;\bred\;N\;\;=\;\;M_2\;\bred\; N
}{
  \beta\textrm{-nf}\;N & M_1 \;\bred\;M_2
}
\]
Questo è il teorema critico che giustifica il cambio dalla riscrittura
con l'eguaglianza alla riscrittura con \bred. Esso dice che se si ha un termine
$M_1\bred N$, dove $N$ è una forma $\beta$-normale, e se $M_1$ riscrive a
$M_2$ sotto \bred, allora il termine originale è uguale a $M_2\bred N$.
Avendo fortuna, $M_2$ sarà sintatticamente identico a $N$, e
la riflessività di \bred{} dimostrerà il risultato desiderato. I teoremi
come questi, che giustificano il cambio da una relazione di riscrittura a
un'altra sono conosciuti come \emph{congruenze d'indebolimento}.

Quando aggiustato in modo appropriato, il semplificatore può essere modificato per sfruttare
i cinque teoremi di sopra, e dimostrare automaticamente risultati come
\[
u ((\lambda f\,x. f (f\,x)) v) \bred u (\lambda x. v(v\,x))
\]
(sotto le assunzioni che i termini $u$ e $v$ siano variabili del
$\lambda$-calcolo, rendendo il risultato come una forma $\beta$-normale).

Inoltre, si avranno probabilmente vari teoremi di riscrittura
che si vorranno usare oltre a quelli specificati di sopra. Per
esempio, se in precedenza si è dimostrato un teorema come
\[
K\,x\,y \bred x
\]
allora il semplificatore può prendere anche questo in considerazione.

La funzione che ottiene tutto questo è
\index{add_relsimp@\ml{add\_relsimp}}
\begin{verbatim}
   simpLib.add_relsimp  : {trans: thm, refl: thm, weakenings: thm list,
                           subsets: thm list, rewrs : thm list} ->
                          simpset -> simpset
\end{verbatim}
I campo del record che è il primo argomento sono:
\begin{description}
\item[\texttt{trans}] Il teorema che afferma che la relazione è
	transitiva, nella forma $\forall x y z. R\,x\,y \land R\,y\,z \Rightarrow R x z$.
\item[\texttt{refl}] Il teorema che afferma che la relazione è
	riflessiva, nella forma $\forall x. R\,x\,x$.
\item[\texttt{weakenings}] Una lista di congruenze d'indebolimento, della
	forma generale $P_1 \Rightarrow P_2 \Rightarrow \cdots (t_1 = t_2)$, dove almeno una delle
	$P_i$ menzionerà presumibilmente la nuova relazione $R$ applicata a una
	variabile che appare in $t_1$. Altri
	antecedenti possono essere condizioni collaterali come il requisito
	nell'esempio di sopra che il termine $N$ sia in forma $\beta$-normale.
\item[\texttt{subsets}] Teoremi della forma $R'\,x\,y \Rightarrow R\,x\,y$.
	Questi sono usati per aumentare il risultante ``filtro'' del \simpset{} così che
	i teoremi nel contesto che menziona $R'$ deriveranno utili riscritture
	che coinvolgono $R$. Nell'esempio della $\beta$-riduzione, si potrebbe avere anche
	una relazione $\rightarrow_{wh}^*$ per la riduzione weak-head. Qualsiasi riduzione
	weak-head è anche una $\beta$-riduzione, così può essere utile anche avere che
	il semplificatore ``promuova'' automaticamente i fatti circa la riduzione weak-head
	a fatti circa la $\beta$-riduzione, e poi li usi come riscritture.
\item[\texttt{rewrs}] Possibilmente riscritture condizionali, presumibilmente la maggior parte
	della forma $P \Rightarrow R\,t_1\,t_2$. Qui possono anche essere
	incluse riscritture sull'eguaglianza, che permettono di includere utili fatti aggiuntivi. Per
	esempio, quando si lavora con il $\lambda$-calcolo, si potrebbe includere sia
	la riscrittura per $K$ di sopra, così come la definizione della
	sostituzione.
\end{description}
} % end of block defining \bred

L'applicazione di questa funzione a un \simpset{} \texttt{ss}
produrrà un \texttt{ss} aumentato che ha tutti i comportamenti del
\texttt{ss} esistente, così come la capacità di riscrivere con la relazione
data.


\index{semplificazione|)}

\section{Efficient Applicative Order Reduction---\texttt{computeLib}}
\label{sec:computeLib}

La Sezione~\ref{sec:datatype} e la Sezione~\ref{TFL} mostrano la capacità di
\HOL{} di rappresentare molti dei costrutti standard della programmazione
funzionale. Se si vuole quindi `eseguire' programmi funzionali su
argomenti, ci sono molte scelte. Primo, si può applicare il
semplificatore, come mostrato nella Sezione~\ref{sec:simpLib}. Questo permette
di esercitare tutto il potere del processo di riscrittura,
inclusa, per esempio, l'applicazione delle procedure di decisione per
dimostrare vincoli sulle regole di riscrittura condizionale. Secondo, si potrebbe
scrivere il programma, e tutti i programmi da cui dipende in modo transitivo,
in un file in una sintassi concreta adatta, e invocare un compilatore o
un interprete. Questa funzionalità è disponibile in \HOL{} attraverso l'uso di
\ml{EmitML.exportML}.

Terzo, si può usare \ml{computeLib}. Questa libreria supporta una valutazione
call-by-value delle funzioni \HOL{} per passi deduttivi. In altre parole, è
molto simile ad avere un interprete \ML{} all'interno della logica \HOL{},
lavorando per inferenza in avanti. Quando usati in questo modo, i programmi
funzionali possono essere eseguiti più velocemente che usando il semplificatore.

Gli entry-point più accessibili per usare la libreria \ml{computeLib}
sono la conversione \ml{EVAL} e la sua tattica corrispondente
\ml{EVAL\_TAC}. Queste dipendono su un database interno che archivia
definizioni di funzione. Nel seguente esempio, caricare \ml{sortingTheory}
aumenta questo database con le definizioni rilevanti, in particolare quella
di Quicksort (\holtxt{QSORT}), e poi possiamo valutare
\holtxt{QSORT} su una lista concreta.
%
\setcounter{sessioncount}{0}
\begin{session}
\begin{verbatim}
  - load "sortingTheory";

  - EVAL ``QSORT (<=) [76;34;102;3;4]``;
  > val it = |- QSORT $<= [76; 34; 102; 3; 4] = [3; 4; 34; 76; 102] : thm
\end{verbatim}
\end{session}
Spesso, l'argomento a una funzione non ha variabili: in quel caso
l'applicazione di \ml{EVAL} dovrebbe restituire un risultato ground,
come nell'esempio di sopra. Tuttavia, \ml{EVAL} può anche valutare funzioni su
argomenti con variabili---la cosiddetta valutazione \emph{simbolica}---e
in quel caso, il comportamento di \ml{EVAL} dipende dalla struttura
dell'equazioni di ricorsione. Per esempio, nella seguente sessione, se c'è
sufficiente informazione nell'input, la valutazione simbolica può restituire
un risultato interessante. Tuttavia, se nell'input non c'è informazione
sufficiente a permettere all'algoritmo alcuna presa, non avrà luogo
alcuna espansione
%
\begin{session}
\begin{verbatim}
  - EVAL ``REVERSE [u;v;w;x;y;z]``;
  > val it = |- REVERSE [u; v; w; x; y; z] = [z; y; x; w; v; u] : thm

  - EVAL ``REVERSE alist``;
  > val it = |- REVERSE alist = REVERSE alist : thm
\end{verbatim}
\end{session}
%

\subsection{Trattare con la divergenza}

La difficoltà maggiore con l'uso di \ml{EVAL} è la terminazione. Troppo
spesso, la valutazione simbolica con \ml{EVAL} divergerà, o genererà
termini enormi. Il caso usuale sono i condizionali con le variabili nel
test. Per esempio, la seguente definizione è probabilmente uguale a \holtxt{FACT},
%
\begin{session}
\begin{verbatim}
  Define `fact n = if n=0 then 1 else n * fact (n-1)`;
  > val it = |- fact n = (if n = 0 then 1 else n * fact (n - 1)) : thm
\end{verbatim}
\end{session}
%
Ma le due definizioni sono valutate in modo completamente differente.
%
\begin{session}
\begin{verbatim}
  EVAL ``FACT n``;
  > val it = |- FACT n = FACT n : thm

  - EVAL ``fact n``;
  <.... interrupt key struck ...>
  > Interrupted.
\end{verbatim}
\end{session}
%
La definizione ricorsiva primitiva di \holtxt{FACT} non si espande
per nulla, mentre la ricorsione stile-decostruttore di \holtxt{fact} non smette mai
di espandere. Un rudimentale strumento di monitoraggio mostra il comportamento, prima
su un argomento ground, poi su un argomento simbolico.
%
\begin{session}
\begin{verbatim}
  - val [fact] = decls "fact";
  - computeLib.monitoring := SOME (same_const fact);

  - EVAL ``fact 4``;
  fact 4 = (if 4 = 0 then 1 else 4 * fact (4 - 1))
  fact 3 = (if 3 = 0 then 1 else 3 * fact (3 - 1))
  fact 2 = (if 2 = 0 then 1 else 2 * fact (2 - 1))
  fact 1 = (if 1 = 0 then 1 else 1 * fact (1 - 1))
  fact 0 = (if 0 = 0 then 1 else 0 * fact (0 - 1))
  > val it = |- fact 4 = 24 : thm

  - EVAL ``fact n``;
  fact n = (if n = 0 then 1 else n * fact (n - 1))
  fact (n - 1) = (if n - 1 = 0 then 1 else (n - 1) * fact (n - 1 - 1))
  fact (n - 1 - 1) =
  (if n - 1 - 1 = 0 then 1 else (n - 1 - 1) * fact (n - 1 - 1 - 1))
  fact (n - 1 - 1 - 1) =
  (if n - 1 - 1 - 1 = 0 then
     1
   else
     (n - 1 - 1 - 1) * fact (n - 1 - 1 - 1 - 1))
     .
     .
     .
\end{verbatim}
\end{session}
%
In ogni espansione ricorsiva, il testo coinvolge una variabile, e di conseguenza
non può essere ridotta a \holtxt{T} o a \holtxt{F}. Così, l'espansione
non si ferma mai.

Some simple remedies can be adopted in trying to deal with
non-terminating symbolic evaluation.

Si possono adottare alcuni semplici rimedi nel provare a trattare con
una valutazione simbolica che non termina.
\begin{itemize}
\item \ml{RESTR\_EVAL\_CONV} si comporta come \ml{EVAL} eccetto
	che prende una lista extra di costanti. Durante
	la valutazione, se si incontra una delle costanti fornite, essa
	non sarà espansa. Questo permette di valutare giù fino a un livello specificato.
	e può essere usato per tagliare alcune valutazioni circolari.
\item anche \ml{set\_skip} può essere usata per controllare
	la valutazione. Si veda la voce per \ml{CBV\_CONV} in \REFERENCE{} per
	una discussione di \ml{set\_skip}

\end{itemize}

\paragraph{Valutatori custom}

Per alcuni problemi, è desiderabile costruire un valutatore
personalizzato, specializzato su un insieme fissato di definizioni. Il tipo
\ml{compset} che si trova in \ml{computeLib} è il tipo dei database di definizione. Le
funzioni \ml{new\_compset}, \ml{bool\_compset}, \ml{add\_funs}, e
\ml{add\_convs} forniscono il modo standard per costruire tali
database. Un altro \holtxt{compset} piuttosto utile è
\ml{reduceLib.num\_compset}, che può essere usato per valutare
termini con numeri e booleani. Dato un \ml{compset}, la funzione
\ml{CBV\_CONV} genera un valutatore: è usata per implementare \ml{EVAL}.
Si veda \REFERENCE{} per maggiori dettagli.

\paragraph{Trattare con Funzioni sui Numeri di Peano}

Le funzioni definite per pattern-matching su numeri nello stile di Peano non sono
nel formato giusto per \ml{EVAL}, dal momento che i calcoli saranno
asintoticamente inefficienti. Piuttosto, la stessa definizione dovrebbe essere
usata su numerali, che è una notazione posizionale descritta nella
Sezione~\ref{sec:numerals}. Tuttavia, è preferibile per le dimostrazioni
lavorare su numeri di Peano. Per colmare questa lacuna, la funzione
\ml{numLib.SUC\_TO\_NUMERAL\_DEFN\_CONV} è usata per convertire una funzione
su numeri di Peano in una su numerali, che è il formato che
\ml{Eval} preferisce. \ml{Define} chiamerà automaticamente
\ml{SUC\_TO\_NUMERAL\_DEFN\_CONV} sul suo risultato.

\paragraph{Archiviare le definizioni}

\ml{Define} aggiunge automaticamente la sua definizione al compset globale
usato da \ml{EVAL} e \ml{EVAL\_TAC}. Tuttavia, quando \ml{Hol\_defn} è
usata per definire una funzione, le sue equazioni di definizione non sono aggiunte al
compset globale fino a quando \ml{tprove} è usata per dimostrare i vincoli
di terminazione. Inoltre, \ml{tprove} non aggiunge la definizione
in modo persistente nel compset globale. Di conseguenza, si deve usare
\ml{add\_persistent\_funs} in una teoria per essere sicuri che le definizioni
fatte da \ml{Hol\_defn} siano disponibili a \ml{Eval} nelle teorie
discendenti. Un altro punto: si deve chiamare \ml{add\_persistent\_funs}
prima di chiamare \ml{export\_theory}.


\section{Le Librerie Aritmetiche---\texttt{numLib}, \texttt{intLib} and \texttt{realLib}}
\label{sec:numLib}
\index{procedure di decisione!Aritemtica Presburger su numeri naturali}

Ognuna delle librerie aritmetiche di \HOL{} fornisce una
suite di definizioni e teoremi così come il supporto per l'inferenza automatica.

\paragraph{numLib}

I numeri più di base in \HOL{} sono i numeri naturali. La
libreria \ml{numLib} comprende le teorie \ml{numTheory},
\ml{prim\_recTheory}, \ml{arithmeticTheory}, e \ml{numeralTheory}.
Questa libreria incorpora anche un valutatore per espressioni numeriche
da \ml{reduceLib} e una procedura di decisione per l'aritmetica lineare
\ml{ARITH\_CONV}. Il valutatore e la procedura di decisione sono
integrati nel simpset \ml{arith\_ss} usato dal semplificatore.
Allo stesso modo, la procedura di decisione dell'aritmetica lineare può essere invocata
direttamente attraverso \ml{DECIDE} e \ml{DECIDE\_TAC}, che si trovano entrambe in
\ml{bossLib}.


\index{procedure di decisione!Aritemtica Presburger su interi}
\paragraph{intLib}

La libreria \ml{intLib} comprende \ml{integerTheory}, un'estesa
teoria degli interi, più due procedure di decisione
per la completa aritmetica Presburger. Queste sono disponibili come
\ml{intLib.COOPER\_CONV} e \ml{intLib.ARITH\_CONV}. Queste
procedure di decisione sono in grado di trattare con l'aritmetica lineare
sugli interi e i numeri naturali, così come di trattare
con un'alternanza arbitraria di quantificatori. La procedura
\ml{ARITH\_CONV} è un'implementazione dell'Omega Test, e sembra
in generale avere migliori performance rispetto all'algoritmo di Cooper. Ci sono
comunque problemi per cui questo non è vero, così è utile avere disponibili
entrambe le procedure.

\paragraph{realLib}

La libreria \ml{realLib} fornisce uno sviluppo fondazionale
dei numeri reali e dell'analisi. Si veda la Sezione~\ref{reals}
per una rapida descrizione delle teorie.
E' anche fornita una teoria dei polinomi, in \theoryimp{polyTheory}.
Una procedura di decisione per l'aritmetica lineare sui numeri reali
è anche fornita da \ml{realLib}, sotto il nome \ml{REAL\_ARITH\_CONV}
e \ml{REAL\_ARITH\_TAC}.

\section{Libreria Bit Vector---\texttt{wordsLib}}

La libreria \theoryimp{wordsLib} fornisce uno strumento di supporto per i bit-vectors, questo include infrastrutture per: la valutazione, il parsing, il pretty-printing e la semplificazione.

\subsection{Valutazione}

La libreria \theoryimp{wordsLib} dovrebbe essere caricata quando si valutano termini bit-vector ground. Questa libreria fornisce un \emph{compset} \ml{words\_compset}, che
può essere usato nella costruzione di \emph{compese} e conversioni personalizzati.
\setcounter{sessioncount}{0}
\begin{session}
\begin{verbatim}
- load "wordsLib";
> val it = () : unit

- EVAL ``8w + 9w:word4``;
> val it = |- 8w + 9w = 1w : thm
\end{verbatim}
\end{session}
Si noti che qui è usata un'annotazione di tipo per designare la dimensione word. Quando la dimensione word è rappresentata da una variabile di tipo (cioè per word i lunghezza arbitraria), la valutazione
può dare risultati parziali o insoddisfacenti.

\subsection{Parsing e pretty-printing}

I word possono essere parsati in binario, decimale e esadecimale. Per esempio:
\begin{session}
\begin{verbatim}
- ``0b111010w : word8``;
> val it = ``58w`` : term

- ``0x3Aw : word8``;
> val it = ``58w`` : term
\end{verbatim}
\end{session}
E' possibile fare il parsing di numeri ottali, ma questo deve essere prima abilitato impostando la reference \ml{base\_tokens.allow\_octal\_input} a true. Per esempio:
\begin{session}
\begin{verbatim}
- ``072w : word8``;
> val it = ``72w`` : term

- base_tokens.allow_octal_input:=true;
> val it = () : unit

- ``072w : word8``;
> val it = ``58w`` : term
\end{verbatim}
\end{session}

I word possono essere stampati usando le basi numeriche standard. Per esempio, la funzione
\ml{wordsLib.output\_words\_as\_bin} selezionerà il formato binario:
\begin{session}
\begin{verbatim}
- wordsLib.output_words_as_bin();
> val it = () : unit

- EVAL ``($FCP ODD):word16``;
> val it = |- $FCP ODD = 0b1010101010101010w : thm
\end{verbatim}
\end{session}
La funzione \ml{output\_words\_as} è più flessibile e permette alla base numerica di variare a seconda della
lunghezza del word e del valore numerico. Il pretty-printer di default (installato quando si carica \theoryimp{wordsLib}) stampa valori piccoli in decimale e valori grandi in esadecimale.
La funzione \ml{output\_words\_as\_oct} abiliterà automaticamente il parsing per i numeri ottali.

La variabile di traccia \ml{"word printing"} fornisce un metodo alternativo per cambiare la base numerica di output --- è particolarmente adatta per selezionare temporaneamente una base numerica, per esempio:
\begin{session}
\begin{verbatim}
- Feedback.trace ("word printing", 1) Parse.print_term ``32w``;
<<HOL message: inventing new type variable names: 'a>>
0b100000w> val it = () : unit
\end{verbatim}
\end{session}
Le scelte sono come segue: 0 (default) -- numeri piccoli in decimale, numeri grandi in esadecimale: 1 -- binario; 2 -- ottale; 3 -- decimale; e 4 -- esadecimale.

\subsubsection{Tipi}

Si può aver notato che \ty{:word4} e \ty{:word8} sono stati usati come convenienti abbreviazioni di parsing per \ty{:\bool[4]} e \ty{:\bool[8]} --- questa agevolazione è disponibile per molte dimensioni standard di word. Gli utenti che desiderano usare questa notazione per dimensioni di non-standard di word possono usare la funzione \ml{wordsLib.mk\_word\_size}:
\begin{session}
\begin{verbatim}
- ``:word15``;
! Uncaught exception:
! HOL_ERR

- wordsLib.mk_word_size 15;
> val it = () : unit

- ``:word15``;
> val it = ``:bool[15]`` : hol_type
\end{verbatim}
\end{session}

\subsubsection{Overloading degli operatori}

I simboli per le operazioni aritmetiche standard (addizione, sottrazione e moltiplicazione) sono sottoposte a overload con operatori per altre teorie standard, cioè per i numeri naturali, interi, razionali e reali. In molti casi l'inferenza di tipo risolverà l'overloading, tuttavia, in alcuni casi questo non è possibile. La scelta dell'operatore dipenderà dall'ordine in cui le teorie sono caricate. Per cambiare questo comportamento sono fornite le funzioni \ml{wordsLib.deprecate\_word} e \ml{wordsLib.prefer\_word}. Per esempio, nella seguente sessione, la selezione degli operatori word è deprecata:
\begin{session}
\begin{verbatim}
- type_of ``a + b``;
<<HOL message: more than one resolution of overloading was possible>>
<<HOL message: inventing new type variable names: 'a>>
> val it = ``:bool['a]`` : hol_type

- wordsLib.deprecate_word();
> val it = () : unit

- type_of ``a + b``;
<<HOL message: more than one resolution of overloading was possible>>
> val it = ``:num`` : hol_type
\end{verbatim}
\end{session}
Di sopra, l'addizione tra numeri naturali è scelta al posto dell'addizione word. Al contrario, i word sono preferiti rispetto agli interi di sotto:
\begin{session}
\begin{verbatim}
- load "intLib"; ...

- type_of ``a + b``;
<<HOL message: more than one resolution of overloading was possible>>
> val it = ``:int`` : hol_type

- wordsLib.prefer_word();
> val it = () : unit
- type_of ``a + b``;
<<HOL message: more than one resolution of overloading was possible>>
<<HOL message: inventing new type variable names: 'a>>
> it = ``:bool['a]`` : hol_type
\end{verbatim}
\end{session}
Naturalmente, potrebbero essere state aggiunte annotazioni di tipo per evitare questo problema completamente. Si noti che, diversamente da \ml{deprecate\_int}, la funzione \ml{deprecate\_word} non rimuove gli overloading, semplicemente abbassa la loro precedenza.

\subsubsection{Indovinare le lunghezze word}

Può essere una seccatura aggiungere annotazioni di tipo quando si specifica il tipo di ritorno per operazioni come: \holtxt{word\_extract}, \holtxt{word\_concat}, \holtxt{concat\_word\_list} e \holtxt{word\_replicate}. Questo perché c'è spesso una lunghezza ``standard'' che potrebbe essere indovinata, ad esempio la concatenazione di solito somma le lunghezze dei word costituenti. Un'agevolazione per indovinare la lunghezza word è controllata dalla reference \ml{wordsLib.guessing\_word\_lengths}, che è falsa di default. Le congetture sono eseguite durante un passo di post-processing che avviene dopo l'applicazione di \ml{Parse.Term}. Questo è mostrato di sotto.
\begin{session}
\begin{verbatim}
- wordsLib.guessing_word_lengths:=true;
> val it = () : unit

- ``concat_word_list [(4 >< 1) (w:word32); w2; w3]``;
<<HOL message: inventing new type variable names: 'a, 'b>>
<<HOL message: assigning word length: 'a <- 4>>
<<HOL message: assigning word length: 'b <- 12>>
> val it =
    ``concat_word_list [(4 >< 1) w; w2; w3]``
     : term
\end{verbatim}
\end{session}
Nell'esempio di sopra, le congetture sulla lunghezza dei word sono attivate. Sono fatte due congetture: ci si aspetta che l'estrazione dia un word di quattro bit, e che la concatenazione dia un word di dodici bit ($3 \times 4$). Se sono richieste lunghezze numeriche non standard allo si possono aggiungere delle annotazioni di tipo per evitare che siano fatte delle congetture. Quando la congettura è disattivata i tipi risultanti rimangono come variabili di tipo inventate, cioé come gli alfa e i beta di sopra.

\subsection{Semplificazione e conversioni}

Sono forniti i seguenti frammenti \emph{simpset}:
\begin{description}
\item[\ml{SIZES\_ss}] valuta un gruppo di funzioni che operano su tipi numerici, come \holtxt{dimindex} e \holtxt{dimword}.
\item[\ml{BIT\_ss}] prova a semplificare le occorrenze della funzione\holtxt{BIT}.
\item[\ml{WORD\_LOGIC\_ss}] semplifica operazioni logiche bitwise.
\item[\ml{WORD\_ARITH\_ss}] semplifica operazioni aritmetiche word. La sottrazione è sostituita con la moltiplicazione da -1.
\item[\ml{WORD\_SHIFT\_ss}] semplifica operazioni shift.
\item[\ml{WORD\_ss}] contiene tutti i frammenti di sopra, e fa anche una valutazione estra dei termini ground. Questo frammento è aggiunto a \ml{srw\_ss}.
\item[\ml{WORD\_ARITH\_EQ\_ss}] semplifica \holtxt{``a = b``} to \holtxt{``a - b = 0w``}.
\item[\ml{WORD\_BIT\_EQ\_ss}] espande in modo aggressivo operazioni bit-vector non-aritmetiche in espressioni booleane. (Dovrebbe essere usata con attenzione -- include \ml{fcpLib.FCP\_ss}.)
\item[\ml{WORD\_EXTRACT\_ss}] semplificazione per una varietà di operazioni: conversioni da word a word; concatenazione; shift e estrazione bit-field.  Può essere usata in  situationi dove \ml{WORD\_BIT\_EQ\_ss} non è adatta.
\item[\ml{WORD\_MUL\_LSL\_ss}] semplifica la moltiplicazione con un letterale word in una somma di prodotti parziali.
\end{description}
Molti di questi frammenti \emph{simpset} hanno delle conversioni corrispondenti. Per esempio, la conversione \ml{WORD\_ARITH\_CONV} è basata si \ml{WORD\_ARITH\_EQ\_ss}, tuttavia, fa del lavoro extra per assicurare che \holtxt{``a = b``} e \holtxt{``b = a``} si convertano nella stessa espressioni. Di conseguenza, questa conversione è adatta per ragionare circa l'eguaglianza delle espressioni aritmetiche word.

Il comportamento dei frammenti elencati di sopra è mostrato usando la seguente funzione:
\begin{session}
\begin{verbatim}
- fun conv ss = SIMP_CONV (pure_ss++ss) [];
> val conv = fn : ssfrag -> term -> thm
\end{verbatim}
\end{session}
La seguente sessione mostra \ml{SIZES\_ss}:
\begin{session}
\begin{verbatim}
- conv wordsLib.SIZES_ss ``dimindex(:12)``;
> val it = |- dimindex (:12) = 12 : thm

- conv wordsLib.SIZES_ss ``FINITE univ(:32)``;
> val it = |- FINITE univ(:32) <=> T : thm
\end{verbatim}
\end{session}
Il frammento \ml{BIT\_ss} converte \holtxt{BIT} in un test di appartenenza su un insieme di posizioni bit (più alte):
\begin{session}
\begin{verbatim}
- conv wordsLib.BIT_ss ``BIT 3 5``;
> val it = |- BIT 3 5 <=> (3 = 0) \/ (3 = 2) : thm

- conv wordsLib.BIT_ss ``BIT i 123``;
> val it = |- BIT i 123 <=> i IN {0; 1; 3; 4; 5; 6} :
  thm
\end{verbatim}
\end{session}
Questa semplificazione fornisce supporto per il ragionamento circa le operazioni bitwise su lunghezze word arbitrarie. I frammenti aritmetico, logico e shift aiutano a ripulire espressioni word di base:
\begin{session}
\begin{verbatim}
- conv wordsLib.WORD_LOGIC_ss ``a && 12w || 11w && a``;
<<HOL message: inventing new type variable names: 'a>>
> val it =
    |- a && 12w || 11w && a = 15w && a :
  thm

- conv wordsLib.WORD_ARITH_ss ``3w * b + a + 2w * b - a * 4w:word2``;
> val it =
    |- 3w * b + a + 2w * b - a * 4w = a + b
     : thm

- conv wordsLib.WORD_SHIFT_ss ``0w << 12 + a >>> 0 + b << 2 << 3``;
<<HOL message: inventing new type variable names: 'a>>
> val it =
    |- 0w << 12 + a >>> 0 + b << 2 << 3 = 0w + a + b << (2 + 3)
     : thm
\end{verbatim}
\end{session}

I frammenti rimanenti non sono inclusi in \ml{wordsLib.WORD\_ss} or \ml{srw\_ss}. Il frammento eguaglianza bit è mostrato di sotto.
\begin{session}
\begin{verbatim}
- SIMP_CONV (std_ss++wordsLib.WORD_BIT_EQ_ss) [] ``a && b = ~0w : word2``;
> val it =
    |- (a && b = ~0w) <=> (a ' 1 /\ b ' 1) /\ a ' 0 /\ b ' 0
     : thm
\end{verbatim}
\end{session}
Il frammento esatto è utile per il ragionamento circa operazioni bit-field ed è usato meglio in combinazione con \ml{wordsLib.SIZES\_ss} o \ml{wordsLib.WORD\_ss}, per esempio:
\begin{session}
\begin{verbatim}
- SIMP_CONV (std_ss++wordsLib.SIZES_ss++wordsLib.WORD_EXTRACT_ss) []
   ``(4 -- 1) ((a:word3) @@ (b:word2)) : word5``;
> val it =
    |- (4 -- 1) (a @@ b) = (2 >< 0) a << 1 || (1 >< 1) b
     : thm
\end{verbatim}
\end{session}
Infine, il frammento \ml{WORD\_MUL\_LSL\_ss} è mostrato di sotto.
\begin{session}
\begin{verbatim}
- conv wordsLib.WORD_MUL_LSL_ss ``5w * a : word8``;
> val it = |- 5w * a = a << 2 + a : thm
\end{verbatim}
\end{session}
Riscrivere con il teorema \ml{wordsTheory.WORD\_MUL\_LSL} fornisce un mezzo per annullare questa semplificazione, per esempio:
\begin{session}
\begin{verbatim}
- SIMP_CONV (std_ss++wordsLib.WORD_ARITH_ss) [wordsTheory.WORD_MUL_LSL]
    ``a << 2 + a : word8``;
> val it = |- a << 2 + a = 5w * a : thm
\end{verbatim}
\end{session}
Ovviamente, senza aggiungere delle garanzie, questo teorema di riscrittura non può essere dispiegato in combinazione con il frammento \ml{WORD\_MUL\_LSL\_ss}.

\subsubsection{Procedure di decisione}

Una procedura di decisione per i word è fornita nella forma di
\ml{blastLib.BBLAST\_PROVE}. Questa procedura usa il \emph{bit-blasting} ---
convertire espressioni word in proposizioni e poi usare il risolutore SAT per
decidere il goal\footnote{Questo approccio permette di dare contro-esempi
quando la negazione di un goal è insoddisfacibile.}. Questo approccio è ragionevolmente generale e
può affrontare un ampia gamma di problemi bit-vector. Tuttavia, ci sono alcune
limitazioni: l'approccio funziona solo per lunghezze word costanti, artimetica
lineare (moltiplicazione per letterali) e per shift e estrazioni
bit-field rispetto a valori letterali. Si noti inoltre che alcuni problemi saranno
potenzialmente lenti da dimostrare, ad esempio quando le dimensioni dei word sono grandi e/o quando
ci sono molte addizioni annidate (magari attraverso la moltiplicazione).


I seguenti esempi mostrano \ml{BBLAST\_PROVE} in uso:
\begin{session}
\begin{verbatim}
- blastLib.BBLAST_PROVE ``a + 2w <+ 4w = a <+ 2w \/ 13w <+ a :word4``;
> val it =
    |- a + 2w <+ 4w <=> a <+ 2w \/ 13w <+ a
     : thm

- blastLib.BBLAST_PROVE ``w2w (a:word8) <+ 256w : word16``;
> val it = |- w2w a <+ 256w : thm
\end{verbatim}
\end{session}
La procedura di decisione \ml{BBLAST\_PROVE} è basata sulla conversione
\ml{BBLAST\_CONV}. Questa conversione può essere usata per convertire problemi bit-vector
in una forma proposizionale; per esempio:
\begin{session}
\begin{verbatim}
- blastLib.BBLAST_CONV ``(((a : word16) + 5w) << 3) ' 5``;
> val it =
   |- ((a + 5w) << 3) ' 5 <=> (~a ' 2 <=> ~(a ' 1 /\ a ' 0))
   : thm
\end{verbatim}
\end{session}
Ci sono anche tattiche bit-blasting: \ml{BBLAST\_TAC} e \ml{FULL\_BBLAST\_TAC}; dove solo la seconda fa uso delle assunzioni del goal.

\section{La libreria \texttt{HolSat}}\label{sec:HolSatLib}
\input{HolSat.tex}


\section{La libreria \texttt{HolQbf}}\label{sec:HolQbfLib}
\input{HolQbf.tex}


\section{La libreria \texttt{HolSmt}}\label{sec:HolSmtLib}
\input{HolSmt.tex}

\section{La libreria \texttt{Quantifier Heuristics}}\label{sec:QuantHeuristicsLib}
\input{QuantHeuristics.tex}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "description"
%%% End:
