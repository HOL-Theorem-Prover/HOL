\chapter{Libraries}\label{HOLlibraries}

% LaTeX macros in HOL manuals
%
% \holtxt{..}     for typewriter text that is HOL types or terms.  To
%                 produce backslashes, for /\, \/ and \x. x + 1, use \bs
% \ml{..}         for typewriter text that is ML input, including the
%                 names of HOL API functions, such as mk_const
% \theoryimp{..}  for names of HOL theories.

% text inside \begin{verbatim} should be indented three spaces, unless
% the verbatim is in turn inside a \begin{session}, in which case it
% should be flush with the left margin.


\newcommand{\simpset}{simpset}
\newcommand{\Simpset}{Simpset}

 \newcommand{\term}      {\mbox{\it term}}
 \newcommand{\vstr}      {\mbox{\it vstr}}

A \emph{library} is an abstraction intended to provide a higher level
of organization for \HOL{} applications. In general, a library can
contain a collection of theories, proof procedures, and supporting
material, such as documentation. Some libraries simply provide proof
procedures, such as \ml{simpLib}, while others provide theories and
proof procedures, such as \ml{intLib}. Libraries can include other
libraries.

In the \HOL{} system, libraries are typically represented by \ML{}
structures named following the convention that library \emph{x} will
be found in the \ML{} structure \ml{xLib}. Loading this structure
should load all the relevant sub-components of the library and set
whatever system parameters are suitable for use of the library.

When the \HOL{} system is invoked in its normal configuration, several
useful libraries are automatically loaded. The most basic \HOL{}
library is \ml{boolLib}, which supports the definitions of the \HOL{}
logic, found in the theory \theoryimp{bool}, and provides a useful
suite of definition and reasoning tools.

Another pervasively used library is found in the structure \ml{Parse}
(the reader can see that we are not strictly faithful to our
convention about library naming). The parser library provides support
for parsing and `pretty-printing' of \HOL{} types, terms, and
theorems.

The \ml{boss} library provides a basic collection of standard
theories and high-level proof procedures, and serves as a standard
platform on which to work. It is preloaded and opened when the \HOL{}
system starts up. It includes \ml{boolLib} and
\ml{Parse}. Theories provided include \theoryimp{pair},
\theoryimp{sum}, \theoryimp{option}; the arithmetic theories
\theoryimp{num}, \theoryimp{prim\_rec}, \theoryimp{arithmetic},
and \theoryimp{numeral}; and \theoryimp{list}. Other libraries
included in \ml{bossLib} are \ml{goalstackLib}, which provides
a proof manager for tactic proofs; \ml{simpLib}, which provides
a variety of simplifiers; \ml{numLib}, which provides a decision
procedure for arithmetic; \ml{Datatype}, which provides
high-level support for defining algebraic datatypes; and \ml{tflLib},
which provides support for defining recursive functions.


\section{Parsing and Prettyprinting}
\label{sec:parsing-printing}

Every type and term in \HOL{} is ultimately built by application of
the primitive (abstract) constructors for types and terms. However, in
order to accommodate a wide variety of mathematical expression, \HOL{}
provides flexible infrastructure for parsing and prettyprinting types
and terms through the \ml{Parse} structure.

The term parser supports type inference, overloading, binders, and
various fixity declaration (infix, prefix, postfix, and
combinations). There are also flags for controlling the behaviour
of the parser. Further, the structure of the parser is exposed so that
new parsers can be quickly constructed to support user applications.

The parser is parameterized by grammars for types and terms. The
behaviour of the parser and prettyprinter is therefore usually altered
by grammar manipulations.
%
\index{parsing, of HOL logic@parsing, of \HOL{} logic!grammars for}
%
These can be of two kinds: \emph{temporary} or \emph{permanent}.
Temporary changes should be used in library implementations, or in
script files for those changes that the user does not wish to have
persist in theories descended from the current one.  Permanent changes
are appropriate for use in script-files, and will be in force in all
descendant theories.  Functions making temporary changes are signified
by a leading \ml{temp\_} in their names.

\subsection{Parsing types}
\index{types, in HOL logic@types, in \HOL{} logic!parsing of|(}

The language of types is a simple one.  An abstract grammar for the
language is presented in Figure~\ref{fig:abstract-type-grammar}.  The
actual grammar (with concrete values for the infix symbols and type
operators) can be inspected using the function \ml{type\_grammar}.
\begin{figure}[tbhp]
\newcommand{\nt}[1]{\mathit{#1}}
\newcommand{\tok}[1]{\texttt{\bfseries #1}}
\renewcommand{\bar}{\;\;|\;\;}
\[
\begin{array}{lcl}
\tau &::=& \tau \odot \tau \bar \nt{vtype} \bar \nt{tyop} \bar
           \tok{(} \;\nt{tylist}\;\tok{)} \;\nt{tyop}\bar \tau \;\nt{tyop}
           \bar \tok{(}\;\tau\;\tok{)} \bar \tau\tok{[}\tau\tok{]}\\
\odot &::=& \tok{->} \bar \tok{\#} \bar \tok{+} \bar \cdots\\
\nt{vtype} &::=& \tok{'a} \bar \tok{'b} \bar \tok{'c} \bar \cdots\\
\nt{tylist} &::=& \tau \bar \tau \;\tok{,}\;\nt{tylist}\\
\nt{tyop} &::=& \tok{bool} \bar \tok{list} \bar \tok{num} \bar
           \tok{fun} \bar \cdots
\end{array}
\]
\caption{An abstract grammar for \HOL{} types ($\tau$).  Infixes ($\odot$)
  always bind more weakly than type operators~($\nt{tyop}$) (and
  type-subscripting~($\tau\tok{[}\tau\tok{]}$)), so that
  $\tau_1 \,\odot\, \tau_2 \,\nt{tyop}$ is always parsed as $\tau_1\, \odot\,
  (\tau_2 \,\nt{tyop})$.  Different infixes can have different
  priorities, and infixes at different priority levels can associate
  differently (to the left, to the right, or not at all).  Users can
  extend the categories $\odot$ and $\nt{tyop}$ by making new type
  definitions, and by directly manipulating the grammar.}
\label{fig:abstract-type-grammar}
\end{figure}

\paragraph{Type infixes}
Infixes may be introduced with the function \ml{add\_infix\_type}.
This sets up a mapping from an infix symbol (such as \texttt{->}) to
the name of an existing type operator (such as \texttt{fun}).  The
binary symbol needs to be given a precedence level and an
associativity. See \REFERENCE{} for more details.

\paragraph{Type abbreviations}
\index{type abbreviations}
\index{parsing, of HOL logic@parsing, of \HOL{} logic!type abbreviations}
\index{abbreviations!for types}

Users can abbreviate common type patterns with \emph{abbreviations}.
This is done with the \ML{} function \ml{type\_abbrev}:
\begin{hol}
\begin{verbatim}
   type_abbrev : string * hol_type -> unit
\end{verbatim}
\end{hol}
An abbreviation is a new type operator, of any number of arguments,
that expands into an existing type.  For example, one might develop a
light-weight theory of numbers extended with an infinity, where the
representing type was \holtxt{num option} (\holtxt{NONE} would
represent the infinity value).  One might set up an abbreviation
\holtxt{infnum} that expanded to this underlying type.
Polymorphic patterns are supported as well.  For example, as described
in Section~\ref{sec:theory-of-sets}, the abbreviation \holtxt{set}, of
one argument, is such that \holtxt{:'a set} expands into the type
\holtxt{:'a -> bool}, for any type \holtxt{:'a}.

When types come to be printed, the expansion of abbreviations done by the parser is reversed if the \ml{type\_abbrev\_pp} entry-point is used; otherwise the abbreviation is for input only.
For more information, see \ml{type\_abbrev}'s entry in \REFERENCE.
\index{types, in HOL logic@types, in \HOL{} logic!parsing of|)}

\index{special syntactic forms for scripts!Type@\ml{Type}}
\index{Type (special syntactic form)@\ml{Type} (special syntactic form)}
There are special syntactic forms available for both type abbreviation entry-points.
Instead of
\begin{hol}
\begin{alltt}
   val _ = type_abbrev("set", \fldq:'a -> bool\frdq)
\end{alltt}
\end{hol}
one can write
\begin{hol}
\begin{alltt}
   Type set = \fldq:'a -> bool\frdq
\end{alltt}
\end{hol}
and if an underlying call to \ml{type\_abbrev\_pp} is desired, the \ml{[pp]} ``attribute'' should be added to the name, thus:
\begin{hol}
\begin{alltt}
   Type set[pp] = \fldq:'a -> bool\frdq
\end{alltt}
\end{hol}

\index{local@\ml{local}!as attribute for type abbreviations}
If a ``temporary'' abbreviation is required (one whose effect will not be apparent in descendent theories), then the \ml{local} attribute can also be added to the \ml{Type} syntax, or the \ML{} functions \ml{temp\_type\_abbrev} and \ml{temp\_type\_abbrev\_pp} can be used.


\subsection{Parsing terms}

The term parser provides a grammar-based infrastructure for supporting
concrete syntax for formalizations. Usually, the \HOL{} grammar gets
extended when a new definition or constant specification is made. (The
introduction of new constants is discussed in
Sections~\ref{sec:constant-definitions} and \ref{conspec}.) However,
any identifier can have a parsing status attached at any time.
In the following, we explore some of the capabilities of the
\HOL{} term parser.


\subsubsection{Parser architecture}
\label{sec:parser:architecture}

The parser turns strings into terms.  It does this in the following
series of phases, all of which are influenced by the provided grammar.
Usually this grammar is the default global grammar, but users can
arrange to use different grammars if they desire.  %
\index{parsing, of HOL logic@parsing, of \HOL{} logic!grammars for}
%
Strictly, parsing occurs after lexing has split the input into a
series of tokens.  For more on lexing, see Section~\ref{HOL-lex}.
\begin{description}
\item[Concrete Syntax:] Features such as infixes, binders and mix-fix
  forms are translated away, creating an intermediate, ``abstract
  syntax'' form (ML type \ml{Absyn}).  The possible fixities are
  discussed in Section~\ref{sec:parseprint:fixities} below.  Concrete
  syntax forms are added to the grammar with functions such as
  \ml{add\_rule} and \ml{set\_fixity} (for which, see the \REFERENCE).
  The action of this phase of parsing is embodied in the function
  \ml{Absyn}.

  The \ml{Absyn} data type is constructed using constructors \ml{AQ}
  (an antiquote, see Section~\ref{sec:quotation-antiquotation}); %
%
  \index{antiquotation, in HOL logic terms@antiquotation, in \HOL{} logic terms}%
%
  \ml{IDENT} (an identifier); \ml{QIDENT} (a qualified identifier,
  given as \holtxt{thy\$ident}%
\index{ dollar sign, in HOL logic parser@\ml{\$} (dollar sign, in \HOL{} logic parser)!generating qualified identifiers}%
); \ml{APP} (an application of one form
  to another); \ml{LAM} (an abstraction of a variable over a body),
  and \ml{TYPED} (a form accompanied by a type constraint\footnote{The
    types in \ml{Absyn} constraints are not full HOL types, but values
    from another intermediate type, \ml{Pretype}.}, see
  Section~\ref{sec:parseprint-type-constraints}).  At this stage of
  the translation, there is no distinction made between constants and
  variables: though \ml{QIDENT} forms must be constants, users are
  also able to refer to constants by giving their bare names.

  It is possible for names that occur in the \ml{Absyn} value to be
  different from any of the tokens that appeared in the original
  input.  For example, the input
\begin{alltt}
   \fholquote{if P then Q else R}
\end{alltt} will turn into
\begin{verbatim}
   APP (APP (APP (IDENT "COND", IDENT "P"), IDENT "Q"), IDENT "R")
\end{verbatim}
  (This is slightly simplified output: the various constructors for
  \ml{Absyn}, including \ml{APP}, also take location parameters.)

  The standard grammar includes a rule that associates the special
  mix-fix form for if-then-else expressions with the underlying
  ``name'' \holtxt{COND}.  It is \holtxt{COND} that will eventually be
  resolved as the constant \holtxt{bool\dol{}COND}.

  If the ``quotation'' syntax with a bare dollar is used,%
%
\index{ dollar sign, in HOL logic parser@\ml{\$} (dollar sign, in \HOL{} logic parser)!as escape character}%
\index{tokens!suppressing parsing behaviour of|(}%
%
then this phase of the parser will not treat strings as part of a
special form.  For example, \holtxt{\fholquote{\dol{}if~P}} turns into
the \ml{Absyn} form
\begin{verbatim}
   APP(IDENT "if", IDENT "P")
\end{verbatim}
  \emph{not} a form involving \holtxt{COND}.

  More typically, one often writes something like
  \holtxt{\fholquote{\$+~x}}, which generates the abstract syntax
\begin{verbatim}
   APP(IDENT "+", IDENT "x")
\end{verbatim}
  Without the dollar-sign, the concrete syntax parser would complain
  about the fact that the infix plus did not have a left-hand
  argument.  When the successful result of parsing is handed to the
  next phase, the fact that there is a constant called \holtxt{+} will
  give the input its desired meaning.

  Symbols can also be ``escaped'' by enclosing them in parentheses.
  Thus, the above could be written \holtxt{\fholquote{(+)~x}} for the
  same effect.
\index{tokens!suppressing parsing behaviour of|)}

  The user can insert intermediate transformation functions of their
  own design into the parsing processing at this point.  This is done
  with the function
\begin{verbatim}
   add_absyn_postprocessor
\end{verbatim}
  The user's function will be of type \ml{Absyn~->~Absyn} and can
  perform whatever changes are appropriate.  Like all other aspects of
  parsing, these functions are part of a grammar: if the user doesn't
  want to see a particular function used, they can arrange for parsing
  to be done with respect to a different grammar.

\item[Name Resolution:] The bare \ml{IDENT} forms in the \ml{Absyn}
  value are resolved as free variables, bound names or constants.
  This process results in a value of the \ml{Preterm} data type, which
  has similar constructors to those in \ml{Absyn} except with forms
  for constants. %
  \index{parsing, of HOL logic@parsing, of \HOL{} logic!preterms}%
  A string can be converted straight to a \ml{Preterm} by way of the
  \ml{Preterm} function.

  A bound name is the first argument to a \ml{LAM} constructor, an
  identifier occurring on the left-hand side of a case-expression's
  arrow, or an identifier occurring within a set comprehension's
  pattern. A constant is a string that is present in the domain of the
  grammar's ``overload map''.  Free variables are all other identifiers.
  Free variables of the same name in a term will all have the same
  type.  Identifiers are tested to see if they are bound, and then to
  see if they are constants.  Thus it is possible to write
\begin{verbatim}
   \SUC. SUC + 3
\end{verbatim}
  and have the string \holtxt{SUC} be treated as a number in the
  context of the given abstraction, rather than as the successor
  constant.

\index{parsing, of HOL logic@parsing, of \HOL{} logic!overloading}
\index{overloading|see{parsing, of \HOL{} logic, overloading}}
  The ``overload map'' is a map from strings to lists of terms.  The
  terms are usually just constants, but can be arbitrary terms (giving
  rise to ``syntactic macros'' or ``patterns'').\index{syntactic macros}
  This facility is used to allow a name such as \holtxt{+} to map to
  different addition constants in theories such as
  \theoryimp{arithmetic}, \theoryimp{integer}, and
  \theoryimp{words}. In this way the ``real'' names of the constants can
  be divorced from what the user types.  In the case of addition, the
  natural number plus actually is called \holtxt{+} (strictly,
  \holtxt{arithmetic\$+}); but over the integers, it is
  \holtxt{int\_add}, and over words it is \holtxt{word\_add}.  (Note
  that because each constant is from a different theory and thus a
  different namespace, they could all have the name \holtxt{+}.)

  \index{type inference!in HOL parser@in \HOL{} parser}
  When name resolution determines that an identifier should be treated
  as a constant, it is mapped to a preterm form that lists all of the
  possibilities for that string.  Subsequently, because the terms in
  the range of the overload map will typically have different types,
  type inference will often eliminate possibilities from the list.  If
  multiple possibilities remain after type inference has been
  performed, then a warning will be printed, and one of the
  possibilities will be chosen.  (Users can control which terms are
  picked when this situation arises.)

  When a term in the overload map is chosen as the best option, it is
  substituted into the term at appropriate position.  If the term is a
  lambda abstraction, then as many $\beta$-reductions are done as
  possible, using any arguments that the term has been applied to.  It
  is in this way that a syntactic pattern can process arguments.  (See
  also Section~\ref{sec:parser:syntactic-patterns} for more on
  syntactic patterns.)
\item[Type Inference:] %
  \index{type inference!in HOL parser@in \HOL{} parser}%
  All terms in the \HOL{} logic are well-typed.  The kernel enforces
  this through the API for the \ml{term} data type.  (In particular,
  the \ml{mk\_comb} function %
  \index{mk_comb@\ml{mk\_comb}}%
  checks that the type of the first argument is a function whose
  domain is equal to the type of the second argument.)  The parser's
  job is to turn user-supplied strings into terms.  For convenience,
  it is vital that the users do not have to provide types for all of the
  identifiers they type. (See Section~\ref{sec:parser:type-inference}
  below.)

  In the presence of overloaded identifiers, type inference may not be
  able to assign a unique type to all constants.  If multiple
  possibilities exist, one will be picked when the \ml{Preterm} is finally
  converted into a genuine term.
\item[Conversion to Term:]%
  When a \ml{Preterm} has been type-checked, the final conversion from
  that type to the \ml{term} type is mostly straightforward.  The user
  can insert further processing at this point as well, so that a
  user-supplied function modifies the result before the parser
  returns.
\end{description}

\subsubsection{Unicode characters}
\label{sec:parser:unicode-characters}

\index{Unicode|(}\index{UTF-8}
\index{parsing, of HOL logic@parsing, of \HOL{} logic!Unicode characters|(}
It is possible to have the \HOL{} parsing and printing infrastructure
use Unicode characters (written in the UTF-8 encoding).  This makes it
possible to write and read terms such as
\begin{alltt}
   \(\forall\)x. P x \(\land\) Q x
\end{alltt}
rather than
\begin{alltt}
   !x. P x /\bs{} Q x
\end{alltt}
If they wish, users may simply define constants that have Unicode
characters in their names, and leave it at that.  The problem with
this approach is that standard tools will likely then create theory
files that include (illegal) \ML{} bindings like \ml{val $\rightarrow$\_def =
  \dots}.  The result will be \ml{...Theory.sig} and
\ml{...Theory.sml} files that fail to compile, even though the call to
\ml{export\_theory} may succeed. This problem can be finessed through
the use of functions like \ml{set\_MLname}, but it's probably best
practice to only use alphanumerics in the names of constants, and to
then use functions like \ml{overload\_on} and \ml{add\_rule} to create
Unicode syntax for the underlying constant.

If users have fonts with the appropriate repertoire of characters to display their syntax, and are confident that any other users of their theories will too, then this is perfectly reasonable.
However, if users wish to retain some backwards compatibility a provide a pure ASCII syntax, they can do so by defining that pure ASCII syntax first.
Having done this, they can create a Unicode version of the syntax with the function \ml{Unicode.unicode\_version}.
\index{traces, controlling HOL feedback@traces, controlling \HOL{} feedback!Unicode in pretty-printing}
Then, either Unicode or ASCII characters can be used to input the syntax, and, while the trace variable \ml{"PP.avoid_unicode"} is $1$, the ASCII syntax will be used for printing.
If the trace is set to $0$, then again, both syntaxes can be used to \emph{write} the terms, but the pretty-printer will prefer the Unicode syntax when the terms are printed by the system.

For example, in \ml{boolScript.sml}, the Unicode character for logical
and (\texttt{$\land$}), is set up as a Unicode alternative for
\texttt{/\bs} with the call
\begin{verbatim}
   val _ = unicode_version {u = UChar.conj, tmnm = "/\\"};
\end{verbatim}
(In this context, the \ml{Unicode} structure has been \ml{open}-ed,
giving access also to the structure \ml{UChar} which contains bindings
for the Greek alphabet, and some common mathematical symbols. )

The argument to \ml{unicode\_version} is a record with fields \ml{u}
and \ml{tmnm}.  Both are strings.  The \ml{tmnm} field can either be
the name of a constant, or a token appearing in a concrete syntax rule
(possibly mapping to some other name).  If the \ml{tmnm} is only the
name of a constant, then, with the trace variable enabled, the string
\ml{u} will be overloaded to the same name.  If the \ml{tmnm} is the
same as a concrete syntax rule's token, then the behaviour is to
create a new rule mapping to the same name, but with the string \ml{u}
used as the token.

\paragraph{Lexing rules with Unicode characters}
%
\index{tokens!Unicode characters}
\index{identifiers, in HOL logic@identifiers, in \HOL{} logic!non-aggregating characters}
%
Roughly speaking, \HOL{} considers characters to be divided into three
classes: alphanumerics, non-aggregating symbols and symbols.  This
affects the behaviour of the lexer when it encounters strings of
characters.  Unless there is a specific ``mixed'' token already in the
grammar, tokens split when the character class changes.  Thus, in the
string
\begin{verbatim}
   ++a
\end{verbatim}
the lexer will see two tokens, \holtxt{++} and \holtxt{a}, because
\holtxt{+} is a symbol and \holtxt{a} is an alphanumeric.
The classification of the additional Unicode characters is very simplistic:
all Greek letters except \holtxt{$\lambda$} are alphanumeric;
the logical negation symbol \holtxt{$\neg$} is non-aggregating;
a variety of parenthetical characters (\eg, $\llparenthesis\,\rrparenthesis$) are non-aggregating, and
everything else is symbolic.
(The exception for \holtxt{$\lambda$} is to allow strings like \holtxt{$\lambda$x.x} to lex into \emph{four} tokens.)

\index{parsing, of HOL logic@parsing, of \HOL{} logic!Unicode characters|)}
\index{Unicode|)}

\subsubsection{Overloading and Syntactic Patterns (``macros'')}
\label{sec:parser:syntactic-patterns}
\label{sec:parsing:overloading}
\index{abbreviations!for terms}
\index{parsing, of HOL logic@parsing, of \HOL{} logic!overloading|(pin}
\index{parsing, of HOL logic@parsing, of \HOL{} logic!term abbreviations}
\index{parsing, of HOL logic@parsing, of \HOL{} logic!syntactic patterns|(}

As earlier alluded to, a limited amount of overloading resolution is performed by the term parser.
For example, the `tilde' symbol (\holtxt{\~{}})
denotes boolean negation in the initial theory of \HOL, and it also denotes
the additive inverse in the \ml{integer} and
\ml{real} theories.
If we load the \ml{integer} theory and enter an ambiguous term featuring \holtxt{\~{}}, the system will inform us that overloading resolution is being performed.

\setcounter{sessioncount}{0}
\begin{session}
\begin{alltt}
>> load "integerTheory";

>> Term `~~x`;

>> type_of it;
\end{alltt}
\end{session}

A priority mechanism is used to resolve multiple possible choices. In
the example, \holtxt{\~{}} could be consistently chosen to have type
\holtxt{:bool -> bool} or \holtxt{:int -> int}, and the
mechanism has chosen the former.
For finer control, explicit type constraints may be used.
In the following session, the \holtxt{\~{}\~{}x} in the first quotation has type \holtxt{:bool}, while in the second, a type constraint ensures that \holtxt{\~{}\~{}x} has type \holtxt{:int}.

\begin{session}
\begin{alltt}
>> show_types := true;

>> Term `~(x = ~~x)`;

>> Term `~(x:int = ~~x)`;
\end{alltt}
\end{session}

Note that the symbol \holtxt{\~{}} stands for two different constants in
the second quotation; its first occurrence is boolean negation, while
the other two occurrences are the additive inverse operation for
integers.

\index{special syntactic forms for scripts!Overload@\ml{Overload}}
\index{Overload (special syntactic form)@\ml{Overload} (special syntactic form)}
The prettiest way to introduce entries into the overload map is to use the \ml{Overload} syntactic form:
\begin{alltt}
   Overload name[attrs] = \fholquote{term/pattern}
\end{alltt}
(The \texttt{name} can be a bare \ML{} alpha-numeric identifier or enclosed in string-literal quotes; names that are ``symbolic'' in any way \emph{must} be enclosed in string-literal double quotes.  For example, \texttt{Overload~"-@-"~=~“...”}.)
Depending on the choice of attributes, this will map into a call to one of a variety of underlying ``overload-on'' functions.
\index{overload_on (ML function)@\ml{overload\_on} (\ML{} function)}
The default (without any attributes) calls \ml{overload\_on}, whose effect is to make an entry in the map that will be exported with the theory.
The overloading effect thus persists in descendent theories.
\index{local@\ml{local}!as overloading attribute}
If the \ml{local} attribute is used, this export doesn't occur, and the overloading effect is only visible for the rest of the containing script file.%
\footnote{The use of \ml{local} induces a call to the \ml{temp\_overload\_on} function.}

\index{inferior@\ml{inferior}!as overloading attribute}
The other attribute that can be used is \ml{inferior}, which causes the entry to be made, but makes it the pretty-printer's last choice when matching terms are printed.

All of this functionality ultimately stems from the ``overload map'' mentioned earlier.
This is actually a combination of maps, one for parsing, and one for printing.
The parsing map is from names to lists of terms, and determines how the names that appear in a \ml{Preterm} will translate into terms.
In essence, bound names turn into bound variables, unbound names not in the domain of the map turn into free variables, and unbound names in the domain of the map turn into one of the elements of the set associated with the given name.
Each term in the set of possibilities may have a different type, so type inference will choose from those that have types consistent with the rest of the given term.
If the resulting list contains more than one element, then the term appearing earlier in the list will be chosen.

The most common use-case for the overload map is have names map to constants.
In this way, for example, the various numeric theories can map the string \ml{"+"} to the relevant notions of addition, each of which is a different constant.
However, the system has extra flexibility because names can map to arbitrary terms.
For example, it is possible to map to specific type-instances of constants.  Thus, the string \ml{"<=>"} maps to equality, but where the arguments are forced to be of type \fholquote{:bool}.

Moreover, if the term mapped to is a lambda-abstraction (\ie, of the form $\lambda x.\;M$), then the parser will perform all possible $\beta$-reductions for that term and the arguments accompanying it.
For example, in \theoryimp{boolTheory} and its descendants, the string \ml{"<>"} is overloaded to the term \fholquote{\holtxt{\bs{}x~y.~\td{}(x~=~y)}}.
Additionally, \ml{"<>"} is set up at the concrete syntax level as an infix.
When the user inputs \fholquote{\holtxt{x~\lt\gt~y}}, the resulting
\ml{Absyn} value is
\begin{verbatim}
   APP(APP(IDENT "<>", IDENT "x"), IDENT "x")
\end{verbatim}
The \ml{"x"}  and \ml{"y"} identifiers will map to free variables, but
the \ml{"<>"} identifier maps to a list containing
\fholquote{\holtxt{\bs{}x~y.~\td(x~=~y)}}. This term has type
\begin{verbatim}
   :'a -> 'a -> bool
\end{verbatim}
and the polymorphic variables are generalisable, allowing type
inference to give appropriate (identical) types to \ml{x} and \ml{y}.
Assuming that this option is the only overloading for \ml{"<>"} left
after type inference, then the resulting term will be
\holtxt{\td(x~=~y)}.  Better, though this will be the underlying
structure of the term in memory, it will actually print as
\fholquote{\holtxt{x~\lt\gt~y}}.

If the term mapped to in the overload map contains any free variables,
these variables will not be instantiated in any way.  In particular,
if these variables have polymorphic types, then the type variables in
those types will be constant: not subject to instantiation by type
inference.

\paragraph{Pretty-printing and syntactic patterns} The second part of
the ``overload map'' is a map from terms to strings, specifying how
terms should be turned back into identifiers.  (Though it does not
actually construct an \ml{Absyn} value, this process reverses the name
resolution phase of parsing, producing something that is then printed
according to the concrete syntax part of the given grammar.)

Because parsing can map single names to complicated term structures,
printing must be able to take a complicated term structure back to a
single name.  It does this by performing
term matching.%
\index{matching!in pretty-printing terms}%
%
\footnote{The matching done is first-order; contrast the higher-order
  matching done in the simplifier.}
If multiple patterns match the same term, then the printer picks the
most specific match (the one that requires least instantiation of the
pattern's variables).  If this still results in multiple, equally
specific, possibilities, the most recently added pattern takes
precedence.  (Users can thus manipulate the printer's preferences by
making otherwise redundant \ml{Overload} declarations.)

In the example of the not-equal-to operator above, the pattern will be
\holtxt{\~{}(?x = ?y)}, where the question-marks indicate instantiable
pattern variables.  If a pattern includes free variables (recall that
the \ml{x} and \ml{y} in this example were bound by an abstraction),
then these will not be instantiable.

There is one further nicety in the use of this facility: ``bigger''
matches, covering more of a term, take precedence.
The difficulty this can cause is illustrated in the \holtxt{IS\_PREFIX} pattern from \theoryimp{rich\_listTheory}.
For the sake of backwards compatibility this identifier maps to
\begin{verbatim}
   \x y. isPREFIX y x
\end{verbatim}
where \holtxt{isPREFIX} is a constant from \theoryimp{listTheory}.
(The issue is that \holtxt{IS\_PREFIX} expects its arguments in reverse order to that expected by \holtxt{isPREFIX}.)
Now, when this macro is set up the overload map already contains a mapping from the string \holtxt{"isPREFIX"} to the constant \holtxt{isPREFIX} (this happens with every constant definition).
But after the call establishing the new pattern for \holtxt{IS\_PREFIX}, the \holtxt{isPREFIX} form will no longer be printed.
Nor is it enough, to repeat the call
\begin{alltt}
   Overload isPREFIX = \fholquote{isPREFIX}
\end{alltt}
Instead (assuming that \holtxt{isPREFIX} is indeed the preferred
printing form), the call must be
\begin{alltt}
   Overload isPREFIX = \fholquote{\bs{}x y. isPREFIX x y}
\end{alltt}
so that \ml{isPREFIX}'s pattern is as long as \ml{IS\_PREFIX}'s.
\index{parsing, of HOL logic@parsing, of \HOL{} logic!syntactic patterns|)}
\index{parsing, of HOL logic@parsing, of \HOL{} logic!overloading|)}


\subsubsection{Type constraints}
\label{sec:parseprint-type-constraints}

\index{type constraint!in HOL parser@in \HOL{} parser}
A term can be constrained to be of a certain type.  For example,
\holtxt{X:bool} constrains the variable \holtxt{X} to have type
\holtxt{bool}. An attempt to constrain a
term inappropriately will raise an exception: for example,
\begin{hol}
\begin{verbatim}
   if T then (X:ind) else (Y:bool)
\end{verbatim}
\end{hol}
will fail because both branches of a conditional must be of the same
type.  Type constraints can be seen as a suffix that binds more
tightly than everything except function application.  Thus $\term\
\ldots\ \term : \type$ is equal to $(\term\ \ldots\ \term) :
\type$, but $x < y:\holtxt{num}$ is a legitimate constraint on just
the variable $y$.

The inclusion of \holtxt{:} in the symbolic identifiers means that some
constraints may need to be separated by white space. For example,
\begin{hol}
\begin{verbatim}
   $=:bool->bool->bool
\end{verbatim}
\end{hol}
will be broken up by the \HOL{} lexer as
\begin{hol}
\begin{verbatim}
   $=: bool -> bool -> bool
\end{verbatim}
\end{hol}
and parsed as an application of the symbolic identifier \holtxt{\$=:} to
the argument list of terms [\holtxt{bool}, \holtxt{->}, \holtxt{bool},
\holtxt{->}, \holtxt{bool}]. A well-placed space will avoid this problem:
\begin{hol}
\begin{verbatim}
   $= :bool->bool->bool
\end{verbatim}
\end{hol}
is parsed as the symbolic identifier ``='' constrained by a type.
Instead of the \holtxt{\$}, one can also use parentheses to remove
special parsing behaviour from lexemes:
\begin{hol}
\begin{verbatim}
   (=):bool->bool->bool
\end{verbatim}
\end{hol}

\subsubsection{Type inference}
\label{sec:parser:type-inference}

\index{type inference!in HOL parser@in \HOL{} parser|(}
Consider the term \holtxt{x = T}: it (and all of its subterms)
has a type in the \HOL{} logic. Now, \holtxt{T} has type \holtxt{bool}. This
means that the constant \holtxt{=} has type \holtxt{xty -> bool -> bool},
for some type \holtxt{xty}. Since the type scheme for \holtxt{=} is
\holtxt{'a -> 'a -> bool}, we know that \holtxt{xty} must in fact be
\holtxt{bool} in order for the type instance to be well-formed. Knowing
this, we can deduce that the type of \holtxt{x} must be \holtxt{bool}.

Ignoring the jargon (``scheme'' and ``instance'') in the previous
paragraph, we have conducted a type assignment to the term structure,
ending up with a well-typed term. It would be very tedious for users
to conduct such argumentation by hand for each term entered to \HOL{}.
Thus, \HOL{} uses an adaptation of Milner's type inference algorithm
for \ML{} when constructing terms via parsing. At the end of type
inference, unconstrained type variables get assigned names by the system.
Usually, this assignment does the right thing. However, at times, the
most general type is not what is desired and the user must add type
constraints to the relevant subterms. For tricky situations, the
global variable \ml{show\_types} can be assigned. When this flag is
set, the prettyprinters for terms and theorems will show how types
have been assigned to subterms. If you do not want the system to
assign type variables for you, the global variable
\ml{guessing\_tyvars} can be set to \ml{false}, in which case the
existence of unassigned type variables at the end of type inference
will raise an exception.
\index{type inference!in HOL parser@in \HOL{} parser|)}



\subsubsection{Fixities}
\label{sec:parseprint:fixities}

In order to provide some notational flexibility, constants come in various flavours or {\it fixities}: besides being an ordinary constant (with no fixity), constants can also be {\it binders}, {\it prefixes}, {\it suffixes}, {\it infixes}, or {\it closefixes}.
More generally, terms can also be represented using reasonably arbitrary {\it mixfix} specifications.
The degree to which terms bind their associated arguments is known as precedence.
The higher this number, the tighter the binding.
For example, when introduced, \verb-+- has a precedence of 500, while the tighter binding multiplication (\verb+*+) has a precedence of 600.

\paragraph{Binders}

A binder is a construct that binds a variable; for example, the
universal quantifier. In \HOL, this is represented using a trick that
goes back to Alonzo Church: a binder is a constant that takes a lambda
abstraction as its argument. The lambda binding is used to implement
the binding of the construct. This is an elegant and uniform solution.
Thus the concrete syntax \verb+!v. M+ is represented by the
application of the constant \verb+!+ to the abstraction \verb+(\v. M)+.

The most common binders are \verb+!+, \verb+?+, \verb+?!+, and
\verb+@+. Sometimes one wants to iterate applications of the same
binder, \eg,
\begin{alltt}
   !x. !y. ?p. ?q. ?r. \term.
\end{alltt}
This can instead be rendered
\begin{alltt}
   !x y. ?p q r. \term.
\end{alltt}

\paragraph{Infixes}

Infix constants can associate in one of three different ways: right,
left or not at all.  (If \holtxt{+} were non-associative, then
\holtxt{3 + 4 + 5} would fail to parse; one would have to write
\holtxt{(3 + 4) + 5} or \holtxt{3 + (4 + 5)} depending on the desired
meaning.)
The precedence ordering for the initial set of infixes is \holtxt{/\bs}, \holtxt{\bs/}, \holtxt{==>}, \holtxt{=}, \begin{Large}\holtxt{,}\end{Large} (comma\footnote{When \theoryimp{pairTheory} has been loaded.}).
Of these, equality is non-associative, and the remainder are right associative. Thus
\begin{hol}
\begin{verbatim}
   X /\ Y ==> C \/ D, P = E, Q
\end{verbatim}
\end{hol}
%
is equal to
%
\begin{hol}
\begin{verbatim}
   ((X /\ Y) ==> (C \/ D)), ((P = E), Q).
\end{verbatim}
\end{hol}
%
\noindent An expression
\[
\term \; \holtxt{<infix>}\; \term
\]
is internally represented as
\[
((\holtxt{<infix>}\; \term)\; \term)
\]

\paragraph{Prefixes}

Where infixes appear between their arguments, prefixes appear before theirs.
This might initially appear to be the same thing as happens with normal function application where the symbol on the left simply has no fixity: is $f$ in $f(x)$ not acting as a prefix?
Actually though, in a term such as $f(x)$, where $f$ and $x$ do not have fixities, the syntax is treated as if there is an invisible infix function application between the two tokens: $f\cdot{}x$.
This infix operator binds tightly, so that when one writes $f\,x + y$, the parse is $(f\cdot{}x) + y$.\footnote{There are tighter infix operators: the dot in field selection causes $f\,x.fld$ to parse as $f\cdot(x.fld)$.\index{record types!field selection functions and notation}}
It is then useful to allow for genuine prefixes so that operators can live at different precedence levels than function application.
An example of this is \verb+~+, logical negation.
This is a prefix with lower precedence than function application.
Normally
\[
   f\;x\; y\qquad \mbox{is parsed as}\qquad (f\; x)\; y
\] but \[
  \holtxt{\~{}}\; x\; y\qquad\mbox{is parsed as}\qquad
  \holtxt{\~{}}\; (x\; y)
\]
because the precedence of \verb+~+ is lower than that of function application.
The unary negation symbol would also typically be defined as a prefix, if only to allow one to write \[
  {\it negop}\,{\it negop}\,3
\]
(whatever {\it negop} happened to be) without needing extra parentheses.

On the other hand, the \holtxt{univ} syntax for the universal set (see Section~\ref{sec:theory-of-sets}\index{universal set}) is an example of a prefix operator that binds more tightly than application.
This means that \holtxt{f\,univ(:'a)} is parsed as \holtxt{f(univ(:'a))}, not \holtxt{(f univ)(:'a)} (which parse would fail to type-check).

\paragraph{Suffixes}

Suffixes appear after their arguments.
There are suffixes \holtxt{\^{}+}, \holtxt{\^{}*} and \holtxt{\^{}=} corresponding to the transitive, the reflexive and transitive, and the ``equivalence'' closure used in \ml{relationTheory} (Section~\ref{relation}).
Suffixes are
associated with a precedence just as infixes and prefixes are.
If \holtxt{p} is a prefix, \holtxt{i} an infix, and \holtxt{s} a
suffix, then there are six possible orderings for the three different
operators based on their precedences, giving five parses for
$\holtxt{p}\; t_1\; \holtxt{i}\; t_2\; \holtxt{s}$ depending on the
relative precedences:
\[
\begin{array}{cl}
\mbox{\begin{tabular}{c}Precedences\\(lowest to highest)\end{tabular}} &
\multicolumn{1}{c}{\mbox{Parses}}\\
\hline
p,\;i,\;s & \holtxt{p}\;(t_1\;\holtxt{i}\;(t_2\;\holtxt{s}))\\
p,\;s,\;i & \holtxt{p}\;((t_1\;\holtxt{i}\;t_2)\;\holtxt{s})\\
i,\;p,\;s & (\holtxt{p}\;t_1)\;\holtxt{i}\;(t_2\;\holtxt{s})\\
i,\;s,\;p & (\holtxt{p}\;t_1)\;\holtxt{i}\;(t_2\;\holtxt{s})\\
s,\;p,\;i & (\holtxt{p}\;(t_1\;\holtxt{i}\;t_2))\;\holtxt{s}\\
s,\;i,\;p & ((\holtxt{p}\;t_1)\;\holtxt{i}\;t_2)\;\holtxt{s}\\
\end{array}
\]

\paragraph{Closefixes}

Closefix terms are operators that completely enclose their arguments.
An example one might use in the development of a theory of
denotational semantics is semantic brackets.  Thus, the \HOL{} parsing
facilities can be configured to allow one to write \holtxt{denotation x}
as \holtxt{[| x |]}.  Closefixes are not associated with precedences
because they can not compete for arguments with other operators.


\subsubsection{Parser tricks and magic}

Here we describe how to achieve some useful effects with the
parser in \HOL{}.

\begin{description}

\item[Aliasing] If one wants a special syntax to be an ``alias'' for a
  normal \HOL{} form, this is easy to achieve; both examples so far
  have effectively done this.  However, if one just wants to have a
  normal one-for-one substitution of one string for another, one can't
  use the grammar/syntax phase of parsing to do this.  Instead, one
  can use the overloading mechanism.  For example, let us alias
  \texttt{MEM} for \texttt{IS\_EL}.  We need to use the function
  \texttt{overload\_on} to overload the original constant for the new
  name:
\begin{alltt}
   val _ = overload_on ("MEM", \fholquote{IS_EL});
\end{alltt}

\item[Making addition right associative] If one has a number of old
  scripts that assume addition is right associative because this is
  how \HOL{} used to be, it might be too much pain to convert.  The trick
  is to remove all of the rules at the given level of the grammar, and
  put them back as right associative infixes.  The easiest way to tell
  what rules are in the grammar is by inspection (use
  \ml{term\_grammar()}).  With just \ml{arithmeticTheory}
  loaded, the only infixes at level 500 are \holtxt{+} and
  \holtxt{-}.  So, we remove the rules for them:
\begin{verbatim}
   val _ = app temp_remove_rules_for_term ["+", "-"];
\end{verbatim}
  \noindent And then we put them back with the appropriate
  associativity:
\begin{verbatim}
   val _ = app (fn s => temp_add_infix(s, 500, RIGHT)) ["+", "-"];
\end{verbatim}
\noindent Note that we use the \ml{temp\_} versions of these two
functions so that other theories depending on this one won't be
affected.  Further note that we can't have two infixes at the same
level of precedence with different associativities, so we have to
remove both operators, not just addition.

\item[Mix-fix syntax for {\it if-then-else}:]
\index{conditionals, in HOL logic@conditionals, in \HOL{} logic!printing of}
%
The first step in bringing this about is to look at the general shape
of expressions of this form.  In this case, it will be:
%
\[
  \holtxt{if}\;\; \dots \;\;\holtxt{then}\;\;\dots\;\;
  \holtxt{else}\;\;\dots
  \]
%
 Because there needs to be a ``dangling'' term to the right, the
  appropriate fixity is \ml{Prefix}.  Knowing that the underlying
  term constant is called \holtxt{COND}, the simplest way to achieve
  the desired syntax is:
\begin{verbatim}
val _ = add_rule
   {term_name = "COND", fixity = Prefix 70,
    pp_elements = [TOK "if", BreakSpace(1,0), TM, BreakSpace(1,0),
                   TOK "then", BreakSpace(1,0), TM, BreakSpace(1,0),
                   TOK "else", BreakSpace(1,0)],
    paren_style = Always,
    block_style = (AroundEachPhrase, (PP.CONSISTENT, 0))};
\end{verbatim}
\noindent The actual rule is slightly more complicated, and
may be found in the sources for the theory \theoryimp{bool}.

\item[Mix-fix syntax for term substitution:]

Here the desire is to be able to write something like:
\[
  \mbox{\texttt{[}}\,t_1\,\mbox{\texttt{/}}\,t_2\,\mbox{\texttt{]}}\,t_3
\]
denoting the substitution of $t_1$ for $t_2$ in $t_3$, perhaps
translating to \holtxt{SUB $t_1$ $t_2$ $t_3$}.  This looks
like it should be another \ml{Prefix}, but the choice of the
square brackets (\holtxt{[} and \holtxt{]}) as delimiters would
conflict with the concrete syntax for list literals if this was done.
Given that list literals are effectively of the \ml{CloseFix}
class, the new syntax must be of the same class.  This is easy enough
to do: we set up syntax
\[
\holtxt{[}\,t_1\,\holtxt{/}\,t_2\,\holtxt{]}
\]
to map to \holtxt{SUB $t_1$ $t_2$}, a value of a functional
type, that when applied to a third argument will look
right.\footnote{Note that doing the same thing for the
  \textit{if-then-else} example in the previous example would be
  inappropriate, as it would allow one to write
\[ \holtxt{if}\;P\;\holtxt{then}\;Q\;\holtxt{else} \]
without the trailing argument.}
The rule for this is thus:
\begin{verbatim}
  val _ = add_rule
           {term_name = "SUB", fixity = Closefix,
            pp_elements = [TOK "[", TM, TOK "/", TM, TOK "]"],
            paren_style = OnlyIfNecessary,
            block_style = (AroundEachPhrase, (PP.INCONSISTENT, 2))};
\end{verbatim}

\end{description}

\subsubsection{Hiding constants}
\label{hidden}

\index{parsing, of HOL logic@parsing, of \HOL{} logic!hiding constant status in|(}
\index{HOL system@\HOL{} system!hiding constants in|(}
\index{constants, in HOL logic@constants, in \HOL{} logic!hiding status of}
\index{parsing, of HOL logic@parsing, of \HOL{} logic!overloading}
%
The following function can be used to hide the constant status of a
name from the quotation parser.

\begin{holboxed}
\index{hide@\ml{hide}|pin}
\begin{verbatim}
  val hide   : string -> ({Name : string, Thy : string} list *
                          {Name : string, Thy : string} list)
\end{verbatim}
\end{holboxed}

\noindent Evaluating \ml{hide "$x$"}
makes the quotation parser treat $x$ as a variable (lexical
rules permitting), even if $x$ is the name of a constant in the current theory
(constants and variables can have the same name).
This is useful if one wants to use variables
%
\index{variables, in HOL logic@variables, in \HOL{} logic!with constant names}
%
with the same names as previously declared (or built-in) constants
(\eg, \ml{o}, \ml{I}, \ml{S}, \etc).  The name $x$ is still a constant
for the constructors, theories, \etc; \ml{hide} affects parsing and
printing by removing the given name from the ``overload map'' described
above in Section~\ref{sec:parser:architecture}.  Note that the effect
of \ml{hide} is \emph{temporary}; its effects do not persist in
theories descended from the current one. See the \REFERENCE{} entry
for \ml{hide} for more details, including an explanation of the return
type.

The function

\begin{holboxed}
\index{reveal@\ml{reveal}|pin}
\begin{verbatim}
   reveal : string -> unit
\end{verbatim}
\end{holboxed}

\noindent undoes hiding.

The function

\begin{holboxed}
\index{hidden@\ml{hidden}|pin}
\begin{verbatim}
   hidden : string -> bool
\end{verbatim}
\end{holboxed}

\noindent tests whether a string is the name of a hidden constant.
\index{HOL system@\HOL{} system!adjustment of user interface of}
\index{HOL system@\HOL{} system!hiding constants in|)}
\index{parsing, of HOL logic@parsing, of \HOL{} logic!hiding constant status in|)}

\subsubsection{Adjusting the pretty-print depth}
\label{sec:pretty-print-depth}
\index{printing, in HOL logic@printing, in \HOL{} logic!structural depth adjustment in}

The following \ML{} reference can be used to adjust the maximum depth
of printing

\begin{holboxed}
\index{max_print_depth@\ml{max\_print\_depth}|pin}
\begin{verbatim}
   max_print_depth : int ref
\end{verbatim}
\end{holboxed}

\index{default print depth, for HOL logic@default print depth, for \HOL{} logic|(}

\noindent The default print depth is $-1$, which is interpreted as
meaning no maximum.  Subterms nested more deeply than the maximum
print depth are printed as \holtxt{...}. For example:

\setcounter{sessioncount}{0}
\begin{session}
\begin{alltt}
>>__ show_types := false;
>> arithmeticTheory.ADD_CLAUSES;

>> max_print_depth := 3;
>> arithmeticTheory.ADD_CLAUSES;
>>__ max_print_depth := ~1;
\end{alltt}
\end{session}
\index{default print depth, for HOL logic@default print depth, for \HOL{} logic|)}

\subsection{Quotations and antiquotation}
\label{sec:quotation-antiquotation}

\index{quotation, in HOL logic@quotation, in \HOL{} logic!parser for}
\index{parsing, of HOL logic@parsing, of \HOL{} logic!of quotation syntax|(}
Logic-related syntax in the HOL system is typically passed to the
parser in special forms known as \emph{quotations}.
A basic quotation is delimited by single quotation characters (`...', Unicode code-points U+2018 and U+2019), or single back-ticks (\ie, \ml{\`}, ASCII character~96).
When quotation values are printed out by the ML interactive loop, they look rather ugly because of the special filtering that is done to these values before the ML interpreter even sees them:
\setcounter{sessioncount}{0}
\begin{session}
\begin{alltt}
>> val q = ‘f x = 3’;
\end{alltt}
\end{session}
Quotations (the ML environment prints the type as \ml{'a frag list}) are the raw input form expected by the various HOL parsers.
They are also polymorphic (to be explained below).
Thus the function \ml{Parse.Term} takes a (term) quotation and returns a term, and is of type
\[
  \ml{term quotation -> term}
\]

The term and type parsers can also be called implicitly by using
double quotations (with ``\dots'', characters U+201C and U+201D), or doubled back-ticks as delimiters.
For the type parser, the first non-space character after the leading delimiter must also be a colon.
Thus:
\begin{session}
\begin{alltt}
>> val t1 = “\x:num. x + 3”;
>> val t2 = ``p /\ q``;

>> val ty = “:'a -> bool”;
\end{alltt}
\end{session}
The expression bound to ML variable \ml{t1} above is actually expanded
to an application of the function \ml{Parse.Term} to the quotation
argument `\ml{p /\bs{} q}'.
Similarly, the declaration of \ml{ty}'s expression expands into an application of \ml{Parse.Type} to the quotation `\ml{:'a -> bool}'.

The significant advantage of quotations over normal \ML{} strings is that they can include new-line and backslash characters without requiring special quoting.
Newlines occur whenever terms get beyond the trivial in size, while backslashes occur in not just the representation of $\lambda$, but also the syntax for conjunction and disjunction.

If a quotation is to include a back-quote character, then this should be done by using the quotation syntax's own escape character, the caret (\ml{\^}, ASCII character~94).
To get a bare caret, things are slightly more complicated.
If a sequence of carets is followed by white-space (including a newline), then that sequence of carets is passed to the HOL parser unchanged.
Otherwise, one caret can be obtained by writing two in a row.
(This last rule is analogous to the way in \ML{} string syntax treats the back-slash.)
Thus:
\begin{session}
\begin{alltt}
>> “f ^` x”;

>> “f ^ x”;
\end{alltt}
\end{session}

Finally, if a single caret is followed by a ``symbol'' character, then the caret and symbol are passed through to HOL unchanged.
Thus the following example illustrates two different ways of writing the same thing (in the first input, two carets become one):
\begin{session}
\begin{alltt}
>> “f ^^+ x”;

>>+ ``f ^+ x``;
\end{alltt}
\end{session}

The main use of the caret is to introduce \emph{antiquotations} (as
suggested in the last example above).  Within a quotation, expressions
of the form {\small\verb+^(+}$t${\small\verb+)+}
%
\index{ antiquotation, in HOL logic@{\small\verb+^+} (antiquotation, in \HOL{} logic)}
%
(where $t$ is an \ML\ expression of type
%
\index{type checking, in HOL logic@type checking, in \HOL{} logic!antiquotation in}
%
\ml{term} or \ml{type}) are called antiquotations.
%
\index{terms, in HOL logic@terms, in \HOL{} logic!antiquotation}%
\index{antiquotation, in HOL logic terms@antiquotation, in \HOL{} logic terms}%
%
An antiquotation \holtxt{\^{}($t$)} evaluates to the
\ML{} value of $t$. For example, ``{\small\verb+x \/ ^(mk_conj(+``\verb+y:bool+''\verb+, +``\verb+z:bool+''\verb+))+}''
evaluates to the same term as {\small``\verb+x \/ (y /\ z)+''}. The
most common use of antiquotation is when the term $t$ is bound to an \ML\
variable $x$. In this case {\small\verb+^(+}$x${\small\verb+)+} can be
abbreviated by {\small\verb+^+}$x$.

The following session illustrates antiquotation.

\setcounter{sessioncount}{0}
\begin{session}
\begin{alltt}
>>__ remove_ovl_mapping "+" {Name = "int_add", Thy = "integer"};
>> val y = “x+1”;

>> val z = “y = ^y”;

>> “!x:num.?y:num.^z”;
\end{alltt}
\end{session}

\noindent Types may be antiquoted as well:

\begin{session}
\begin{alltt}
>> val pred = “:'a -> bool”;

>> “:^pred -> bool”;
\end{alltt}
\end{session}

\noindent Quotations are polymorphic, and the type variable of a
quotation corresponds to the type of entity that can be antiquoted
into that quotation.  Because the term parser expects only antiquoted
terms, antiquoting a type into a term quotation requires the use of
\holtxt{ty\_antiq}. For example,%
%
\index{ty_antiq@\ml{ty\_antiq}}

\begin{session}
\begin{alltt}
>>+ “!P:^pred. P x ==> Q x”;

>> “!P:^(ty_antiq pred). P x ==> Q x”;
\end{alltt}
\end{session}
%
\index{parsing, of HOL logic@parsing, of \HOL{} logic!of quotation syntax|)}



\subsection{Backwards compatibility of syntax}

This section of the manual documents the (extensive) changes made to
the parsing of \HOL{} terms and types in the Taupo release (one of the
HOL3 releases) and beyond from the point of view of a user who doesn't
want to know how to use the new facilities, but wants to make sure
that their old code continues to work cleanly.

The changes which may cause old terms to fail to parse are:
\begin{itemize}
\newcommand\condexp{\holtxt{$p$ => $q$ | $r$}}
\item The precedence of type annotations has completely changed.  It
  is now a very tight suffix (though with a precedence weaker than
  that associated with function application), instead of a weak one.
  This means that \mbox{\tt (x,y:bool \# bool)} should now be written
  as \mbox{\tt (x,y):bool \# bool}. The previous form will now be
  parsed as a type annotation applying to just the \verb+y+.  This
  change brings the syntax of the logic closer to that of SML and
  should make it generally easier to annotate tuples, as one can now
  write \[ (x\,:\,\tau_1,\;y\,:\,\tau_2,\dots z\,:\,\tau_n)
  \] instead of \[
  (x\,:\,\tau_1, \;(y\,:\,\tau_2, \dots (z\,:\,\tau_n)))
  \] where extra parentheses have had to be added just to allow one to
  write a frequently occurring form of constraint.
\item Most arithmetic operators are now left associative instead of
  right associative.  In particular, $+$, $-$, $*$ and {\tt DIV} are
  all left associative.  Similarly, the analogous operators in other
  numeric theories such as {\tt integer} and {\tt real} are also left
  associative.  This brings the \HOL{} parser in line with standard
  mathematical practice.
\item The binding equality in {\tt let} expressions is treated exactly
  the same way as equalities in other contexts.  In previous versions
  of \HOL, equalities in this context have a different, weak binding
  precedence.
\item The old syntax for conditional expressions has been
  removed. Thus the string \holquote{\condexp} must now be written
  $\holquote{\texttt{if}\;p\;\texttt{then}\;q\;\texttt{else}\;r}$
  instead.
\item Some lexical categories are more strictly policed.  String
  literals (strings inside double quotes) and numerals can't be used
  unless the relevant theories have been loaded.  Nor can these
  literals be used as variables inside binding scopes.
\end{itemize}


\section{A Simple Interactive Proof Manager}\label{sec:goalstack}

The \emph{goal stack} provides a simple interface to tactic-based
interactive proof. When one uses tactics to decompose a proof, many
intermediate states arise; the goalstack takes care of the necessary
bookkeeping. The implementation of goalstacks reported here is a
re-design of Larry Paulson's original conception.

The goalstack library is automatically loaded when \HOL{} starts up.
Editor modes can support the process of using the proof manager; here we describe the underlying \ML{} interface.

The abstract types \ml{goalstack} and \ml{proofs} are the
focus of backwards proof operations. The type \ml{proofs} can be
regarded as a list of independent goalstacks. Most operations act on
the head of the list of goalstacks; there are also operations so that
the focus can be changed.

\subsection{Starting a goalstack proof}

\begin{hol}
\begin{verbatim}
   g        : term quotation -> proofs
   set_goal : goal -> proofs
\end{verbatim}
\end{hol}

Recall that the type \ml{goal} is an abbreviation for
\ml{term list * term}. To start on a new goal, one gives
\ml{set\_goal} a goal. This creates a new goalstack and makes it the
focus of further operations.

A shorthand for \ml{set\_goal} is the function \ml{g}: it
invokes the parser automatically, and it doesn't allow the goal to
have any assumptions.

Calling \ml{set\_goal}, or \ml{g}, adds a new proof attempt to the existing ones, \ie, rather than overwriting the current proof attempt, the new attempt is stacked on top.

\subsection{Applying a tactic to a goal}

\begin{hol}
\begin{verbatim}
   expandf : tactic -> goalstack
   expand  : tactic -> goalstack
   e       : tactic -> goalstack
\end{verbatim}
\end{hol}

How does one actually do a goalstack proof then?
In most cases, the application of tactics to the current goal is done with the function \ml{expand}.
In the rare case that one wants to apply an \emph{invalid} tactic, then \ml{expandf} is used.
(For an explanation of invalid tactics, see Section~\ref{tactics}.)
The abbreviation \ml{e} may also be used to expand a tactic.

\subsection{Undo}

\begin{hol}
\begin{verbatim}
   b          : unit -> goalstack
   drop       : unit -> proofs
   dropn      : int  -> proofs
   backup     : unit -> goalstack
   restart    : unit -> goalstack
   set_backup : int  -> unit
\end{verbatim}
\end{hol}

Often (we are tempted to say {\it usually}!) one takes a wrong path
in doing a proof, or makes a mistake when setting a goal. To undo a step
in the goalstack, the function \ml{backup} and its abbreviation
\ml{b} are used. This will restore the goalstack to its previous
state.


To directly back up all the way to the original goal, the function
\ml{restart} may be used. Obviously, it is also important to get
rid of proof attempts that are wrong; for that there is \ml{drop},
which gets rid of the current proof attempt, and \ml{dropn}, which
eliminates the top $n$ proof attempts.


Each proof attempt has its own \emph{undo-list} of previous
states. The undo-list for each attempt is of fixed size (initially
12). If you wish to set this value for the current proof attempt, the
function \ml{set\_backup} can be used. If the size of the backup
list is set to be smaller than it currently is, the undo list will be
immediately truncated. You can not undo a ``proofs-level'' operation, such
as \ml{set\_goal} or \ml{drop}.

\subsection{Viewing the state of the proof manager}

\begin{hol}
\begin{verbatim}
   p            : unit -> goalstack
   status       : unit -> proofs
   top_goal     : unit -> goal
   top_goals    : unit -> goal list
   initial_goal : unit -> goal
   top_thm      : unit -> thm
\end{verbatim}
\end{hol}

To view the state of the proof manager at any time, the functions
\ml{p} and \ml{status} can be used. The former only shows
the top subgoals in the current goalstack, while the second gives a
summary of every proof attempt.

To get the top goal or goals of a proof attempt, use \ml{top\_goal}
and \ml{top\_goals}. To get the original goal of a proof attempt,
use \ml{initial\_goal}.

Once a theorem has been proved, the goalstack that was used to derive it
still exists (including its undo-list): its main job now is to
hold the theorem. This theorem can be retrieved with
\ml{top\_thm}.

\subsection{Switch focus to a different subgoal or proof attempt}

\begin{hol}
\begin{verbatim}
   r             : int -> goalstack
   R             : int -> proofs
   rotate        : int -> goalstack
   rotate_proofs : int -> proofs
\end{verbatim}
\end{hol}

Often we want to switch our attention to a different goal in the current
proof, or a different proof. The functions that do this are
\ml{rotate} and \ml{rotate\_proofs}, respectively. The abbreviations
\ml{r} and \ml{R} are simpler to type in.

\section{High Level Proof---\texttt{bossLib}}
% would use \ml{boss} above but it puts LaTeX into fits
\label{sec:bossLib}
\newcommand\bossLib{\ml{bossLib}}

\index{bossLib@\ml{bossLib}}
The library \bossLib\ marshals some of the most widely used theorem
proving tools in \HOL{} and provides them with a convenient interface
for interaction. The library currently focuses on three things:
definition of datatypes and functions; high-level interactive proof
operations, and composition of automated reasoners. Loading \bossLib\
commits one to working in a context that already supplies the theories
of booleans, pairs, sums, the option type, arithmetic, and lists.


\subsection{Support for high-level proof steps}
\label{sec:high-level-proof-steps}

The following functions use information in the database to ease the
application of \HOL's underlying functionality:

\index{Induct_on@\ml{Induct\_on}}
\index{Cases_on@\ml{Cases\_on}}
\begin{verbatim}
   type_rws     : hol_type -> thm list
   Induct       : tactic
   Cases        : tactic
   Cases_on     : term quotation -> tactic
   Induct_on    : term quotation -> tactic
\end{verbatim}

\index{type_rws@\ml{type\_rws}}
\index{TypeBase@\ml{TypeBase}}
%
The function \ml{type\_rws} will search for the given type in the
underlying \ml{TypeBase} database and return useful rewrite rules for
that type. The rewrite rules of the datatype are built from the
injectivity and distinctness theorems, along with the case constant
definition. The simplification tactics \ml{RW\_TAC}, \ml{SRW\_TAC},
and the \simpset{} \ml{(srw\_ss())} automatically include these
theorems.  Other tactics used with other \simpset{}s will need these
theorems to be manually added.

\index{induction theorems, in HOL logic@induction theorems, in \HOL{} logic!for algebraic data types}
%
The \ml{Induct} tactic makes it convenient to invoke induction. When
it is applied to a goal, the leading universal quantifier is examined;
if its type is that of a known datatype, the appropriate structural
induction tactic is extracted and applied.

The \ml{Cases} tactic makes it convenient to invoke case
analysis. The leading universal quantifier in the goal is examined; if
its type is that of a known datatype, the appropriate structural
case analysis theorem is extracted and applied.

The \ml{Cases\_on} tactic takes a quotation, which is
parsed into a term $M$, and then $M$ is searched for in the goal. If $M$
is a variable, then a variable with the same name is searched for. Once
the term to split over is known, its type and the associated facts are
obtained from the underlying database and used to perform the case
split. If some free variables of $M$ are bound in the goal, an attempt
is made to remove (universal) quantifiers so that the case split has
force. Finally, $M$ need not appear in the goal, although it should at
least contain some free variables already appearing in the goal. Note
that the \ml{Cases\_on} tactic is more general than \ml{Cases}, but
it does require an explicit term to be given.

\index{Induct_on (ML induction tactic)@\ml{Induct\_on} (\ML{} induction tactic)}
The \ml{Induct\_on} tactic takes a quotation, which is parsed into a
term $M$, and then $M$ is searched for in the goal. If $M$ is a
variable, then a variable with the same name is searched for. Once the
term to induct on is known, its type and the associated facts are
obtained from the underlying database and used to perform the
induction.  If $M$ is not a variable, a new variable $v$ not already
occurring in the goal is created, and used to build a term $v = M$
which the goal is made conditional on before the induction is
performed. First however, all terms containing free variables from $M$
are moved from the assumptions to the conclusion of the goal, and all
free variables of $M$ are universally quantified. \ml{Induct\_on} is
more general than \ml{Induct}, but it does require an explicit term to
be given.

Three supplementary entry-points have been provided for more exotic
inductions:
\begin{description}
\item [\ml{completeInduct\_on}] performs complete induction on the
  term denoted by the given quotation. Complete induction allows a
  seemingly\footnote{Complete induction and ordinary mathematical
    induction are each derivable from the other.} stronger induction
  hypothesis than ordinary mathematical induction: to wit, when
  inducting on $n$, one is allowed to assume the property holds for
  \emph{all} $m$ smaller than $n$. Formally: $\forall P.\ (\forall x.\
  (\forall y.\ y < x \supset P\, y) \supset P\,x) \supset \forall x.\
  P\,x$. This allows the inductive hypothesis to be used more than
  once, and also allows instantiating the inductive hypothesis to
  other than the predecessor.

\item [\ml{measureInduct\_on}] takes a quotation, and breaks it
  apart to find a term and a measure function with which to induct.
  For example, if one wanted to induct on the length of a list
  \holtxt{L}, the invocation \ml{measureInduct\_on~`LENGTH L`}
  would be appropriate.

\item [\ml{recInduct}] takes a induction theorem generated by
\ml{Define} or \ml{Hol\_defn} and applies it to the current goal.

\end{description}


\subsection{Automated reasoners}
\label{sec:automated-reasoners}

\ml{bossLib} brings together the most powerful reasoners in \HOL{} and
tries to make it easy to compose them in a simple way. We take our basic
reasoners from \ml{mesonLib}, \ml{simpLib}, and \ml{numLib},
but the point of \ml{bossLib} is to provide a layer of abstraction so
the user has to know only a few entry-points.\footnote{In the mid 1980's
Graham Birtwistle advocated such an approach, calling it `Ten Tactic
HOL'.} (These underlying libraries, and others providing similarly
powerful tools are described in detail in sections below.)
\begin{hol}
\begin{verbatim}
   PROVE      : thm list -> term -> thm
   PROVE_TAC  : thm list -> tactic

   METIS_TAC  : thm list -> tactic
   METIS_PROVE: thm list -> term -> thm

   DECIDE     : term quotation -> thm
   DECIDE_TAC : tactic
\end{verbatim}
\end{hol}
The inference rule \texttt{PROVE} (and the corresponding tactic
\texttt{PROVE\_TAC}) takes a list of theorems and a term, and attempts
to prove the term using a first order reasoner.  The two \ml{METIS}
functions perform the same functionality but use a different
underlying proof method.  The \texttt{PROVE} entry-points refer to the
\texttt{meson} library, which is further described in
Section~\ref{sec:mesonLib} below. The \ml{METIS} system is described
in Section~\ref{sec:metisLib}.  The inference rule \texttt{DECIDE}
(and the corresponding tactic \texttt{DECIDE\_TAC}) applies a decision
procedure that (at least) handles statements of linear arithmetic.

\begin{hol}
\begin{verbatim}
   RW_TAC   : simpset -> thm list -> tactic
   SRW_TAC  : ssfrag list -> thm list -> tactic
   &&       : simpset * thm list -> simpset  (* infix *)
   std_ss   : simpset
   arith_ss : simpset
   list_ss  : simpset
   srw_ss   : unit -> simpset
\end{verbatim}
\end{hol}
%
\index{RW_TAC@\ml{RW\_TAC}} The rewriting tactic \ml{RW\_TAC} works by
first adding the given theorems into the given \simpset; then it
simplifies the goal as much as possible; then it performs case splits
on any conditional expressions in the goal; then it repeatedly (1)
eliminates all hypotheses of the form $v = M$ or $M = v$ where $v$ is
a variable not occurring in $M$, (2) breaks down any equations between
constructor terms occurring anywhere in the goal. Finally,
\ml{RW\_TAC} lifts \holtxt{let}-expressions within the goal so that
the binding equations appear as
abbreviations\index{abbreviations!tactic-based proof} in the
assumptions.

\index{SRW_TAC@\ml{SRW\_TAC}} The tactic \ml{SRW\_TAC} is similar to
\ml{RW\_TAC}, but works with respect to an underlying \simpset{}
(accessible through the function \ml{srw\_ss}) that is updated as new
context is loaded.  This \simpset{} can be augmented through the
addition of ``\simpset{} fragments'' (\ml{ssfrag} values) and
theorems.  In situations where there are many large types stored in
the system, \ml{RW\_TAC}'s performance can suffer because it
repeatedly adds all of the rewrite theorems for the known types into a
\simpset{} before attacking the goal.  On the other hand,
\ml{SRW\_TAC} loads rewrites into the \simpset{} underneath
\ml{srw\_ss()} just once, making for faster operation in this
situation.

\ml{bossLib} provides a number of simplification sets. The
simpset for pure logic, sums, pairs, and the \ml{option} type is
named \ml{std\_ss}. The simpset for arithmetic is named
\ml{arith\_ss}, and the simpset for lists is named \ml{list\_ss}.
The simpsets provided by \bossLib{} strictly increase in strength:
\ml{std\_ss} is contained in \ml{arith\_ss}, and \ml{arith\_ss} is
contained in \ml{list\_ss}.  The infix combinator \ml{\&\&} is used
to build a new \simpset{} from a given \simpset{} and a list of
theorems. \HOL's simplification technology is described further in
Section~\ref{sec:simpLib} below and in the \REFERENCE.

\begin{hol}
\begin{verbatim}
   by : term quotation * tactic -> tactic (* infix 8 *)
   SPOSE_NOT_THEN : (thm -> tactic) -> tactic
\end{verbatim}
\end{hol}
The function \ml{by} is an infix operator that takes a quotation
and a tactic $tac$. The quotation is parsed into a term $M$. When the
invocation ``\ml{$M$ by $\mathit{tac}$}'' is applied to a goal
$(A,g)$, a new subgoal $(A,M)$ is created and $tac$ is applied to it.
If the goal is proved, the resulting theorem is broken down and added
to the assumptions of the original goal; thus the proof proceeds with
the goal $((M::A), g)$. (Note however, that case-splitting will happen
if the breaking-down of $\ \vdash M$ exposes disjunctions.) Thus
\ml{by} allows a useful style of `assertional' or `Mizar-like'
reasoning to be mixed with ordinary tactic proof.\footnote{Proofs in
  the Mizar system are readable documents, unlike most
  tactic-based proofs.}

The \ml{SPOSE\_NOT\_THEN} entry-point initiates a proof by
contradiction by assuming the negation of the goal and driving the
negation inwards through quantifiers. It provides the resulting
theorem as an argument to the supplied function, which will use the
theorem to build and apply a tactic.

\section{First Order Proof---\texttt{mesonLib} and \texttt{metisLib}}
\label{sec:first-order-proof}
\index{decision procedures!first-order logic}

First order proof is a powerful theorem-proving technique that can
finish off complicated goals.  Unlike tools such as the simplifier, it
either proves a goal outright, or fails.  It can not transform a goal
into a different (and more helpful) form.

\subsection{Model elimination---\texttt{mesonLib}}
\label{sec:mesonLib}

\index{meson (model elimination) procedure@\ml{meson} (model elimination) procedure}
\index{model elimination method for first-order logic}

The \ml{meson} library is an implementation of the
model-elimination method for finding proofs of goals in first-order
logic.  There are three main entry-points:
\begin{hol}
\begin{verbatim}
   MESON_TAC     : thm list -> tactic
   ASM_MESON_TAC : thm list -> tactic
   GEN_MESON_TAC : int -> int -> int -> thm list -> tactic
\end{verbatim}
\end{hol}

Each of these tactics attempts to prove the goal.  They will either
succeed in doing so, or fail with a ``depth exceeded'' exception.  If
the branching factor in the search-space is high, the \texttt{meson}
tactics may also take a very long time to reach the maximum depth.

All of the \texttt{meson} tactics take a list of theorems.  These
extra facts are used by the decision procedure to help prove the goal.
\texttt{MESON\_TAC} ignores the goal's assumptions; the other two
entry-points include the assumptions as part of the sequent to be
proved.

The extra parameters to \ml{GEN\_MESON\_TAC} provide extra control of
the behaviour of the iterative deepening that is at the heart of the
search for a proof.  In any given iteration, the algorithm searches
for a proof of depth no more than a parameter $d$.  The default
behaviour for \ml{MESON\_TAC} and \ml{ASM\_MESON\_TAC} is to start $d$
at 0, to increment it by one each time a search fails, and to fail if
$d$ exceeds the value stored in the reference value
\ml{mesonLib.max\_depth}.  By way of contrast,
\ml{GEN\_MESON\_TAC~min~max~step} starts $d$ at \ml{min}, increments
it by \ml{step}, and gives up when $d$ exceeds \ml{max}.

The \ml{PROVE\_TAC} function from \ml{bossLib} performs some
normalisation, before passing a goal and its assumptions to
\ml{ASM\_MESON\_TAC}.  Because of this normalisation, in most
circumstances, \ml{PROVE\_TAC} should be preferred to
\ml{ASM\_MESON\_TAC}.

\subsection{Resolution---\texttt{metisLib}}
\label{sec:metisLib}

\index{metis (resolution) procedure@\ml{metis} (resolution) procedure}
\index{resolution method for first-order logic}

The \ml{metis} library is an implementation of the resolution method
for finding proofs of goals in first-order logic. There are two main
entry-points:

\begin{hol}
\begin{verbatim}
   METIS_TAC   : thm list -> tactic
   METIS_PROVE : thm list -> term -> thm
\end{verbatim}
\end{hol}

Both functions take a list of theorems, and these are used as lemmas
in the proof. \texttt{METIS\_TAC} is a tactic, and will either succeed
in proving the goal, or if unsuccessful will either fail or loop
forever. \texttt{METIS\_PROVE} takes a term $t$ and tries to prove a
theorem with conclusion $t$: if successful, the theorem $\vdash t$ is
returned. As for \texttt{METIS\_TAC}, it might fail or loop forever if
the proof search is unsuccessful.

The \texttt{metisLib} family of proof tools implement the ordered
resolution and ordered paramodulation calculus for first order logic,
which usually makes them better suited to goals requiring non-trivial
equality reasoning than the tactics in \texttt{mesonLib}.


\input{simplifier}
>>__ remove_ovl_mapping GrammarSpecials.fromNum_str
       {Name = "int_of_num", Thy = "integer"};

\section{Efficient Applicative Order Reduction---\texttt{computeLib}}
\label{sec:computeLib}
\index{computeLib@\ml{computeLib}|see{\ml{EVAL}}}

Section \ref{sec:datatype} and Section \ref{TFL} show the ability of
\HOL{} to represent many of the standard constructs of functional
programming. If one then wants to `run' functional programs on
arguments, there are several choices. First, one could apply the
simplifier, as demonstrated in Section \ref{sec:simpLib}. This allows
all the power of the rewriting process to be brought to bear,
including, for example, the application of decision procedures to
prove constraints on conditional rewrite rules.  Second, one could
write the program, and all the programs it transitively depends on,
out to a file in a suitable concrete syntax, and invoke a compiler or
interpreter. This functionality is available in \HOL{} via use of
\ml{EmitML.exportML}.

Third, \ml{computeLib} can be used. This library supports call-by-value
evaluation of \HOL{} functions by deductive steps. In other words, it
is quite similar to having an \ML{} interpreter inside the \HOL{} logic,
working by forward inference. When used in this way, functional
programs can be executed more quickly than by using the simplifier.

The most accessible entry-points for using the \ml{computeLib} library
are the conversion \ml{EVAL} and its tactic counterpart
\ml{EVAL\_TAC}.  These depend on an internal database that stores
function definitions. In the following example, loading \ml{sortingTheory}
augments this database with relevant definitions, that of Quicksort
(\holtxt{QSORT}) in particular, and then we can evaluate
\holtxt{QSORT} on a concrete list.
%
\setcounter{sessioncount}{0}
\begin{session}
\begin{alltt}
>> load "sortingTheory";

>> EVAL ``QSORT (<=) [76;34;102;3;4]``;
\end{alltt}
\end{session}
\index{EVAL@\ml{EVAL}!on symbolic arguments}
Often, the argument to a function has no variables: in that case
application of \ml{EVAL} ought to return a ground result,
as in the above example. However, \ml{EVAL} can also evaluate functions on
arguments with variables---so-called \emph{symbolic} evaluation---and
in that case, the behaviour of \ml{EVAL} depends on the structure of the
recursion equations. For example, in the following session, if there is
sufficient information in the input, symbolic execution can deliver
an interesting result. However, if there is not enough information
in the input to allow the algorithm any traction, no expansion will
take place.
%
\begin{session}
\begin{alltt}
>> EVAL ``REVERSE [u;v;w;x;y;z]``;

>> EVAL ``REVERSE alist``;
\end{alltt}
\end{session}
%

\subsection{Dealing with divergence}

The major difficulty with using \ml{EVAL} is termination. All too
often, symbolic evaluation with \ml{EVAL} will diverge, or generate
enormous terms. The usual cause is conditionals with variables in the
test. For example, the following definition is provably equal to \holtxt{FACT},
%
\begin{session}
\begin{alltt}
>> Define `fact n = if n=0 then 1 else n * fact (n-1)`;
\end{alltt}
\end{session}
%
But the two definitions evaluate completely differently.
%
\begin{session}
\begin{alltt}
>> EVAL ``FACT n``;

> EVAL ``fact n``;
  <.... interrupt key struck ...>
Interrupted.
\end{alltt}
\end{session}
%
The primitive-recursive definition of \holtxt{FACT} does not expand
at all, while the destructor-style recursion of \holtxt{fact} never stops
expanding. A rudimentary monitoring facility shows the behaviour, first
on a ground argument, then on a symbolic argument.
%
\begin{session}
\begin{alltt}
>> val [fact] = decls "fact";
>> computeLib.monitoring := SOME (same_const fact);

>> EVAL ``fact 4``;

> EVAL ``fact n``;
fact n = (if n = 0 then 1 else n * fact (n - 1))
fact (n - 1) = (if n - 1 = 0 then 1 else (n - 1) * fact (n - 1 - 1))
fact (n - 1 - 1) =
(if n - 1 - 1 = 0 then 1 else (n - 1 - 1) * fact (n - 1 - 1 - 1))
fact (n - 1 - 1 - 1) =
(if n - 1 - 1 - 1 = 0 then
   1
 else
   (n - 1 - 1 - 1) * fact (n - 1 - 1 - 1 - 1))
   .
   .
   .
\end{alltt}
\end{session}
%
In each recursive expansion, the test involves a variable, and hence
cannot be reduced to either \holtxt{T} or \holtxt{F}. Thus, expansion
never stops.

Some simple remedies can be adopted in trying to deal with
non-terminating symbolic evaluation.
\begin{itemize}
\item \ml{RESTR\_EVAL\_CONV} behaves like \ml{EVAL} except
  it takes an extra list of constants. During
  evaluation, if one of the supplied constants is encountered, it will
  not be expanded. This allows evaluation down to a specified level,
  and can be used to cut-off some looping evaluations.
\item \ml{set\_skip} can also be used to control
 evaluation. See the \REFERENCE{} entry for \ml{CBV\_CONV} for
 discussion of \ml{set\_skip}.

\end{itemize}

\paragraph{Custom evaluators}

\index{EVAL@\ml{EVAL}!custom compsets@custom \ml{compsets}}
For some problems, it is desirable to construct a customized
evaluator, specialized to a fixed set of definitions. The \ml{compset}
type found in \ml{computeLib} is the type of definition databases. The
functions \ml{new\_compset}, \ml{bool\_compset}, \ml{add\_funs}, and
\ml{add\_convs} provide the standard way to build up such
databases. Another quite useful \holtxt{compset} is
\ml{reduceLib.num\_compset}, which may be used for evaluating
terms with numbers and booleans.  Given a \ml{compset}, the function
\ml{CBV\_CONV} generates an evaluator: it is used to implement \ml{EVAL}.
See \REFERENCE{} for more details.

\paragraph{Dealing with Functions over Peano Numbers}
\index{EVAL@\ml{EVAL}!functions over numbers}
Functions defined by pattern-matching over Peano-style numbers cannot be used by \ml{EVAL} to compute the application of those functions to numerals.
This is because numerals are represented with a binary positional notation (described in Section~\ref{sec:numerals}).
The Peano-style presentation is important for proofs about these functions, so we encourage definitions in this style.
For the sake of computation, \HOL{}'s definitional facilities will automatically use the \ML{} function
\ml{numLib.SUC\_TO\_NUMERAL\_DEFN\_CONV} to derive equations over numerals from an equation over \holtxt{SUC}.
\begin{session}
\begin{alltt}
>>_ Definition mod3_def:
      mod3 0 = 0 /\
      mod3 (SUC n) = let m = mod3 n in
                     if m = 2 then 0 else m + 1
    End
>> EVAL “mod3 11”;
\end{alltt}
\end{session}

\paragraph{Storing and using definitions}
\index{EVAL@\ml{EVAL}!automatic use of definitions}
\index{EVAL@\ml{EVAL}!compute and nocompute attributes@\ml{compute} and \ml{nocompute} attributes}
\HOL{}'s top-level definition facilities (\ie, the \ml{Define} function and the \ml{Definition} syntax) automatically add definitions to the global compset
used by \ml{EVAL} and \ml{EVAL\_TAC}.

Occasionally, one does \emph{not} want a definition automatically added to the global compset.
\index{theorem attributes!nocompute@\ml{nocompute}}
The easiest way to achieve this is to use the \ml{nocompute} ``pseudo-attribute'':\footnote{The \ml{nocompute} attribute does nothing when applied to \ml{Theorem} declarations.}
\begin{session}
\begin{alltt}
>> Definition f_def[nocompute]: f x = x + 10
   End
>> EVAL ``f 6``;
\end{alltt}
\end{session}

By using the \ml{nocompute} attribute, or lower level tools such as \ml{Hol\_defn}, defining equations are not added to the global compset.
Subsequently, one may want to specify theorems to be used as the basis for \ml{EVAL}'s computation.
This can be done with the \ml{compute} attribute attached to a theorem declaration.
\index{theorem attributes!compute@\ml{compute}}
For example, one might write
\begin{session}
\begin{alltt}
>> Theorem f_6[compute]: f 6 = 16
   Proof simp[f_def]
   QED

>> EVAL “f 6 = f 7”;
\end{alltt}
\end{session}

\section{Computation with First-Order Terms---\texttt{cv\_computeLib} and \texttt{cv\_transLib}}
\label{sec:cv-computeLib}
\index{cv\_computeLib@\ml{cv\_computeLib}}
\index{cv\_transLib@\ml{cv\_transLib}}

The library \ml{cv\_computeLib} supports fast evaluation of \HOL{} terms.
It exports a single conversion, \ml{cv\_compute}, which accepts terms in a simple, first-order, untyped language built from a recursive datatype of pairs and natural numbers.
Its accompanying theory, \ml{cvTheory}, defines this type (called \ty{:cv}) and its operations.
When applicable, \ml{cv\_computeLib} will often execute several orders of magnitude faster than \ml{computeLib} (Section~\ref{sec:computeLib}) on similar inputs.
Such performance is possible because \ml{cv\_computeLib} relies on an interpreter that is implemented inside the kernel, and that uses native ML datatypes and arbitrary precision integer arithmetic to execute quickly.
This implementation is a Standard ML adaption of the compute facility in the Candle theorem prover, which has been proved to be sound wrt. the inference rules of higher-order logic~\cite{ITP23}.

The library \ml{cv\_transLib} provides a user-friendly interface to \ml{cv\_computeLib}.
It exports automation which translates functional \HOL{} definitions into equivalent functions which operate over the \ty{:cv} type, and a wrapper around \ml{cv\_computeLib.cv\_compute} called \ml{cv\_eval} which can be used much like \ml{computeLib.EVAL}~(Section~\ref{sec:computeLib}).
Its accompanying theories (\ml{cv\_primTheory} and \ml{cv\_stdTheory}) define and translate various common operations over primitive types.

\subsection{Computing with \ml{cv\_compute} directly}
\label{ssec:cv-exprs}

The following example shows how to define a (very simple) function and use it in a computation with \ml{cv\_compute}.

\textbf{NB:} this example illustrates how \ml{cv\_compute} works, but is not the recommended workflow for using it.
Instead, use \ml{cv\_transLib}~(Section~\ref{ssec:cv-trans-lib}).

\begin{session}
\begin{alltt}
>> load "cv_computeLib";
>> Definition square_def:
     square x = cv_mul x x
   End
>> cv_computeLib.cv_compute [square_def] ``square (cv$Num 7)``;
\end{alltt}
\end{session}
To reduce a term involving the constant \holtxt{square}, \ml{cv\_compute} must be given its defining equation (\ml{square\_def}), and both the input term and this equation must be written in a special style, using a special set of operations (such as \holtxt{cv\_mul}).
We call defining equations and terms in this style \emph{code equations} and \emph{compute expressions}, respectively.

\paragraph{Compute expressions}
A compute expression is a closed, first-order expression with type \ty{:cv}.
The \ty{:cv} datatype is defined in \ml{cvTheory} as follows:
\begin{holboxed}
\begin{alltt}
  Datatype: cv = Pair cv cv
               | Num num
  End
\end{alltt}
\end{holboxed}

Aside from the \ty{:cv} datatype constructors, the following operations can be used to construct new compute expressions:
\begin{hol}
\begin{tabular}{ll}
\multicolumn{2}{l}{\mbox{Arithmetic}}\\
\hline
\holtxt{cv\_add: cv -> cv -> cv} & Addition\\
\holtxt{cv\_sub: cv -> cv -> cv} & Subtraction\\
\holtxt{cv\_mul: cv -> cv -> cv} & Multiplication\\
\holtxt{cv\_div: cv -> cv -> cv} & Division (defined for zero)\\
\holtxt{cv\_mod: cv -> cv -> cv} & Modulus (defined for zero)\\
\holtxt{cv\_lt: cv -> cv -> cv}  & Less-than (\holtxt{<}) comparison\\[0.5cm]
\multicolumn{2}{l}{\mbox{Pairs}}\\
\hline
\holtxt{cv\_fst: cv -> cv} & First pair projection\\
\holtxt{cv\_snd: cv -> cv} & Second pair projection\\
\holtxt{cv\_ispair: cv -> cv} & Pair recognizer\\[0.5cm]
\multicolumn{2}{l}{\mbox{Miscellaneous}}\\
\hline
\holtxt{cv\_eq: cv -> cv -> cv} & Equality\\
\holtxt{cv\_if: cv -> cv -> cv -> cv} & \holtxt{if-then-else}\\
\holtxt{let} \(x\;=\;y\) \holtxt{in} \(z\) & Let-binding: \(x, y, z\) must have type \ty{:cv}\\
\(\holtxt{f}\;x_1\;\cdots\;x_n\) & Function application: all \(x_i\) are of type \ty{:cv}\\
\(x\) & Variable: with type \ty{:cv}
\end{tabular}
\end{hol}

The following holds for the semantics of these operations:
\begin{itemize}
\item Arithmetic works as on \HOL{}'s natural numbers.
\item Boolean-like expressions (such as \holtxt{cv\_if}) treat \holtxt{Num 1} as true, and all other values as false.
\item All ill-typed expressions (such as \holtxt{cv\_fst (Num 3)}) are defined as \holtxt{Num 0}.
\item Function constants must have a corresponding code equation, see below.
\end{itemize}

\paragraph{Code equations}
A theorem \(\holtxt{f}\;x_1 \cdots x_n = e\) is a code equation for \holtxt{f}, if:
\begin{itemize}
  \item \(x_1 \cdots x_n\) are variables of type \ty{:cv},
  \item \(e\) has type \ty{:cv},
  \item \(e\) is a compute expression, except that the variables \(x_i\) may be free in \(e\).
\end{itemize}

\paragraph{Example: computing factorial}

The following example is taken from the \ml{cv\_computeLib} examples, available in \ml{exampleTheory} in \ml{examples/cv\_compute}, and has been modified to showcase let-bindings:
\begin{session}
\begin{alltt}
>> load "cv_computeLib";
>>__ open cvTheory;

>>_ Definition fact_def:
     fact n =
       let one = cv$Num 1 in
       cv_if (cv_lt n one)
             one
             (cv_mul n (fact (cv_sub n one)))
   Termination
     WF_REL_TAC `measure cv_size` >>
     Cases >>
     simp [cv_size_def, CaseEq "bool", c2b_def]
   End

##linelen_limit 75
>> time (cv_computeLib.cv_compute [fact_def]) ``fact (cv$Num 1234)``;
##nolinelen_limit
\end{alltt}
\end{session}
On a modern machine, the call to \holtxt{cv\_compute} finishes in less than two tenths of a second.

\subsection{\texttt{Thm.compute}}
\label{ssec:thm-compute}

The conversion \ml{cv\_compute} is built on top of a kernel primitive accessible through the function \ml{Thm.compute}:
\begin{hol}
\begin{alltt}
  type instantiation =
    \{ cval_terms : (string * term) list,
      cval_type  : hol_type,
      num_type   : hol_type,
      char_eqns  : (string * thm) list \}
  val compute : instantiation -> thm list -> term -> thm
\end{alltt}
\end{hol}

Before it can be used, \ml{Thm.compute} must be instantiated with a record containing constants, types and characteristic theorems for the constants and types.
The list of characteristic equations is rather large, and need only be passed to \ml{compute} once (the application is cached).
Indeed, this instantiation is the sole duty performed by \ml{cv\_computeLib}.

The reason for why this instantiation must occur is as follows.
Internally, \ml{Thm.compute} takes apart the \HOL{} logic's terms and converts them into its own representation, performs computation, and converts the result back into a term.
The soundness of this procedure depends on various constants and types having certain meanings; for example, that \holtxt{+} is natural number addition, and that the following holds:
\begin{hol}
\begin{alltt}
>>__ open arithmeticTheory;
##thm ADD
\end{alltt}
\end{hol}
However, natural numbers, arithmetic, and theorems like \ml{ADD} are derived long after the kernel code is compiled.

\paragraph{Alternative instantiations}
It is possible to instantiate \ml{Thm.compute} differently, for example if one wants to use a different type of numbers (as long as it satisfies the same axioms).
An example instantiation can be seen in the source code of \ml{cv\_computeLib} in the \HOL{} sources; the list of types, constants and symbols required is too long to include here.

\paragraph{Further reading}
For an in-depth explanation of \ml{Thm.compute}, we refer the reader to the Candle theorem prover's \ml{compute} primitive~\cite{ITP23}, on which \ml{Thm.compute} is based.

\subsection{Computing with \ml{cv\_compute} via \ml{cv\_transLib}}
\label{ssec:cv-trans-lib}

It is also possible to use \ml{cv\_compute} on \HOL{} functions that are not defined using the \ty{:cv} type, using \ml{cv\_transLib}.
This library supports automatic translation of functions to equivalent versions operating over the \ty{:cv} type, and maintains a database of known translations.
Then, the \ml{cv\_eval} entrypoint can be used on a regular \HOL{} term: it uses the database to translate the term to an equivalent \ty{:cv} version, invokes \ml{cv\_compute}, and translates the result back from the \ty{:cv} type.
Note therefore that, like \ml{cv\_compute}, \ml{cv\_eval} accepts only closed terms.
All constants in its input must also be found in its database of known translations.
Therefore, the intended workflow for using \ml{cv\_transLib} is as follows:
\begin{enumerate}
  \item Define \HOL{} functions in the usual way.
  \item Invoke \ml{cv\_transLib} to translate these functions to \ty{:cv} versions, populating the database of known translations.
  \item Use \ml{cv\_transLib.cv\_eval} to evaluate terms composed of known constants efficiently.
\end{enumerate}

\paragraph{Translation entrypoints}
There are eight entrypoints in \ml{cv\_transLib} for translating \HOL{} functions to \ty{:cv} equivalents:
  \ml{cv\_trans}, \ml{cv\_trans\_pre}, \ml{cv\_trans\_rec}, \ml{cv\_trans\_rec\_pre},
  \ml{cv\_auto\_trans}, \ml{cv\_auto\_trans\_pre}, \ml{cv\_auto\_trans\_rec}, and \ml{cv\_auto\_trans\_rec\_pre}.

All accept a theorem representing a \HOL{} definition.
Those with prefix \ml{cv\_trans} fail if, during translation, they encounter a constant which does not have a known translation.
Those with prefix \ml{cv\_auto\_trans} invoke themselves recursively on any such unknown constants.

Translation of some \HOL{} functions give rise to a precondition (for example, \ml{listheory.HD} requires its argument to be non-empty).
All eight entrypoints will attempt to discharge simple preconditions, but the variants containing \ml{pre} allow a more complex precondition to persist and return its definition to the user.
Preconditions bubble up through further translations, and must be discharged for any term used with \ml{cv\_eval}.
The other variants fail if they encounter a precondition they cannot discharge.

Translation of a recursive \HOL{} function produces a recursive \ty{:cv} function, which may require a termination proof.
All eight entrypoints attempt to discharge simple termination proofs, but the variants containing \ml{rec} accept an additional argument, a tactic which should discharge a more complex termination goal.
The other variants will fail if they cannot prove termination.

\paragraph{Examples: computing squares and factorials}

The following example mirrors those in Section \ref{ssec:cv-exprs}, but it uses \ml{cv\_transLib} instead of interacting directly with the \ty{:cv} type.

\begin{session}
\begin{alltt}
>> load "cv_transLib";
>> load "cv_stdTheory";

>> Definition square'_def:
     square' (x:num) = x * x
   End

>> cv_transLib.cv_trans square'_def;

>> cv_transLib.cv_eval ``square' 7``;

>> arithmeticTheory.FACT;

>> cv_transLib.cv_trans arithmeticTheory.FACT;

##linelen_limit 75
>> time cv_transLib.cv_eval ``FACT 1234``;
##nolinelen_limit
\end{alltt}
\end{session}

Further usage examples are located in \ml{examples/cv\_compute}.

\section{Arithmetic Libraries---\texttt{numLib}, \texttt{intLib} and \texttt{realLib}}
\label{sec:numLib}
\index{decision procedures!Presburger arithmetic over natural numbers}

Each of the arithmetic libraries of \HOL{} provide a
suite of definitions and theorems as well as automated inference support.

\paragraph{numLib}

The most basic numbers in \HOL{} are the natural numbers. The
\ml{numLib} library encompasses the theories \ml{numTheory},
\ml{prim\_recTheory}, \ml{arithmeticTheory}, and \ml{numeralTheory}.
This library also incorporates an evaluator for numeric expression
from \ml{reduceLib} and a decision procedure for linear arithmetic
\ml{ARITH\_CONV}. The evaluator and the decision procedure are
integrated into the simpset \ml{arith\_ss} used by the simplifier.
As well, the linear arithmetic decision procedure can be directly
invoked through \ml{DECIDE} and \ml{DECIDE\_TAC}, both found in
\ml{bossLib}.


\index{decision procedures!Presburger arithmetic over integers}
\paragraph{intLib}

The \ml{intLib} library comprises \ml{integerTheory}, an extensive
theory of the integers, plus two decision procedures
for full Presburger arithmetic. These are available as
\ml{intLib.COOPER\_CONV} and \ml{intLib.ARITH\_CONV}. These
decision procedures are able to deal with linear arithmetic
over the integers and the natural numbers, as well as dealing
with arbitrary alternation of quantifiers.  The \ml{ARITH\_CONV}
procedure is an implementation of the Omega Test, and seems to
generally perform better than Cooper's algorithm.  There are problems
for which this is not true however, so it is useful to have both
procedures available.

\paragraph{realLib}

The \ml{realLib} library provides a foundational development
of the real numbers and analysis. See Section \ref{reals}
for a quick description of the theories.
Also provided is a theory of polynomials, in \theoryimp{polyTheory}.
A decision procedure for linear arithmetic on the real numbers
is also provided by \ml{realLib}, under the name \ml{REAL\_ARITH\_CONV}
and \ml{REAL\_ARITH\_TAC}.

\section{Pattern Matches Library---\texttt{patternMatchesLib}}\label{sec:PatternMatchesLib}
\input{PatternMatchesLib.tex}


\section{Bit Vector Library---\texttt{wordsLib}}

The library \theoryimp{wordsLib} provides tool support for bit-vectors, this includes facilities for: evaluation, parsing, pretty-printing and simplification.

\subsection{Evaluation}

The library \theoryimp{wordsLib} should be loaded when evaluating ground bit-vector terms.  This library provides a \emph{compset} \ml{words\_compset}, which
can be used in the construction of custom \emph{compsets} and conversions.
\setcounter{sessioncount}{0}
\begin{session}
\begin{alltt}
>> load "wordsLib";

>> EVAL ``8w + 9w:word4``;
\end{alltt}
\end{session}
Note that a type annotation is used here to designate the word size.
When the word size is represented by a type variable (\ie, for arbitrary length words), evaluation may give partial or unsatisfactory results.

\subsection{Parsing and pretty-printing}

Words can be parsed in binary, decimal and hexadecimal.   For example:
\begin{session}
\begin{alltt}
>> ``0b111010w : word8``;

>> ``0x3Aw : word8``;
\end{alltt}
\end{session}
It is possible to parse octal numbers, but this must be enabled first by setting the reference \ml{base\_tokens.allow\_octal\_input} to true.  For example:
\begin{session}
\begin{alltt}
>> ``072w : word8``;

>> base_tokens.allow_octal_input:=true;

>> ``072w : word8``;
\end{alltt}
\end{session}

Words can be pretty-printed using the standard number bases. For example, the function
\ml{wordsLib.output\_words\_as\_bin} will select binary format:
\begin{session}
\begin{alltt}
>> wordsLib.output_words_as_bin();

>> EVAL ``($FCP ODD):word16``;
\end{alltt}
\end{session}
The function \ml{output\_words\_as} is more flexible and allows the number base to vary depending on
the word length and numeric value.  The default pretty-printer (installed when loading \theoryimp{wordsLib}) prints small values in decimal and large values in hexadecimal.
The function \ml{output\_words\_as\_oct} will automatically enable the parsing of octal numbers.

The trace variable \ml{"word printing"} provides an alternative method for changing the output number base --- it is particularly suited to temporarily selecting a number base, for example:
\begin{session}
\begin{alltt}
>> Feedback.trace ("word printing", 1) Parse.term_to_string ``32w``;
\end{alltt}
\end{session}
The choices are as follows: 0 (default) -- small numbers decimal, large numbers hexadecimal; 1 -- binary; 2 -- octal; 3 -- decimal; and 4 -- hexadecimal.

\subsubsection{Types}

You may have noticed that \ty{:word4} and \ty{:word8} have been used as convenient parsing abbreviations for \ty{:\bool[4]} and \ty{:\bool[8]} --- this facility is available for many standard word sizes.  Users wishing to use this notation for non-standard word sizes can use the function \ml{wordsLib.mk\_word\_size}:
\begin{session}
\begin{alltt}
>>+ Lib.try Parse.Type `:word15` handle _ => bool;

>> wordsLib.mk_word_size 15;

>> ``:word15``;
\end{alltt}
\end{session}

\subsubsection{Operator overloading}

The symbols for the standard arithmetic operations (addition, subtraction and multiplication) are overloaded with operators from other standard theories, \ie, for the natural, integer, rational and real numbers.  In many cases type inference will resolve overloading, however, in some cases this is not possible.  The choice of operator will then depend upon the order in which theories are loaded.  To change this behaviour the functions \ml{wordsLib.deprecate\_word} and \ml{wordsLib.prefer\_word} are provided.  For example, in the following session, the selection of word operators is deprecated:
\begin{session}
\begin{alltt}
>> type_of ``a + b``;

>> wordsLib.deprecate_word();

>> type_of ``a + b``;
\end{alltt}
\end{session}
In the above, natural number addition is chosen in preference to word addition.  Conversely, words are preferred over the integers below:
\begin{session}
\begin{alltt}
>>_ load "intLib";
>>__ temp_overload_on("+", ``int_add``);

>> type_of ``a + b``;

>> wordsLib.prefer_word();
>> type_of ``a + b``;
\end{alltt}
\end{session}
Of course, type annotations could have been added to avoid this problem entirely.

\subsubsection{Guessing word lengths}

It can be a nuisance to add type annotations when specifying the return type for operations such as: \holtxt{word\_extract}, \holtxt{word\_concat}, \holtxt{concat\_word\_list} and \holtxt{word\_replicate}.
This is because there is often a ``standard'' length that could be guessed, \eg, concatenation usually sums the constituent word lengths.
A facility for word length guessing is controlled by the reference \ml{wordsLib.guessing\_word\_lengths}, which is false by default.
The guesses are made during a post-processing step that occurs after the application of \ml{Parse.Term}.
This is demonstrated below.
\begin{session}
\begin{alltt}
>> wordsLib.guessing_word_lengths:=true;

>> ``concat_word_list [(4 >< 1) (w:word32); w2; w3]``;
\end{alltt}
\end{session}
In the example above, word length guessing is turned on.  Two guesses are made: the extraction is expected to give a four bit word, and the concatenation gives a twelve bit word ($3 \times 4$).  If non-standard numeric lengths are required then type annotations can be added to avoid guesses being made.  With guessing turned off, the result types would remain as invented type variables, \ie, as alpha and beta above.

\subsection{Simplification and conversions}

The following \emph{simpset} fragments are provided:
\begin{description}
\item[\ml{SIZES\_ss}] evaluates a group of functions that operate over numeric types, such as \holtxt{dimindex} and \holtxt{dimword}.
\item[\ml{BIT\_ss}] tries to simplify occurrences of the function \holtxt{BIT}.
\item[\ml{WORD\_LOGIC\_ss}] simplifies bitwise logic operations.
\item[\ml{WORD\_ARITH\_ss}] simplifies word arithmetic operations.  Subtraction is replaced with multiplication by -1.
\item[\ml{WORD\_SHIFT\_ss}] simplifies shift operations.
\item[\ml{WORD\_ss}] contains all of the above fragments, and also does some extra ground term evaluation.  This fragment is added to \ml{srw\_ss}.
\item[\ml{WORD\_ARITH\_EQ\_ss}] simplifies \holtxt{``a = b``} to \holtxt{``a - b = 0w``}.
\item[\ml{WORD\_BIT\_EQ\_ss}] aggressively expands non-arithmetic bit-vector operations into Boolean expressions.  (Should be used with care -- it includes \ml{fcpLib.FCP\_ss}.)
\item[\ml{WORD\_EXTRACT\_ss}] simplification for a variety of operations: word-to-word conversions; concatenation; shifts and bit-field extraction.  Can be used in situations where \ml{WORD\_BIT\_EQ\_ss} is unsuitable.
\item[\ml{WORD\_MUL\_LSL\_ss}] simplifies multiplication by a word literal into a sum of partial products.
\end{description}
Many of these \emph{simpset} fragments have corresponding conversions.  For example, the conversion \ml{WORD\_ARITH\_CONV} is based on \ml{WORD\_ARITH\_EQ\_ss}, however, it does some extra work to ensure that \holtxt{``a = b``} and \holtxt{``b = a``} convert into the same expression.  Therefore, this conversion is suited to reasoning about the equality of arithmetic word expressions.

The behaviour of the fragments listed above are demonstrated using the following function:
\begin{session}
\begin{alltt}
>> fun conv ss = SIMP_CONV (pure_ss++ss) [];
\end{alltt}
\end{session}
The following session demonstrates \ml{SIZES\_ss}:
\begin{session}
\begin{alltt}
>> conv wordsLib.SIZES_ss ``dimindex(:12)``;

>> conv wordsLib.SIZES_ss ``FINITE univ(:32)``;
\end{alltt}
\end{session}
The fragment \ml{BIT\_ss} converts \holtxt{BIT} into membership test over a set of (high) bit positions:
\begin{session}
\begin{alltt}
>> conv wordsLib.BIT_ss ``BIT 3 5``;

>> conv wordsLib.BIT_ss ``BIT i 123``;
\end{alltt}
\end{session}
This simplification provides some support for reasoning about bitwise operations over arbitrary word lengths.  The arithmetic, logic and shift fragments help tidy up basic word expressions:
\begin{session}
\begin{alltt}
>> conv wordsLib.WORD_LOGIC_ss ``a && 12w || 11w && a``;

>> conv wordsLib.WORD_ARITH_ss ``3w * b + a + 2w * b - a * 4w:word2``;

>> conv wordsLib.WORD_SHIFT_ss ``0w << 12 + a >>> 0 + b << 2 << 3``;
\end{alltt}
\end{session}

The remaining fragments are not included in \ml{wordsLib.WORD\_ss} or \ml{srw\_ss}.  The bit equality fragment is demonstrated below.
\begin{session}
\begin{alltt}
>> SIMP_CONV (std_ss++wordsLib.WORD_BIT_EQ_ss) [] ``a && b = ~0w : word2``;
\end{alltt}
\end{session}
The extract fragment is useful for reasoning about bit-field operations and is best used in combination with \ml{wordsLib.SIZES\_ss} or \ml{wordsLib.WORD\_ss}, for example:
\begin{session}
\begin{alltt}
>> SIMP_CONV (std_ss++wordsLib.SIZES_ss++wordsLib.WORD_EXTRACT_ss) []
     ``(4 -- 1) ((a:word3) @@ (b:word2)) : word5``;
\end{alltt}
\end{session}
Finally, the fragment \ml{WORD\_MUL\_LSL\_ss} is demonstrated below.
\begin{session}
\begin{alltt}
>> conv wordsLib.WORD_MUL_LSL_ss ``5w * a : word8``;
\end{alltt}
\end{session}
Rewriting with the theorem \ml{wordsTheory.WORD\_MUL\_LSL} provides an means to undo this simplification, for example:
\begin{session}
\begin{alltt}
>> SIMP_CONV (std_ss++wordsLib.WORD_ARITH_ss) [wordsTheory.WORD_MUL_LSL]
     ``a << 2 + a : word8``;
\end{alltt}
\end{session}
Obviously, without adding safeguards, this rewrite theorem cannot be deployed when used in combination with the \ml{WORD\_MUL\_LSL\_ss} fragment.

\subsubsection{Decision procedures}

A decision procedure for words is provided in the form of
\ml{blastLib.BBLAST\_PROVE}.  This procedure uses \emph{bit-blasting} ---
converting word expressions into propositions and then using a SAT solver to
decide the goal.\footnote{This approach enables counter-examples to be given
when a goal's negation is satisfiable.} This approach is reasonably general and
can tackle a wide range of bit-vector problems.  However, there are some
limitations: the approach only works for constant word lengths, linear
arithmetic (multiplication by literals) and for shifts and bit-field
extractions with respect to literal values.  Also note that some problems will
be potentially slow to prove, \eg, when word sizes are large and/or when
there are many nested additions (perhaps through multiplication).

The following examples show \ml{BBLAST\_PROVE} in use:
\begin{session}
\begin{alltt}
>>_ load "blastLib";
>> blastLib.BBLAST_PROVE ``a + 2w <+ 4w <=> a <+ 2w \/ 13w <+ a :word4``;

>> blastLib.BBLAST_PROVE ``w2w (a:word8) <+ 256w : word16``;
\end{alltt}
\end{session}
The decision procedure \ml{BBLAST\_PROVE} is based on the conversion
\ml{BBLAST\_CONV}. This conversion can be used to convert bit-vector problems
into a propositional form; for example:
\begin{session}
\begin{alltt}
>> blastLib.BBLAST_CONV ``(((a : word16) + 5w) << 3) ' 5``;
\end{alltt}
\end{session}
There are also bit-blasting tactics: \ml{BBLAST\_TAC} and \ml{FULL\_BBLAST\_TAC}; with only the latter making use of goal assumptions.

\section{The \texttt{HolSat} Library}\label{sec:HolSatLib}
\input{HolSat.tex}


\section{The \texttt{HolQbf} Library}\label{sec:HolQbfLib}
\input{HolQbf.tex}


\section{The \texttt{HolSmt} library}\label{sec:HolSmtLib}
\input{HolSmt.tex}

\section{The \texttt{Quantifier Heuristics} library}\label{sec:QuantHeuristicsLib}
\input{QuantHeuristics.tex}

\section{Tree-Structured Finite Sets and Finite Maps}\label{sec:enumfset}
\input{enumfset.tex}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "description"
%%% End:
