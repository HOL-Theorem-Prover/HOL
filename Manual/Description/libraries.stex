\chapter{Libraries}\label{HOLlibraries}

% LaTeX macros in HOL manuals
%
% \holtxt{..}     for typewriter text that is HOL types or terms.  To
%                 produce backslashes, for /\, \/ and \x. x + 1, use \bs
% \ml{..}         for typewriter text that is ML input, including the
%                 names of HOL API functions, such as mk_const
% \theoryimp{..}  for names of HOL theories.

% text inside \begin{verbatim} should be indented three spaces, unless
% the verbatim is in turn inside a \begin{session}, in which case it
% should be flush with the left margin.


\newcommand{\simpset}{simpset}
\newcommand{\Simpset}{Simpset}

 \newcommand{\term}      {\mbox{\it term}}
 \newcommand{\vstr}      {\mbox{\it vstr}}

A \emph{library} is an abstraction intended to provide a higher level
of organization for \HOL{} applications. In general, a library can
contain a collection of theories, proof procedures, and supporting
material, such as documentation. Some libraries simply provide proof
procedures, such as \ml{simpLib}, while others provide theories and
proof procedures, such as \ml{intLib}. Libraries can include other
libraries.

In the \HOL{} system, libraries are typically represented by \ML{}
structures named following the convention that library \emph{x} will
be found in the \ML{} structure \ml{xLib}. Loading this structure
should load all the relevant sub-components of the library and set
whatever system parameters are suitable for use of the library.

When the \HOL{} system is invoked in its normal configuration, several
useful libraries are automatically loaded. The most basic \HOL{}
library is \ml{boolLib}, which supports the definitions of the \HOL{}
logic, found in the theory \theoryimp{bool}, and provides a useful
suite of definition and reasoning tools.

Another pervasively used library is found in the structure \ml{Parse}
(the reader can see that we are not strictly faithful to our
convention about library naming). The parser library provides support
for parsing and `pretty-printing' of \HOL{} types, terms, and
theorems.

The \ml{boss} library provides a basic collection of standard
theories and high-level proof procedures, and serves as a standard
platform on which to work. It is preloaded and opened when the \HOL{}
system starts up. It includes \ml{boolLib} and
\ml{Parse}. Theories provided include \theoryimp{pair},
\theoryimp{sum}, \theoryimp{option}; the arithmetic theories
\theoryimp{num}, \theoryimp{prim\_rec}, \theoryimp{arithmetic},
and \theoryimp{numeral}; and \theoryimp{list}. Other libraries
included in \ml{bossLib} are \ml{goalstackLib}, which provides
a proof manager for tactic proofs; \ml{simpLib}, which provides
a variety of simplifiers; \ml{numLib}, which provides a decision
procedure for arithmetic; \ml{Datatype}, which provides
high-level support for defining algebraic datatypes; and \ml{tflLib},
which provides support for defining recursive functions.


\section{Parsing and Prettyprinting}
\label{sec:parsing-printing}

Every type and term in \HOL{} is ultimately built by application of
the primitive (abstract) constructors for types and terms. However, in
order to accommodate a wide variety of mathematical expression, \HOL{}
provides flexible infrastructure for parsing and prettyprinting types
and terms through the \ml{Parse} structure.

The term parser supports type inference, overloading, binders, and
various fixity declaration (infix, prefix, postfix, and
combinations). There are also flags for controlling the behaviour
of the parser. Further, the structure of the parser is exposed so that
new parsers can be quickly constructed to support user applications.

The parser is parameterized by grammars for types and terms. The
behaviour of the parser and prettyprinter is therefore usually altered
by grammar manipulations.
%
\index{parsing, of HOL logic@parsing, of \HOL{} logic!grammars for}
%
These can be of two kinds: \emph{temporary} or \emph{permanent}.
Temporary changes should be used in library implementations, or in
script files for those changes that the user does not wish to have
persist in theories descended from the current one.  Permanent changes
are appropriate for use in script-files, and will be in force in all
descendant theories.  Functions making temporary changes are signified
by a leading \ml{temp\_} in their names.

\subsection{Parsing types}
\index{types, in HOL logic@types, in \HOL{} logic!parsing of|(}

The language of types is a simple one.  An abstract grammar for the
language is presented in Figure~\ref{fig:abstract-type-grammar}.  The
actual grammar (with concrete values for the infix symbols and type
operators) can be inspected using the function \ml{type\_grammar}.
\begin{figure}[tbhp]
\newcommand{\nt}[1]{\mathit{#1}}
\newcommand{\tok}[1]{\texttt{\bfseries #1}}
\renewcommand{\bar}{\;\;|\;\;}
\[
\begin{array}{lcl}
\tau &::=& \tau \odot \tau \bar \nt{vtype} \bar \nt{tyop} \bar
           \tok{(} \;\nt{tylist}\;\tok{)} \;\nt{tyop}\bar \tau \;\nt{tyop}
           \bar \tok{(}\;\tau\;\tok{)} \bar \tau\tok{[}\tau\tok{]}\\
\odot &::=& \tok{->} \bar \tok{\#} \bar \tok{+} \bar \cdots\\
\nt{vtype} &::=& \tok{'a} \bar \tok{'b} \bar \tok{'c} \bar \cdots\\
\nt{tylist} &::=& \tau \bar \tau \;\tok{,}\;\nt{tylist}\\
\nt{tyop} &::=& \tok{bool} \bar \tok{list} \bar \tok{num} \bar
           \tok{fun} \bar \cdots
\end{array}
\]
\caption{An abstract grammar for \HOL{} types ($\tau$).  Infixes ($\odot$)
  always bind more weakly than type operators~($\nt{tyop}$) (and
  type-subscripting~($\tau\tok{[}\tau\tok{]}$)), so that
  $\tau_1 \,\odot\, \tau_2 \,\nt{tyop}$ is always parsed as $\tau_1\, \odot\,
  (\tau_2 \,\nt{tyop})$.  Different infixes can have different
  priorities, and infixes at different priority levels can associate
  differently (to the left, to the right, or not at all).  Users can
  extend the categories $\odot$ and $\nt{tyop}$ by making new type
  definitions, and by directly manipulating the grammar.}
\label{fig:abstract-type-grammar}
\end{figure}

\paragraph{Type infixes}
Infixes may be introduced with the function \ml{add\_infix\_type}.
This sets up a mapping from an infix symbol (such as \texttt{->}) to
the name of an existing type operator (such as \texttt{fun}).  The
binary symbol needs to be given a precedence level and an
associativity. See \REFERENCE{} for more details.

\paragraph{Type abbreviations}
\index{type abbreviations}
\index{parsing, of HOL logic@parsing, of \HOL{} logic!type abbreviations}
\index{abbreviations!for types}

Users can abbreviate common type patterns with \emph{abbreviations}.
This is done with the \ML{} function \ml{type\_abbrev}:
\begin{hol}
\begin{verbatim}
   type_abbrev : string * hol_type -> unit
\end{verbatim}
\end{hol}
An abbreviation is a new type operator, of any number of arguments,
that expands into an existing type.  For example, one might develop a
light-weight theory of numbers extended with an infinity, where the
representing type was \holtxt{num option} (\holtxt{NONE} would
represent the infinity value).  One might set up an abbreviation
\holtxt{infnum} that expanded to this underlying type.
Polymorphic patterns are supported as well.  For example, as described
in Section~\ref{sec:theory-of-sets}, the abbreviation \holtxt{set}, of
one argument, is such that \holtxt{:'a set} expands into the type
\holtxt{:'a -> bool}, for any type \holtxt{:'a}.

When types come to be printed, the expansion of abbreviations done by the parser is reversed if the \ml{type\_abbrev\_pp} entry-point is used; otherwise the abbreviation is for input only.
For more information, see \ml{type\_abbrev}'s entry in \REFERENCE.
\index{types, in HOL logic@types, in \HOL{} logic!parsing of|)}

\index{special syntactic forms for scripts!Type@\ml{Type}}
\index{Type (special syntactic form)@\ml{Type} (special syntactic form)}
There are special syntactic forms available for both type abbreviation entry-points.
Instead of
\begin{hol}
\begin{alltt}
   val _ = type_abbrev("set", \fldq:'a -> bool\frdq)
\end{alltt}
\end{hol}
one can write
\begin{hol}
\begin{alltt}
   Type set = \fldq:'a -> bool\frdq
\end{alltt}
\end{hol}
and if an underlying call to \ml{type\_abbrev\_pp} is desired, the \ml{[pp]} ``attribute'' should be added to the name, thus:
\begin{hol}
\begin{alltt}
   Type set[pp] = \fldq:'a -> bool\frdq
\end{alltt}
\end{hol}

\index{local@\ml{local}!as attribute for type abbreviations}
If a ``temporary'' abbreviation is required (one whose effect will not be apparent in descendent theories), then the \ml{local} attribute can also be added to the \ml{Type} syntax, or the \ML{} functions \ml{temp\_type\_abbrev} and \ml{temp\_type\_abbrev\_pp} can be used.


\subsection{Parsing terms}

The term parser provides a grammar-based infrastructure for supporting
concrete syntax for formalizations. Usually, the \HOL{} grammar gets
extended when a new definition or constant specification is made. (The
introduction of new constants is discussed in
Sections~\ref{sec:constant-definitions} and \ref{conspec}.) However,
any identifier can have a parsing status attached at any time.
In the following, we explore some of the capabilities of the
\HOL{} term parser.


\subsubsection{Parser architecture}
\label{sec:parser:architecture}

The parser turns strings into terms.  It does this in the following
series of phases, all of which are influenced by the provided grammar.
Usually this grammar is the default global grammar, but users can
arrange to use different grammars if they desire.  %
\index{parsing, of HOL logic@parsing, of \HOL{} logic!grammars for}
%
Strictly, parsing occurs after lexing has split the input into a
series of tokens.  For more on lexing, see Section~\ref{HOL-lex}.
\begin{description}
\item[Concrete Syntax:] Features such as infixes, binders and mix-fix
  forms are translated away, creating an intermediate, ``abstract
  syntax'' form (ML type \ml{Absyn}).  The possible fixities are
  discussed in Section~\ref{sec:parseprint:fixities} below.  Concrete
  syntax forms are added to the grammar with functions such as
  \ml{add\_rule} and \ml{set\_fixity} (for which, see the \REFERENCE).
  The action of this phase of parsing is embodied in the function
  \ml{Absyn}.

  The \ml{Absyn} data type is constructed using constructors \ml{AQ}
  (an antiquote, see Section~\ref{sec:quotation-antiquotation}); %
%
  \index{antiquotation, in HOL logic terms@antiquotation, in \HOL{} logic terms}%
%
  \ml{IDENT} (an identifier); \ml{QIDENT} (a qualified identifier,
  given as \holtxt{thy\$ident}%
\index{ dollar sign, in HOL logic parser@\ml{\$} (dollar sign, in \HOL{} logic parser)!generating qualified identifiers}%
); \ml{APP} (an application of one form
  to another); \ml{LAM} (an abstraction of a variable over a body),
  and \ml{TYPED} (a form accompanied by a type constraint\footnote{The
    types in \ml{Absyn} constraints are not full HOL types, but values
    from another intermediate type, \ml{Pretype}.}, see
  Section~\ref{sec:parseprint-type-constraints}).  At this stage of
  the translation, there is no distinction made between constants and
  variables: though \ml{QIDENT} forms must be constants, users are
  also able to refer to constants by giving their bare names.

  It is possible for names that occur in the \ml{Absyn} value to be
  different from any of the tokens that appeared in the original
  input.  For example, the input
\begin{alltt}
   \fholquote{if P then Q else R}
\end{alltt} will turn into
\begin{verbatim}
   APP (APP (APP (IDENT "COND", IDENT "P"), IDENT "Q"), IDENT "R")
\end{verbatim}
  (This is slightly simplified output: the various constructors for
  \ml{Absyn}, including \ml{APP}, also take location parameters.)

  The standard grammar includes a rule that associates the special
  mix-fix form for if-then-else expressions with the underlying
  ``name'' \holtxt{COND}.  It is \holtxt{COND} that will eventually be
  resolved as the constant \holtxt{bool\dol{}COND}.

  If the ``quotation'' syntax with a bare dollar is used,%
%
\index{ dollar sign, in HOL logic parser@\ml{\$} (dollar sign, in \HOL{} logic parser)!as escape character}%
\index{tokens!suppressing parsing behaviour of|(}%
%
then this phase of the parser will not treat strings as part of a
special form.  For example, \holtxt{\fholquote{\dol{}if~P}} turns into
the \ml{Absyn} form
\begin{verbatim}
   APP(IDENT "if", IDENT "P")
\end{verbatim}
  \emph{not} a form involving \holtxt{COND}.

  More typically, one often writes something like
  \holtxt{\fholquote{\$+~x}}, which generates the abstract syntax
\begin{verbatim}
   APP(IDENT "+", IDENT "x")
\end{verbatim}
  Without the dollar-sign, the concrete syntax parser would complain
  about the fact that the infix plus did not have a left-hand
  argument.  When the successful result of parsing is handed to the
  next phase, the fact that there is a constant called \holtxt{+} will
  give the input its desired meaning.

  Symbols can also be ``escaped'' by enclosing them in parentheses.
  Thus, the above could be written \holtxt{\fholquote{(+)~x}} for the
  same effect.
\index{tokens!suppressing parsing behaviour of|)}

  The user can insert intermediate transformation functions of their
  own design into the parsing processing at this point.  This is done
  with the function
\begin{verbatim}
   add_absyn_postprocessor
\end{verbatim}
  The user's function will be of type \ml{Absyn~->~Absyn} and can
  perform whatever changes are appropriate.  Like all other aspects of
  parsing, these functions are part of a grammar: if the user doesn't
  want to see a particular function used, they can arrange for parsing
  to be done with respect to a different grammar.

\item[Name Resolution:] The bare \ml{IDENT} forms in the \ml{Absyn}
  value are resolved as free variables, bound names or constants.
  This process results in a value of the \ml{Preterm} data type, which
  has similar constructors to those in \ml{Absyn} except with forms
  for constants. %
  \index{parsing, of HOL logic@parsing, of \HOL{} logic!preterms}%
  A string can be converted straight to a \ml{Preterm} by way of the
  \ml{Preterm} function.

  A bound name is the first argument to a \ml{LAM} constructor, an
  identifier occurring on the left-hand side of a case-expression's
  arrow, or an identifier occurring within a set comprehension's
  pattern. A constant is a string that is present in the domain of the
  grammar's ``overload map''.  Free variables are all other identifiers.
  Free variables of the same name in a term will all have the same
  type.  Identifiers are tested to see if they are bound, and then to
  see if they are constants.  Thus it is possible to write
\begin{verbatim}
   \SUC. SUC + 3
\end{verbatim}
  and have the string \holtxt{SUC} be treated as a number in the
  context of the given abstraction, rather than as the successor
  constant.

\index{parsing, of HOL logic@parsing, of \HOL{} logic!overloading}
\index{overloading|see{parsing, of \HOL{} logic, overloading}}
  The ``overload map'' is a map from strings to lists of terms.  The
  terms are usually just constants, but can be arbitrary terms (giving
  rise to ``syntactic macros'' or ``patterns'').\index{syntactic macros}
  This facility is used to allow a name such as \holtxt{+} to map to
  different addition constants in theories such as
  \theoryimp{arithmetic}, \theoryimp{integer}, and
  \theoryimp{words}. In this way the ``real'' names of the constants can
  be divorced from what the user types.  In the case of addition, the
  natural number plus actually is called \holtxt{+} (strictly,
  \holtxt{arithmetic\$+}); but over the integers, it is
  \holtxt{int\_add}, and over words it is \holtxt{word\_add}.  (Note
  that because each constant is from a different theory and thus a
  different namespace, they could all have the name \holtxt{+}.)

  \index{type inference!in HOL parser@in \HOL{} parser}
  When name resolution determines that an identifier should be treated
  as a constant, it is mapped to a preterm form that lists all of the
  possibilities for that string.  Subsequently, because the terms in
  the range of the overload map will typically have different types,
  type inference will often eliminate possibilities from the list.  If
  multiple possibilities remain after type inference has been
  performed, then a warning will be printed, and one of the
  possibilities will be chosen.  (Users can control which terms are
  picked when this situation arises.)

  When a term in the overload map is chosen as the best option, it is
  substituted into the term at appropriate position.  If the term is a
  lambda abstraction, then as many $\beta$-reductions are done as
  possible, using any arguments that the term has been applied to.  It
  is in this way that a syntactic pattern can process arguments.  (See
  also Section~\ref{sec:parser:syntactic-patterns} for more on
  syntactic patterns.)
\item[Type Inference:] %
  \index{type inference!in HOL parser@in \HOL{} parser}%
  All terms in the \HOL{} logic are well-typed.  The kernel enforces
  this through the API for the \ml{term} data type.  (In particular,
  the \ml{mk\_comb} function %
  \index{mk_comb@\ml{mk\_comb}}%
  checks that the type of the first argument is a function whose
  domain is equal to the type of the second argument.)  The parser's
  job is to turn user-supplied strings into terms.  For convenience,
  it is vital that the users do not have to provide types for all of the
  identifiers they type. (See Section~\ref{sec:parser:type-inference}
  below.)

  In the presence of overloaded identifiers, type inference may not be
  able to assign a unique type to all constants.  If multiple
  possibilities exist, one will be picked when the \ml{Preterm} is finally
  converted into a genuine term.
\item[Conversion to Term:]%
  When a \ml{Preterm} has been type-checked, the final conversion from
  that type to the \ml{term} type is mostly straightforward.  The user
  can insert further processing at this point as well, so that a
  user-supplied function modifies the result before the parser
  returns.
\end{description}

\subsubsection{Unicode characters}
\label{sec:parser:unicode-characters}

\index{Unicode|(}\index{UTF-8}
\index{parsing, of HOL logic@parsing, of \HOL{} logic!Unicode characters|(}
It is possible to have the \HOL{} parsing and printing infrastructure
use Unicode characters (written in the UTF-8 encoding).  This makes it
possible to write and read terms such as
\begin{alltt}
   \(\forall\)x. P x \(\land\) Q x
\end{alltt}
rather than
\begin{alltt}
   !x. P x /\bs{} Q x
\end{alltt}
If they wish, users may simply define constants that have Unicode
characters in their names, and leave it at that.  The problem with
this approach is that standard tools will likely then create theory
files that include (illegal) \ML{} bindings like \ml{val $\rightarrow$\_def =
  \dots}.  The result will be \ml{...Theory.sig} and
\ml{...Theory.sml} files that fail to compile, even though the call to
\ml{export\_theory} may succeed. This problem can be finessed through
the use of functions like \ml{set\_MLname}, but it's probably best
practice to only use alphanumerics in the names of constants, and to
then use functions like \ml{overload\_on} and \ml{add\_rule} to create
Unicode syntax for the underlying constant.

If users have fonts with the appropriate repertoire of characters to display their syntax, and are confident that any other users of their theories will too, then this is perfectly reasonable.
However, if users wish to retain some backwards compatibility a provide a pure ASCII syntax, they can do so by defining that pure ASCII syntax first.
Having done this, they can create a Unicode version of the syntax with the function \ml{Unicode.unicode\_version}.
\index{traces, controlling HOL feedback@traces, controlling \HOL{} feedback!Unicode in pretty-printing}
Then, either Unicode or ASCII characters can be used to input the syntax, and, while the trace variable \ml{"PP.avoid_unicode"} is $1$, the ASCII syntax will be used for printing.
If the trace is set to $0$, then again, both syntaxes can be used to \emph{write} the terms, but the pretty-printer will prefer the Unicode syntax when the terms are printed by the system.

For example, in \ml{boolScript.sml}, the Unicode character for logical
and (\texttt{$\land$}), is set up as a Unicode alternative for
\texttt{/\bs} with the call
\begin{verbatim}
   val _ = unicode_version {u = UChar.conj, tmnm = "/\\"};
\end{verbatim}
(In this context, the \ml{Unicode} structure has been \ml{open}-ed,
giving access also to the structure \ml{UChar} which contains bindings
for the Greek alphabet, and some common mathematical symbols. )

The argument to \ml{unicode\_version} is a record with fields \ml{u}
and \ml{tmnm}.  Both are strings.  The \ml{tmnm} field can either be
the name of a constant, or a token appearing in a concrete syntax rule
(possibly mapping to some other name).  If the \ml{tmnm} is only the
name of a constant, then, with the trace variable enabled, the string
\ml{u} will be overloaded to the same name.  If the \ml{tmnm} is the
same as a concrete syntax rule's token, then the behaviour is to
create a new rule mapping to the same name, but with the string \ml{u}
used as the token.

\paragraph{Lexing rules with Unicode characters}
%
\index{tokens!Unicode characters}
\index{identifiers, in HOL logic@identifiers, in \HOL{} logic!non-aggregating characters}
%
Roughly speaking, \HOL{} considers characters to be divided into three
classes: alphanumerics, non-aggregating symbols and symbols.  This
affects the behaviour of the lexer when it encounters strings of
characters.  Unless there is a specific ``mixed'' token already in the
grammar, tokens split when the character class changes.  Thus, in the
string
\begin{verbatim}
   ++a
\end{verbatim}
the lexer will see two tokens, \holtxt{++} and \holtxt{a}, because
\holtxt{+} is a symbol and \holtxt{a} is an alphanumeric.  The
classification of the additional Unicode characters is very
simplistic: all Greek letters except \holtxt{$\lambda$} are alphanumeric;
the logical negation symbol \holtxt{$\neg$} is non-aggregating; and
everything else is symbolic.  (The exception for \holtxt{$\lambda$} is to
allow strings like \holtxt{$\lambda$x.x} to lex into \emph{four} tokens.)

\index{parsing, of HOL logic@parsing, of \HOL{} logic!Unicode characters|)}
\index{Unicode|)}

\subsubsection{Overloading and Syntactic Patterns (``macros'')}
\label{sec:parser:syntactic-patterns}
\index{abbreviations!for terms}
\index{parsing, of HOL logic@parsing, of \HOL{} logic!overloading}
\index{parsing, of HOL logic@parsing, of \HOL{} logic!term abbreviations}
\index{parsing, of HOL logic@parsing, of \HOL{} logic!syntactic patterns|(}

The ``overload map'' mentioned previously is actually a combination of maps, one for parsing, and one for printing.
The parsing map is from names to lists of terms, and determines how the names that appear in a \ml{Preterm} will translate into terms.
In essence, bound names turn into bound variables, unbound names not in the domain of the map turn into free variables, and unbound names in the domain of the map turn into one of the elements of the set associated with the given name.
Each term in the set of possibilities may have a different type, so type inference will choose from those that have types consistent with the rest of the given term.
If the resulting list contains more than one element, then the term appearing earlier in the list will be chosen.

The most common use-case for the overload map is have names map to constants.
In this way, for example, the various numeric theories can map the string \ml{"+"} to the relevant notions of addition, each of which is a different constant.
However, the system has extra flexibility because names can map to arbitrary terms.
For example, it is possible to map to specific type-instances of constants.  Thus, the string \ml{"<=>"} maps to equality, but where the arguments are forced to be of type \fholquote{:bool}.

Moreover, if the term mapped to is a lambda-abstraction (\ie, of the form $\lambda x.\;M$), then the parser will perform all possible $\beta$-reductions for that term and the arguments accompanying it.
For example, in \theoryimp{boolTheory} and its descendants, the string \ml{"<>"} is overloaded to the term \fholquote{\holtxt{\bs{}x~y.~\td{}(x~=~y)}}.
Additionally, \ml{"<>"} is set up at the concrete syntax level as an infix.
When the user inputs \fholquote{\holtxt{x~\lt\gt~y}}, the resulting
\ml{Absyn} value is
\begin{verbatim}
   APP(APP(IDENT "<>", IDENT "x"), IDENT "x")
\end{verbatim}
The \ml{"x"}  and \ml{"y"} identifiers will map to free variables, but
the \ml{"<>"} identifier maps to a list containing
\fholquote{\holtxt{\bs{}x~y.~\td(x~=~y)}}. This term has type
\begin{verbatim}
   :'a -> 'a -> bool
\end{verbatim}
and the polymorphic variables are generalisable, allowing type
inference to give appropriate (identical) types to \ml{x} and \ml{y}.
Assuming that this option is the only overloading for \ml{"<>"} left
after type inference, then the resulting term will be
\holtxt{\td(x~=~y)}.  Better, though this will be the underlying
structure of the term in memory, it will actually print as
\fholquote{\holtxt{x~\lt\gt~y}}.

If the term mapped to in the overload map contains any free variables,
these variables will not be instantiated in any way.  In particular,
if these variables have polymorphic types, then the type variables in
those types will be constant: not subject to instantiation by type
inference.

\index{special syntactic forms for scripts!Overload@\ml{Overload}}
\index{Overload (special syntactic form)@\ml{Overload} (special syntactic form)}
The prettiest way to introduce entries into the overload map is to use the \ml{Overload} syntactic form:
\begin{alltt}
   Overload name[attrs] = \fholquote{term/pattern}
\end{alltt}
Depending on the choice of attributes, this will map into a call to one of a variety of underlying ``overload-on'' functions.
\index{overload_on (ML function)@\ml{overload\_on} (\ML{} function)}
The default (without any attributes) calls \ml{overload\_on}, whose effect is to make an entry in the map that will be exported with the theory.
The overloading effect thus persists in descendent theories.
\index{local@\ml{local}!as overloading attribute}
If the \ml{local} attribute is used, this export doesn't occur, and the overloading effect is only visible for the rest of the containing script file.%
\footnote{The use of \ml{local} induces a call to the \ml{temp\_overload\_on} function.}

\index{inferior@\ml{inferior}!as overloading attribute}
The other attribute that can be used is \ml{inferior}, which causes the entry to be made, but makes it the pretty-printer's last choice when matching terms are printed.

\paragraph{Pretty-printing and syntactic patterns} The second part of
the ``overload map'' is a map from terms to strings, specifying how
terms should be turned back into identifiers.  (Though it does not
actually construct an \ml{Absyn} value, this process reverses the name
resolution phase of parsing, producing something that is then printed
according to the concrete syntax part of the given grammar.)

Because parsing can map single names to complicated term structures,
printing must be able to take a complicated term structure back to a
single name.  It does this by performing
term matching.%
\index{matching!in pretty-printing terms}%
%
\footnote{The matching done is first-order; contrast the higher-order
  matching done in the simplifier.}
If multiple patterns match the same term, then the printer picks the
most specific match (the one that requires least instantiation of the
pattern's variables).  If this still results in multiple, equally
specific, possibilities, the most recently added pattern takes
precedence.  (Users can thus manipulate the printer's preferences by
making otherwise redundant \ml{Overload} declarations.)

In the example of the not-equal-to operator above, the pattern will be
\holtxt{\~{}(?x = ?y)}, where the question-marks indicate instantiable
pattern variables.  If a pattern includes free variables (recall that
the \ml{x} and \ml{y} in this example were bound by an abstraction),
then these will not be instantiable.

There is one further nicety in the use of this facility: ``bigger''
matches, covering more of a term, take precedence.   The difficulty
this can cause is illustrated in the \holtxt{IS\_PREFIX} pattern from
\theoryimp{rich\_listTheory}.  For the sake of backwards compatibility
this identifier maps to
\begin{verbatim}
   \x y. isPREFIX y x
\end{verbatim}
where \holtxt{isPREFIX} is a constant from \theoryimp{listTheory}.
(The issue is that \holtxt{IS\_PREFIX} expects its arguments in
reverse order to that expected by \holtxt{isPREFIX}.) Now, when this
macro is set up the overload map already contains a mapping from the
string \holtxt{"isPREFIX"} to the constant \holtxt{isPREFIX} (this
happens with every constant definition).  But after the call
establishing the new pattern for \holtxt{IS\_PREFIX}, the
\holtxt{isPREFIX} form will no longer be printed.  Nor is it enough,
to repeat the call
\begin{alltt}
   overload_on("isPREFIX", \fholquote{isPREFIX})
\end{alltt}
Instead (assuming that \holtxt{isPREFIX} is indeed the preferred
printing form), the call must be
\begin{alltt}
   overload_on("isPREFIX", \fholquote{\bs{}x y. isPREFIX x y})
\end{alltt}
so that \ml{isPREFIX}'s pattern is as long as \ml{IS\_PREFIX}'s.
\index{parsing, of HOL logic@parsing, of \HOL{} logic!syntactic patterns|)}


\subsubsection{Type constraints}
\label{sec:parseprint-type-constraints}

\index{type constraint!in HOL parser@in \HOL{} parser}
A term can be constrained to be of a certain type.  For example,
\holtxt{X:bool} constrains the variable \holtxt{X} to have type
\holtxt{bool}. An attempt to constrain a
term inappropriately will raise an exception: for example,
\begin{hol}
\begin{verbatim}
   if T then (X:ind) else (Y:bool)
\end{verbatim}
\end{hol}
will fail because both branches of a conditional must be of the same
type.  Type constraints can be seen as a suffix that binds more
tightly than everything except function application.  Thus $\term\
\ldots\ \term : \type$ is equal to $(\term\ \ldots\ \term) :
\type$, but $x < y:\holtxt{num}$ is a legitimate constraint on just
the variable $y$.

The inclusion of \holtxt{:} in the symbolic identifiers means that some
constraints may need to be separated by white space. For example,
\begin{hol}
\begin{verbatim}
   $=:bool->bool->bool
\end{verbatim}
\end{hol}
will be broken up by the \HOL{} lexer as
\begin{hol}
\begin{verbatim}
   $=: bool -> bool -> bool
\end{verbatim}
\end{hol}
and parsed as an application of the symbolic identifier \holtxt{\$=:} to
the argument list of terms [\holtxt{bool}, \holtxt{->}, \holtxt{bool},
\holtxt{->}, \holtxt{bool}]. A well-placed space will avoid this problem:
\begin{hol}
\begin{verbatim}
   $= :bool->bool->bool
\end{verbatim}
\end{hol}
is parsed as the symbolic identifier ``='' constrained by a type.
Instead of the \holtxt{\$}, one can also use parentheses to remove
special parsing behaviour from lexemes:
\begin{hol}
\begin{verbatim}
   (=):bool->bool->bool
\end{verbatim}
\end{hol}

\subsubsection{Type inference}
\label{sec:parser:type-inference}

\index{type inference!in HOL parser@in \HOL{} parser|(}
Consider the term \holtxt{x = T}: it (and all of its subterms)
has a type in the \HOL{} logic. Now, \holtxt{T} has type \holtxt{bool}. This
means that the constant \holtxt{=} has type \holtxt{xty -> bool -> bool},
for some type \holtxt{xty}. Since the type scheme for \holtxt{=} is
\holtxt{'a -> 'a -> bool}, we know that \holtxt{xty} must in fact be
\holtxt{bool} in order for the type instance to be well-formed. Knowing
this, we can deduce that the type of \holtxt{x} must be \holtxt{bool}.

Ignoring the jargon (``scheme'' and ``instance'') in the previous
paragraph, we have conducted a type assignment to the term structure,
ending up with a well-typed term. It would be very tedious for users
to conduct such argumentation by hand for each term entered to \HOL{}.
Thus, \HOL{} uses an adaptation of Milner's type inference algorithm
for \ML{} when constructing terms via parsing. At the end of type
inference, unconstrained type variables get assigned names by the system.
Usually, this assignment does the right thing. However, at times, the
most general type is not what is desired and the user must add type
constraints to the relevant subterms. For tricky situations, the
global variable \ml{show\_types} can be assigned. When this flag is
set, the prettyprinters for terms and theorems will show how types
have been assigned to subterms. If you do not want the system to
assign type variables for you, the global variable
\ml{guessing\_tyvars} can be set to \ml{false}, in which case the
existence of unassigned type variables at the end of type inference
will raise an exception.
\index{type inference!in HOL parser@in \HOL{} parser|)}


\subsubsection{Overloading}
\label{sec:parsing:overloading}

\index{parsing, of HOL logic@parsing, of \HOL{} logic!overloading|(}
A limited amount of overloading resolution is performed by the term
parser. For example, the `tilde' symbol (\holtxt{\~{}})
denotes boolean negation in the initial theory of \HOL, and it also denotes
the additive inverse in the \ml{integer} and
\ml{real} theories. If we load the \ml{integer}
theory and enter an ambiguous term featuring \holtxt{\~{}}, the
system will inform us that overloading resolution is being performed.

\setcounter{sessioncount}{0}
\begin{session}
\begin{alltt}
>> load "integerTheory";

>> Term `~~x`;

>> type_of it;
\end{alltt}
\end{session}

A priority mechanism is used to resolve multiple possible choices. In
the example, \holtxt{\~{}} could be consistently chosen to have type
\holtxt{:bool -> bool} or \holtxt{:int -> int}, and the
mechanism has chosen the former. For finer control, explicit type
constraints may be used. In the following session, the
\holtxt{\~{}\~{}x} in the first quotation has type \holtxt{:bool},
while in the second, a type constraint ensures that \holtxt{\~{}\~{}x} has
type \holtxt{:int}.

\begin{session}
\begin{alltt}
>> show_types := true;

>> Term `~(x = ~~x)`;

>> Term `~(x:int = ~~x)`;
\end{alltt}
\end{session}

Note that the symbol \holtxt{\~{}} stands for two different constants in
the second quotation; its first occurrence is boolean negation, while
the other two occurrences are the additive inverse operation for
integers.
\index{parsing, of HOL logic@parsing, of \HOL{} logic!overloading|)}

\subsubsection{Fixities}
\label{sec:parseprint:fixities}

In order to provide some notational flexibility, constants come in various flavours or {\it fixities}: besides being an ordinary constant (with no fixity), constants can also be {\it binders}, {\it prefixes}, {\it suffixes}, {\it infixes}, or {\it closefixes}.
More generally, terms can also be represented using reasonably arbitrary {\it mixfix} specifications.
The degree to which terms bind their associated arguments is known as precedence.
The higher this number, the tighter the binding.
For example, when introduced, \verb-+- has a precedence of 500, while the tighter binding multiplication (\verb+*+) has a precedence of 600.

\paragraph{Binders}

A binder is a construct that binds a variable; for example, the
universal quantifier. In \HOL, this is represented using a trick that
goes back to Alonzo Church: a binder is a constant that takes a lambda
abstraction as its argument. The lambda binding is used to implement
the binding of the construct. This is an elegant and uniform solution.
Thus the concrete syntax \verb+!v. M+ is represented by the
application of the constant \verb+!+ to the abstraction \verb+(\v. M)+.

The most common binders are \verb+!+, \verb+?+, \verb+?!+, and
\verb+@+. Sometimes one wants to iterate applications of the same
binder, \eg,
\begin{alltt}
   !x. !y. ?p. ?q. ?r. \term.
\end{alltt}
This can instead be rendered
\begin{alltt}
   !x y. ?p q r. \term.
\end{alltt}

\paragraph{Infixes}

Infix constants can associate in one of three different ways: right,
left or not at all.  (If \holtxt{+} were non-associative, then
\holtxt{3 + 4 + 5} would fail to parse; one would have to write
\holtxt{(3 + 4) + 5} or \holtxt{3 + (4 + 5)} depending on the desired
meaning.)  The precedence ordering for the initial set of infixes is
\holtxt{/\bs}, \holtxt{\bs/}, \holtxt{==>}, \holtxt{=},
\begin{Large}\holtxt{,}\end{Large} (comma\footnote{When
  \theoryimp{pairTheory} has been loaded.}). Moreover, all of these
constants are right associative. Thus
\begin{hol}
\begin{verbatim}
   X /\ Y ==> C \/ D, P = E, Q
\end{verbatim}
\end{hol}
%
is equal to
%
\begin{hol}
\begin{verbatim}
   ((X /\ Y) ==> (C \/ D)), ((P = E), Q).
\end{verbatim}
\end{hol}
%
\noindent An expression
\[
\term \; \holtxt{<infix>}\; \term
\]
is internally represented as
\[
((\holtxt{<infix>}\; \term)\; \term)
\]

\paragraph{Prefixes}

Where infixes appear between their arguments, prefixes appear before theirs.
This might initially appear to be the same thing as happens with normal function application where the symbol on the left simply has no fixity: is $f$ in $f(x)$ not acting as a prefix?
Actually though, in a term such as $f(x)$, where $f$ and $x$ do not have fixities, the syntax is treated as if there is an invisible infix function application between the two tokens: $f\cdot{}x$.
This infix operator binds tightly, so that when one writes $f\,x + y$, the parse is $(f\cdot{}x) + y$.\footnote{There are tighter infix operators: the dot in field selection causes $f\,x.fld$ to parse as $f\cdot(x.fld)$.\index{record types!field selection functions and notation}}
It is then useful to allow for genuine prefixes so that operators can live at different precedence levels than function application.
An example of this is \verb+~+, logical negation.
This is a prefix with lower precedence than function application.
Normally
\[
   f\;x\; y\qquad \mbox{is parsed as}\qquad (f\; x)\; y
\] but \[
  \holtxt{\~{}}\; x\; y\qquad\mbox{is parsed as}\qquad
  \holtxt{\~{}}\; (x\; y)
\]
because the precedence of \verb+~+ is lower than that of function application.
The unary negation symbol would also typically be defined as a prefix, if only to allow one to write \[
  {\it negop}\,{\it negop}\,3
\]
(whatever {\it negop} happened to be) without needing extra parentheses.

On the other hand, the \holtxt{univ} syntax for the universal set (see Section~\ref{sec:theory-of-sets}\index{universal set}) is an example of a prefix operator that binds more tightly than application.
This means that \holtxt{f\,univ(:'a)} is parsed as \holtxt{f(univ(:'a))}, not \holtxt{(f univ)(:'a)} (which parse would fail to type-check).

\paragraph{Suffixes}

Suffixes appear after their arguments.
There are suffixes \holtxt{\^{}+}, \holtxt{\^{}*} and \holtxt{\^{}=} corresponding to the transitive, the reflexive and transitive, and the ``equivalence'' closure used in \ml{relationTheory} (Section~\ref{relation}).
Suffixes are
associated with a precedence just as infixes and prefixes are.
If \holtxt{p} is a prefix, \holtxt{i} an infix, and \holtxt{s} a
suffix, then there are six possible orderings for the three different
operators based on their precedences, giving five parses for
$\holtxt{p}\; t_1\; \holtxt{i}\; t_2\; \holtxt{s}$ depending on the
relative precedences:
\[
\begin{array}{cl}
\mbox{\begin{tabular}{c}Precedences\\(lowest to highest)\end{tabular}} &
\multicolumn{1}{c}{\mbox{Parses}}\\
\hline
p,\;i,\;s & \holtxt{p}\;(t_1\;\holtxt{i}\;(t_2\;\holtxt{s}))\\
p,\;s,\;i & \holtxt{p}\;((t_1\;\holtxt{i}\;t_2)\;\holtxt{s})\\
i,\;p,\;s & (\holtxt{p}\;t_1)\;\holtxt{i}\;(t_2\;\holtxt{s})\\
i,\;s,\;p & (\holtxt{p}\;t_1)\;\holtxt{i}\;(t_2\;\holtxt{s})\\
s,\;p,\;i & (\holtxt{p}\;(t_1\;\holtxt{i}\;t_2))\;\holtxt{s}\\
s,\;i,\;p & ((\holtxt{p}\;t_1)\;\holtxt{i}\;t_2)\;\holtxt{s}\\
\end{array}
\]

\paragraph{Closefixes}

Closefix terms are operators that completely enclose their arguments.
An example one might use in the development of a theory of
denotational semantics is semantic brackets.  Thus, the \HOL{} parsing
facilities can be configured to allow one to write \holtxt{denotation x}
as \holtxt{[| x |]}.  Closefixes are not associated with precedences
because they can not compete for arguments with other operators.


\subsubsection{Parser tricks and magic}

Here we describe how to achieve some useful effects with the
parser in \HOL{}.

\begin{description}

\item[Aliasing] If one wants a special syntax to be an ``alias'' for a
  normal \HOL{} form, this is easy to achieve; both examples so far
  have effectively done this.  However, if one just wants to have a
  normal one-for-one substitution of one string for another, one can't
  use the grammar/syntax phase of parsing to do this.  Instead, one
  can use the overloading mechanism.  For example, let us alias
  \texttt{MEM} for \texttt{IS\_EL}.  We need to use the function
  \texttt{overload\_on} to overload the original constant for the new
  name:
\begin{alltt}
   val _ = overload_on ("MEM", \fholquote{IS_EL});
\end{alltt}

\item[Making addition right associative] If one has a number of old
  scripts that assume addition is right associative because this is
  how \HOL{} used to be, it might be too much pain to convert.  The trick
  is to remove all of the rules at the given level of the grammar, and
  put them back as right associative infixes.  The easiest way to tell
  what rules are in the grammar is by inspection (use
  \ml{term\_grammar()}).  With just \ml{arithmeticTheory}
  loaded, the only infixes at level 500 are \holtxt{+} and
  \holtxt{-}.  So, we remove the rules for them:
\begin{verbatim}
   val _ = app temp_remove_rules_for_term ["+", "-"];
\end{verbatim}
  \noindent And then we put them back with the appropriate
  associativity:
\begin{verbatim}
   val _ = app (fn s => temp_add_infix(s, 500, RIGHT)) ["+", "-"];
\end{verbatim}
\noindent Note that we use the \ml{temp\_} versions of these two
functions so that other theories depending on this one won't be
affected.  Further note that we can't have two infixes at the same
level of precedence with different associativities, so we have to
remove both operators, not just addition.

\item[Mix-fix syntax for {\it if-then-else}:]
\index{conditionals, in HOL logic@conditionals, in \HOL{} logic!printing of}
%
The first step in bringing this about is to look at the general shape
of expressions of this form.  In this case, it will be:
%
\[
  \holtxt{if}\;\; \dots \;\;\holtxt{then}\;\;\dots\;\;
  \holtxt{else}\;\;\dots
  \]
%
 Because there needs to be a ``dangling'' term to the right, the
  appropriate fixity is \ml{Prefix}.  Knowing that the underlying
  term constant is called \holtxt{COND}, the simplest way to achieve
  the desired syntax is:
\begin{verbatim}
val _ = add_rule
   {term_name = "COND", fixity = Prefix 70,
    pp_elements = [TOK "if", BreakSpace(1,0), TM, BreakSpace(1,0),
                   TOK "then", BreakSpace(1,0), TM, BreakSpace(1,0),
                   TOK "else", BreakSpace(1,0)],
    paren_style = Always,
    block_style = (AroundEachPhrase, (PP.CONSISTENT, 0))};
\end{verbatim}
\noindent The actual rule is slightly more complicated, and
may be found in the sources for the theory \theoryimp{bool}.

\item[Mix-fix syntax for term substitution:]

Here the desire is to be able to write something like:
\[
  \mbox{\texttt{[}}\,t_1\,\mbox{\texttt{/}}\,t_2\,\mbox{\texttt{]}}\,t_3
\]
denoting the substitution of $t_1$ for $t_2$ in $t_3$, perhaps
translating to \holtxt{SUB $t_1$ $t_2$ $t_3$}.  This looks
like it should be another \ml{Prefix}, but the choice of the
square brackets (\holtxt{[} and \holtxt{]}) as delimiters would
conflict with the concrete syntax for list literals if this was done.
Given that list literals are effectively of the \ml{CloseFix}
class, the new syntax must be of the same class.  This is easy enough
to do: we set up syntax
\[
\holtxt{[}\,t_1\,\holtxt{/}\,t_2\,\holtxt{]}
\]
to map to \holtxt{SUB $t_1$ $t_2$}, a value of a functional
type, that when applied to a third argument will look
right.\footnote{Note that doing the same thing for the
  \textit{if-then-else} example in the previous example would be
  inappropriate, as it would allow one to write
\[ \holtxt{if}\;P\;\holtxt{then}\;Q\;\holtxt{else} \]
without the trailing argument.}
The rule for this is thus:
\begin{verbatim}
  val _ = add_rule
           {term_name = "SUB", fixity = Closefix,
            pp_elements = [TOK "[", TM, TOK "/", TM, TOK "]"],
            paren_style = OnlyIfNecessary,
            block_style = (AroundEachPhrase, (PP.INCONSISTENT, 2))};
\end{verbatim}

\end{description}

\subsubsection{Hiding constants}
\label{hidden}

\index{parsing, of HOL logic@parsing, of \HOL{} logic!hiding constant status in|(}
\index{HOL system@\HOL{} system!hiding constants in|(}
\index{constants, in HOL logic@constants, in \HOL{} logic!hiding status of}
\index{parsing, of HOL logic@parsing, of \HOL{} logic!overloading}
%
The following function can be used to hide the constant status of a
name from the quotation parser.

\begin{holboxed}
\index{hide@\ml{hide}|pin}
\begin{verbatim}
  val hide   : string -> ({Name : string, Thy : string} list *
                          {Name : string, Thy : string} list)
\end{verbatim}
\end{holboxed}

\noindent Evaluating \ml{hide "$x$"}
makes the quotation parser treat $x$ as a variable (lexical
rules permitting), even if $x$ is the name of a constant in the current theory
(constants and variables can have the same name).
This is useful if one wants to use variables
%
\index{variables, in HOL logic@variables, in \HOL{} logic!with constant names}
%
with the same names as previously declared (or built-in) constants
(\eg\ \ml{o}, \ml{I}, \ml{S}, \etc).  The name $x$ is still a constant
for the constructors, theories, \etc; \ml{hide} affects parsing and
printing by removing the given name from the ``overload map'' described
above in Section~\ref{sec:parser:architecture}.  Note that the effect
of \ml{hide} is \emph{temporary}; its effects do not persist in
theories descended from the current one. See the \REFERENCE{} entry
for \ml{hide} for more details, including an explanation of the return
type.

The function

\begin{holboxed}
\index{reveal@\ml{reveal}|pin}
\begin{verbatim}
   reveal : string -> unit
\end{verbatim}
\end{holboxed}

\noindent undoes hiding.

The function

\begin{holboxed}
\index{hidden@\ml{hidden}|pin}
\begin{verbatim}
   hidden : string -> bool
\end{verbatim}
\end{holboxed}

\noindent tests whether a string is the name of a hidden constant.
\index{HOL system@\HOL{} system!adjustment of user interface of}
\index{HOL system@\HOL{} system!hiding constants in|)}
\index{parsing, of HOL logic@parsing, of \HOL{} logic!hiding constant status in|)}

\subsubsection{Adjusting the pretty-print depth}
\label{sec:pretty-print-depth}
\index{printing, in HOL logic@printing, in \HOL{} logic!structural depth adjustment in}

The following \ML{} reference can be used to adjust the maximum depth
of printing

\begin{holboxed}
\index{max_print_depth@\ml{max\_print\_depth}|pin}
\begin{verbatim}
   max_print_depth : int ref
\end{verbatim}
\end{holboxed}

\index{default print depth, for HOL logic@default print depth, for \HOL{} logic|(}

\noindent The default print depth is $-1$, which is interpreted as
meaning no maximum.  Subterms nested more deeply than the maximum
print depth are printed as \holtxt{...}. For example:

\setcounter{sessioncount}{0}
\begin{session}
\begin{alltt}
>>__ show_types := false;
>> arithmeticTheory.ADD_CLAUSES;

>> max_print_depth := 3;
>> arithmeticTheory.ADD_CLAUSES;
>>__ max_print_depth := ~1;
\end{alltt}
\end{session}
\index{default print depth, for HOL logic@default print depth, for \HOL{} logic|)}

\subsection{Quotations and antiquotation}
\label{sec:quotation-antiquotation}

\index{quotation, in HOL logic@quotation, in \HOL{} logic!parser for}
\index{parsing, of HOL logic@parsing, of \HOL{} logic!of quotation syntax|(}
Logic-related syntax in the HOL system is typically passed to the
parser in special forms known as \emph{quotations}.
A basic quotation is delimited by single quotation characters (`...', Unicode code-points U+2018 and U+2019), or single back-ticks (i.e., \ml{\`}, ASCII character~96).
When quotation values are printed out by the ML interactive loop, they look rather ugly because of the special filtering that is done to these values before the ML interpreter even sees them:
\setcounter{sessioncount}{0}
\begin{session}
\begin{alltt}
>> val q = ‘f x = 3’;
\end{alltt}
\end{session}
Quotations (the ML environment prints the type as \ml{'a frag list}) are the raw input form expected by the various HOL parsers.
They are also polymorphic (to be explained below).
Thus the function \ml{Parse.Term} takes a (term) quotation and returns a term, and is of type
\[
  \ml{term quotation -> term}
\]

The term and type parsers can also be called implicitly by using
double quotations (with ``\dots'', characters U+201C and U+201D), or doubled back-ticks as delimiters.
For the type parser, the first non-space character after the leading delimiter must also be a colon.
Thus:
\begin{session}
\begin{alltt}
>> val t1 = “\x:num. x + 3”;
>> val t2 = ``p /\ q``;

>> val ty = “:'a -> bool”;
\end{alltt}
\end{session}
The expression bound to ML variable \ml{t1} above is actually expanded
to an application of the function \ml{Parse.Term} to the quotation
argument `\ml{p /\bs{} q}'.
Similarly, the declaration of \ml{ty}'s expression expands into an application of \ml{Parse.Type} to the quotation `\ml{:'a -> bool}'.

The significant advantage of quotations over normal \ML{} strings is that they can include new-line and backslash characters without requiring special quoting.
Newlines occur whenever terms get beyond the trivial in size, while backslashes occur in not just the representation of $\lambda$, but also the syntax for conjunction and disjunction.

If a quotation is to include a back-quote character, then this should be done by using the quotation syntax's own escape character, the caret (\ml{\^}, ASCII character~94).
To get a bare caret, things are slightly more complicated.
If a sequence of carets is followed by white-space (including a newline), then that sequence of carets is passed to the HOL parser unchanged.
Otherwise, one caret can be obtained by writing two in a row.
(This last rule is analogous to the way in \ML{} string syntax treats the back-slash.)
Thus:
\begin{session}
\begin{alltt}
>> “f ^` x”;

>> “f ^ x”;
\end{alltt}
\end{session}

Finally, if a single caret is followed by a ``symbol'' character, then the caret and symbol are passed through to HOL unchanged.
Thus the following example illustrates two different ways of writing the same thing (in the first input, two carets become one):
\begin{session}
\begin{alltt}
>> “f ^^+ x”;

>>+ ``f ^+ x``;
\end{alltt}
\end{session}

The main use of the caret is to introduce \emph{antiquotations} (as
suggested in the last example above).  Within a quotation, expressions
of the form {\small\verb+^(+}$t${\small\verb+)+}
%
\index{ antiquotation, in HOL logic@{\small\verb+^+} (antiquotation, in \HOL{} logic)}
%
(where $t$ is an \ML\ expression of type
%
\index{type checking, in HOL logic@type checking, in \HOL{} logic!antiquotation in}
%
\ml{term} or \ml{type}) are called antiquotations.
%
\index{terms, in HOL logic@terms, in \HOL{} logic!antiquotation}%
\index{antiquotation, in HOL logic terms@antiquotation, in \HOL{} logic terms}%
%
An antiquotation \holtxt{\^{}($t$)} evaluates to the
\ML{} value of $t$. For example, ``{\small\verb+x \/ ^(mk_conj(+``\verb+y:bool+''\verb+, +``\verb+z:bool+''\verb+))+}''
evaluates to the same term as {\small``\verb+x \/ (y /\ z)+''}. The
most common use of antiquotation is when the term $t$ is bound to an \ML\
variable $x$. In this case {\small\verb+^(+}$x${\small\verb+)+} can be
abbreviated by {\small\verb+^+}$x$.

The following session illustrates antiquotation.

\setcounter{sessioncount}{0}
\begin{session}
\begin{alltt}
>>__ remove_ovl_mapping "+" {Name = "int_add", Thy = "integer"};
>> val y = “x+1”;

>> val z = “y = ^y”;

>> “!x:num.?y:num.^z”;
\end{alltt}
\end{session}

\noindent Types may be antiquoted as well:

\begin{session}
\begin{alltt}
>> val pred = “:'a -> bool”;

>> “:^pred -> bool”;
\end{alltt}
\end{session}

\noindent Quotations are polymorphic, and the type variable of a
quotation corresponds to the type of entity that can be antiquoted
into that quotation.  Because the term parser expects only antiquoted
terms, antiquoting a type into a term quotation requires the use of
\holtxt{ty\_antiq}. For example,%
%
\index{ty_antiq@\ml{ty\_antiq}}

\begin{session}
\begin{alltt}
>>+ “!P:^pred. P x ==> Q x”;

>> “!P:^(ty_antiq pred). P x ==> Q x”;
\end{alltt}
\end{session}
%
\index{parsing, of HOL logic@parsing, of \HOL{} logic!of quotation syntax|)}



\subsection{Backwards compatibility of syntax}

This section of the manual documents the (extensive) changes made to
the parsing of \HOL{} terms and types in the Taupo release (one of the
HOL3 releases) and beyond from the point of view of a user who doesn't
want to know how to use the new facilities, but wants to make sure
that their old code continues to work cleanly.

The changes which may cause old terms to fail to parse are:
\begin{itemize}
\newcommand\condexp{\holtxt{$p$ => $q$ | $r$}}
\item The precedence of type annotations has completely changed.  It
  is now a very tight suffix (though with a precedence weaker than
  that associated with function application), instead of a weak one.
  This means that \mbox{\tt (x,y:bool \# bool)} should now be written
  as \mbox{\tt (x,y):bool \# bool}. The previous form will now be
  parsed as a type annotation applying to just the \verb+y+.  This
  change brings the syntax of the logic closer to that of SML and
  should make it generally easier to annotate tuples, as one can now
  write \[ (x\,:\,\tau_1,\;y\,:\,\tau_2,\dots z\,:\,\tau_n)
  \] instead of \[
  (x\,:\,\tau_1, \;(y\,:\,\tau_2, \dots (z\,:\,\tau_n)))
  \] where extra parentheses have had to be added just to allow one to
  write a frequently occurring form of constraint.
\item Most arithmetic operators are now left associative instead of
  right associative.  In particular, $+$, $-$, $*$ and {\tt DIV} are
  all left associative.  Similarly, the analogous operators in other
  numeric theories such as {\tt integer} and {\tt real} are also left
  associative.  This brings the \HOL{} parser in line with standard
  mathematical practice.
\item The binding equality in {\tt let} expressions is treated exactly
  the same way as equalities in other contexts.  In previous versions
  of \HOL, equalities in this context have a different, weak binding
  precedence.
\item The old syntax for conditional expressions has been
  removed. Thus the string \holquote{\condexp} must now be written
  $\holquote{\texttt{if}\;p\;\texttt{then}\;q\;\texttt{else}\;r}$
  instead.
\item Some lexical categories are more strictly policed.  String
  literals (strings inside double quotes) and numerals can't be used
  unless the relevant theories have been loaded.  Nor can these
  literals be used as variables inside binding scopes.
\end{itemize}


\section{A Simple Interactive Proof Manager}\label{sec:goalstack}

The \emph{goal stack} provides a simple interface to tactic-based
interactive proof. When one uses tactics to decompose a proof, many
intermediate states arise; the goalstack takes care of the necessary
bookkeeping. The implementation of goalstacks reported here is a
re-design of Larry Paulson's original conception.

The goalstack library is automatically loaded when \HOL{} starts up.

The abstract types \ml{goalstack} and \ml{proofs} are the
focus of backwards proof operations. The type \ml{proofs} can be
regarded as a list of independent goalstacks. Most operations act on
the head of the list of goalstacks; there are also operations so that
the focus can be changed.

\subsection{Starting a goalstack proof}

\begin{hol}
\begin{verbatim}
   g        : term quotation -> proofs
   set_goal : goal -> proofs
\end{verbatim}
\end{hol}

Recall that the type \ml{goal} is an abbreviation for
\ml{term list * term}. To start on a new goal, one gives
\ml{set\_goal} a goal. This creates a new goalstack and makes it the
focus of further operations.

A shorthand for \ml{set\_goal} is the function \ml{g}: it
invokes the parser automatically, and it doesn't allow the goal to
have any assumptions.

Calling \ml{set\_goal}, or \ml{g}, adds a new proof attempt to the
existing ones, \textit{i.e.}, rather than overwriting the current
proof attempt, the new attempt is stacked on top.

\subsection{Applying a tactic to a goal}

\begin{hol}
\begin{verbatim}
   expandf : tactic -> goalstack
   expand  : tactic -> goalstack
   e       : tactic -> goalstack
\end{verbatim}
\end{hol}

How does one actually do a goalstack proof then? In most cases, the
application of tactics to the current goal is done with the function
\verb+expand+. In the rare case that one wants to apply an
{\it invalid\/} tactic, then \verb+expandf+ is used. (For an
explanation of invalid tactics, see Chapter 24 of Gordon \& Melham.) The
abbreviation \verb+e+ may also be used to expand a tactic.


\subsection{Undo}

\begin{hol}
\begin{verbatim}
   b          : unit -> goalstack
   drop       : unit -> proofs
   dropn      : int  -> proofs
   backup     : unit -> goalstack
   restart    : unit -> goalstack
   set_backup : int  -> unit
\end{verbatim}
\end{hol}

Often (we are tempted to say {\it usually}!) one takes a wrong path
in doing a proof, or makes a mistake when setting a goal. To undo a step
in the goalstack, the function \ml{backup} and its abbreviation
\ml{b} are used. This will restore the goalstack to its previous
state.


To directly back up all the way to the original goal, the function
\ml{restart} may be used. Obviously, it is also important to get
rid of proof attempts that are wrong; for that there is \ml{drop},
which gets rid of the current proof attempt, and \ml{dropn}, which
eliminates the top $n$ proof attempts.


Each proof attempt has its own \emph{undo-list} of previous
states. The undo-list for each attempt is of fixed size (initially
12). If you wish to set this value for the current proof attempt, the
function \ml{set\_backup} can be used. If the size of the backup
list is set to be smaller than it currently is, the undo list will be
immediately truncated. You can not undo a ``proofs-level'' operation, such
as \ml{set\_goal} or \ml{drop}.

\subsection{Viewing the state of the proof manager}

\begin{hol}
\begin{verbatim}
   p            : unit -> goalstack
   status       : unit -> proofs
   top_goal     : unit -> goal
   top_goals    : unit -> goal list
   initial_goal : unit -> goal
   top_thm      : unit -> thm
\end{verbatim}
\end{hol}

To view the state of the proof manager at any time, the functions
\ml{p} and \ml{status} can be used. The former only shows
the top subgoals in the current goalstack, while the second gives a
summary of every proof attempt.

To get the top goal or goals of a proof attempt, use \ml{top\_goal}
and \ml{top\_goals}. To get the original goal of a proof attempt,
use \ml{initial\_goal}.

Once a theorem has been proved, the goalstack that was used to derive it
still exists (including its undo-list): its main job now is to
hold the theorem. This theorem can be retrieved with
\ml{top\_thm}.

\subsection{Switch focus to a different subgoal or proof attempt}

\begin{hol}
\begin{verbatim}
   r             : int -> goalstack
   R             : int -> proofs
   rotate        : int -> goalstack
   rotate_proofs : int -> proofs
\end{verbatim}
\end{hol}

Often we want to switch our attention to a different goal in the current
proof, or a different proof. The functions that do this are
\ml{rotate} and \ml{rotate\_proofs}, respectively. The abbreviations
\ml{r} and \ml{R} are simpler to type in.

\section{High Level Proof---\texttt{bossLib}}
% would use \ml{boss} above but it puts LaTeX into fits
\label{sec:bossLib}
\newcommand\bossLib{\ml{bossLib}}

\index{bossLib@\ml{bossLib}}
The library \bossLib\ marshals some of the most widely used theorem
proving tools in \HOL{} and provides them with a convenient interface
for interaction. The library currently focuses on three things:
definition of datatypes and functions; high-level interactive proof
operations, and composition of automated reasoners. Loading \bossLib\
commits one to working in a context that already supplies the theories
of booleans, pairs, sums, the option type, arithmetic, and lists.


\subsection{Support for high-level proof steps}
\label{sec:high-level-proof-steps}

The following functions use information in the database to ease the
application of \HOL's underlying functionality:

\index{Induct_on (ML induction tactic)@\ml{Induct\_on} (\ML{} induction tactic)}
\index{Cases_on (ML case-split tactic)@\ml{Cases\_on} (\ML{} case-split tactic)}
\begin{verbatim}
   type_rws     : hol_type -> thm list
   Induct       : tactic
   Cases        : tactic
   Cases_on     : term quotation -> tactic
   Induct_on    : term quotation -> tactic
\end{verbatim}

\index{type_rws@\ml{type\_rws}}
\index{TypeBase@\ml{TypeBase}}
%
The function \ml{type\_rws} will search for the given type in the
underlying \ml{TypeBase} database and return useful rewrite rules for
that type. The rewrite rules of the datatype are built from the
injectivity and distinctness theorems, along with the case constant
definition. The simplification tactics \ml{RW\_TAC}, \ml{SRW\_TAC},
and the \simpset{} \ml{(srw\_ss())} automatically include these
theorems.  Other tactics used with other \simpset{}s will need these
theorems to be manually added.

\index{induction theorems, in HOL logic@induction theorems, in \HOL{} logic!for algebraic data types}
%
The \ml{Induct} tactic makes it convenient to invoke induction. When
it is applied to a goal, the leading universal quantifier is examined;
if its type is that of a known datatype, the appropriate structural
induction tactic is extracted and applied.

The \ml{Cases} tactic makes it convenient to invoke case
analysis. The leading universal quantifier in the goal is examined; if
its type is that of a known datatype, the appropriate structural
case analysis theorem is extracted and applied.

The \ml{Cases\_on} tactic takes a quotation, which is
parsed into a term $M$, and then $M$ is searched for in the goal. If $M$
is a variable, then a variable with the same name is searched for. Once
the term to split over is known, its type and the associated facts are
obtained from the underlying database and used to perform the case
split. If some free variables of $M$ are bound in the goal, an attempt
is made to remove (universal) quantifiers so that the case split has
force. Finally, $M$ need not appear in the goal, although it should at
least contain some free variables already appearing in the goal. Note
that the \ml{Cases\_on} tactic is more general than \ml{Cases}, but
it does require an explicit term to be given.

\index{Induct_on (ML induction tactic)@\ml{Induct\_on} (\ML{} induction tactic)}
The \ml{Induct\_on} tactic takes a quotation, which is parsed into a
term $M$, and then $M$ is searched for in the goal. If $M$ is a
variable, then a variable with the same name is searched for. Once the
term to induct on is known, its type and the associated facts are
obtained from the underlying database and used to perform the
induction.  If $M$ is not a variable, a new variable $v$ not already
occurring in the goal is created, and used to build a term $v = M$
which the goal is made conditional on before the induction is
performed. First however, all terms containing free variables from $M$
are moved from the assumptions to the conclusion of the goal, and all
free variables of $M$ are universally quantified. \ml{Induct\_on} is
more general than \ml{Induct}, but it does require an explicit term to
be given.

Three supplementary entry-points have been provided for more exotic
inductions:
\begin{description}
\item [\ml{completeInduct\_on}] performs complete induction on the
  term denoted by the given quotation. Complete induction allows a
  seemingly\footnote{Complete induction and ordinary mathematical
    induction are each derivable from the other.} stronger induction
  hypothesis than ordinary mathematical induction: to wit, when
  inducting on $n$, one is allowed to assume the property holds for
  \emph{all} $m$ smaller than $n$. Formally: $\forall P.\ (\forall x.\
  (\forall y.\ y < x \supset P\, y) \supset P\,x) \supset \forall x.\
  P\,x$. This allows the inductive hypothesis to be used more than
  once, and also allows instantiating the inductive hypothesis to
  other than the predecessor.

\item [\ml{measureInduct\_on}] takes a quotation, and breaks it
  apart to find a term and a measure function with which to induct.
  For example, if one wanted to induct on the length of a list
  \holtxt{L}, the invocation \ml{measureInduct\_on~`LENGTH L`}
  would be appropriate.

\item [\ml{recInduct}] takes a induction theorem generated by
\ml{Define} or \ml{Hol\_defn} and applies it to the current goal.

\end{description}


\subsection{Automated reasoners}
\label{sec:automated-reasoners}

\ml{bossLib} brings together the most powerful reasoners in \HOL{} and
tries to make it easy to compose them in a simple way. We take our basic
reasoners from \ml{mesonLib}, \ml{simpLib}, and \ml{numLib},
but the point of \ml{bossLib} is to provide a layer of abstraction so
the user has to know only a few entry-points.\footnote{In the mid 1980's
Graham Birtwistle advocated such an approach, calling it `Ten Tactic
HOL'.} (These underlying libraries, and others providing similarly
powerful tools are described in detail in sections below.)
\begin{hol}
\begin{verbatim}
   PROVE      : thm list -> term -> thm
   PROVE_TAC  : thm list -> tactic

   METIS_TAC  : thm list -> tactic
   METIS_PROVE: thm list -> term -> thm

   DECIDE     : term quotation -> thm
   DECIDE_TAC : tactic
\end{verbatim}
\end{hol}
The inference rule \texttt{PROVE} (and the corresponding tactic
\texttt{PROVE\_TAC}) takes a list of theorems and a term, and attempts
to prove the term using a first order reasoner.  The two \ml{METIS}
functions perform the same functionality but use a different
underlying proof method.  The \texttt{PROVE} entry-points refer to the
\texttt{meson} library, which is further described in
Section~\ref{sec:mesonLib} below. The \ml{METIS} system is described
in Section~\ref{sec:metisLib}.  The inference rule \texttt{DECIDE}
(and the corresponding tactic \texttt{DECIDE\_TAC}) applies a decision
procedure that (at least) handles statements of linear arithmetic.

\begin{hol}
\begin{verbatim}
   RW_TAC   : simpset -> thm list -> tactic
   SRW_TAC  : ssfrag list -> thm list -> tactic
   &&       : simpset * thm list -> simpset  (* infix *)
   std_ss   : simpset
   arith_ss : simpset
   list_ss  : simpset
   srw_ss   : unit -> simpset
\end{verbatim}
\end{hol}
%
\index{RW_TAC@\ml{RW\_TAC}} The rewriting tactic \ml{RW\_TAC} works by
first adding the given theorems into the given \simpset; then it
simplifies the goal as much as possible; then it performs case splits
on any conditional expressions in the goal; then it repeatedly (1)
eliminates all hypotheses of the form $v = M$ or $M = v$ where $v$ is
a variable not occurring in $M$, (2) breaks down any equations between
constructor terms occurring anywhere in the goal. Finally,
\ml{RW\_TAC} lifts \holtxt{let}-expressions within the goal so that
the binding equations appear as
abbreviations\index{abbreviations!tactic-based proof} in the
assumptions.

\index{SRW_TAC@\ml{SRW\_TAC}} The tactic \ml{SRW\_TAC} is similar to
\ml{RW\_TAC}, but works with respect to an underlying \simpset{}
(accessible through the function \ml{srw\_ss}) that is updated as new
context is loaded.  This \simpset{} can be augmented through the
addition of ``\simpset{} fragments'' (\ml{ssfrag} values) and
theorems.  In situations where there are many large types stored in
the system, \ml{RW\_TAC}'s performance can suffer because it
repeatedly adds all of the rewrite theorems for the known types into a
\simpset{} before attacking the goal.  On the other hand,
\ml{SRW\_TAC} loads rewrites into the \simpset{} underneath
\ml{srw\_ss()} just once, making for faster operation in this
situation.

\ml{bossLib} provides a number of simplification sets. The
simpset for pure logic, sums, pairs, and the \ml{option} type is
named \ml{std\_ss}. The simpset for arithmetic is named
\ml{arith\_ss}, and the simpset for lists is named \ml{list\_ss}.
The simpsets provided by \bossLib{} strictly increase in strength:
\ml{std\_ss} is contained in \ml{arith\_ss}, and \ml{arith\_ss} is
contained in \ml{list\_ss}.  The infix combinator \ml{\&\&} is used
to build a new \simpset{} from a given \simpset{} and a list of
theorems. \HOL's simplification technology is described further in
Section~\ref{sec:simpLib} below and in the \REFERENCE.

\begin{hol}
\begin{verbatim}
   by : term quotation * tactic -> tactic (* infix 8 *)
   SPOSE_NOT_THEN : (thm -> tactic) -> tactic
\end{verbatim}
\end{hol}
The function \ml{by} is an infix operator that takes a quotation
and a tactic $tac$. The quotation is parsed into a term $M$. When the
invocation ``\ml{$M$ by $\mathit{tac}$}'' is applied to a goal
$(A,g)$, a new subgoal $(A,M)$ is created and $tac$ is applied to it.
If the goal is proved, the resulting theorem is broken down and added
to the assumptions of the original goal; thus the proof proceeds with
the goal $((M::A), g)$. (Note however, that case-splitting will happen
if the breaking-down of $\ \vdash M$ exposes disjunctions.) Thus
\ml{by} allows a useful style of `assertional' or `Mizar-like'
reasoning to be mixed with ordinary tactic proof.\footnote{Proofs in
  the Mizar system are readable documents, unlike most
  tactic-based proofs.}

The \ml{SPOSE\_NOT\_THEN} entry-point initiates a proof by
contradiction by assuming the negation of the goal and driving the
negation inwards through quantifiers. It provides the resulting
theorem as an argument to the supplied function, which will use the
theorem to build and apply a tactic.

\section{First Order Proof---\texttt{mesonLib} and \texttt{metisLib}}
\label{sec:first-order-proof}
\index{decision procedures!first-order logic}

First order proof is a powerful theorem-proving technique that can
finish off complicated goals.  Unlike tools such as the simplifier, it
either proves a goal outright, or fails.  It can not transform a goal
into a different (and more helpful) form.

\subsection{Model elimination---\texttt{mesonLib}}
\label{sec:mesonLib}

\index{meson (model elimination) procedure@\ml{meson} (model elimination) procedure}
\index{model elimination method for first-order logic}

The \ml{meson} library is an implementation of the
model-elimination method for finding proofs of goals in first-order
logic.  There are three main entry-points:
\begin{hol}
\begin{verbatim}
   MESON_TAC     : thm list -> tactic
   ASM_MESON_TAC : thm list -> tactic
   GEN_MESON_TAC : int -> int -> int -> thm list -> tactic
\end{verbatim}
\end{hol}

Each of these tactics attempts to prove the goal.  They will either
succeed in doing so, or fail with a ``depth exceeded'' exception.  If
the branching factor in the search-space is high, the \texttt{meson}
tactics may also take a very long time to reach the maximum depth.

All of the \texttt{meson} tactics take a list of theorems.  These
extra facts are used by the decision procedure to help prove the goal.
\texttt{MESON\_TAC} ignores the goal's assumptions; the other two
entry-points include the assumptions as part of the sequent to be
proved.

The extra parameters to \ml{GEN\_MESON\_TAC} provide extra control of
the behaviour of the iterative deepening that is at the heart of the
search for a proof.  In any given iteration, the algorithm searches
for a proof of depth no more than a parameter $d$.  The default
behaviour for \ml{MESON\_TAC} and \ml{ASM\_MESON\_TAC} is to start $d$
at 0, to increment it by one each time a search fails, and to fail if
$d$ exceeds the value stored in the reference value
\ml{mesonLib.max\_depth}.  By way of contrast,
\ml{GEN\_MESON\_TAC~min~max~step} starts $d$ at \ml{min}, increments
it by \ml{step}, and gives up when $d$ exceeds \ml{max}.

The \ml{PROVE\_TAC} function from \ml{bossLib} performs some
normalisation, before passing a goal and its assumptions to
\ml{ASM\_MESON\_TAC}.  Because of this normalisation, in most
circumstances, \ml{PROVE\_TAC} should be preferred to
\ml{ASM\_MESON\_TAC}.

\subsection{Resolution---\texttt{metisLib}}
\label{sec:metisLib}

\index{metis (resolution) procedure@\ml{metis} (resolution) procedure}
\index{resolution method for first-order logic}

The \ml{metis} library is an implementation of the resolution method
for finding proofs of goals in first-order logic. There are two main
entry-points:

\begin{hol}
\begin{verbatim}
   METIS_TAC   : thm list -> tactic
   METIS_PROVE : thm list -> term -> thm
\end{verbatim}
\end{hol}

Both functions take a list of theorems, and these are used as lemmas
in the proof. \texttt{METIS\_TAC} is a tactic, and will either succeed
in proving the goal, or if unsuccessful will either fail or loop
forever. \texttt{METIS\_PROVE} takes a term $t$ and tries to prove a
theorem with conclusion $t$: if successful, the theorem $\vdash t$ is
returned. As for \texttt{METIS\_TAC}, it might fail or loop forever if
the proof search is unsuccessful.

The \texttt{metisLib} family of proof tools implement the ordered
resolution and ordered paramodulation calculus for first order logic,
which usually makes them better suited to goals requiring non-trivial
equality reasoning than the tactics in \texttt{mesonLib}.


\section{Simplification---\texttt{simpLib}}
\label{sec:simpLib}
\index{simplification|(}

The simplifier is \HOL's most sophisticated rewriting engine.  It is
recommended as a general purpose work-horse during interactive
theorem-proving.  As a rewriting tool, the simplifier's general role
is to apply theorems of the general form
\[
\vdash l = r
\]
to terms, replacing instances of $l$ in the term with $r$. Thus, the
basic simplification routine is a \emph{conversion}, taking a term
$t$, and returning a theorem $\vdash t = t'$, or the exception
\ml{UNCHANGED}.

The basic conversion is
\begin{hol}
\begin{verbatim}
   simpLib.SIMP_CONV : simpLib.simpset -> thm list -> term -> thm
\end{verbatim}
\end{hol}
The first argument, a \simpset, is the standard way of providing a
collection of rewrite rules (and other data, to be explained below) to
the simplifier.  There are \simpset{}s accompanying most of \HOL's
major theories.  For example, the \simpset{} \ml{bool\_ss}
in \ml{boolSimps} embodies all of the usual rewrite theorems one would want over boolean
formulas:
\setcounter{sessioncount}{0}
\begin{session}
\begin{alltt}
>>__ show_types := false;
>> SIMP_CONV bool_ss [] ``p /\ T \/ ~(q /\ r)``;
\end{alltt}
\end{session}
In addition to rewriting with the obvious theorems, \ml{bool\_ss} is
also capable of performing simplifications that are not expressible as
simple theorems:
\begin{session}
\begin{alltt}
>> SIMP_CONV bool_ss [] ``?x. (\y. P (f y)) x /\ (x = z)``;
\end{alltt}
\end{session}
In this example, the simplifier performed a $\beta$-reduction in the
first conjunct under the existential quantifier, and then did an
``unwinding'' or ``one-point'' reduction, recognising that the only
possible value for the quantified variable \holtxt{x} was the value
\holtxt{z}.

The second argument to \ml{SIMP\_CONV} is a list of theorems to be
added to the provided \simpset, and used as additional rewrite rules.
In this way, users can temporarily augment standard \simpset{}s with
their own rewrites.  If a particular set of theorems is often used as
such an argument, then it is possible to build a \simpset{} value to
embody these new rewrites.

For example, the rewrite \ml{arithmeticTheory.LEFT\_ADD\_DISTRIB}, which
states that $p(m + n) = pm + pn$ is not part of any of \HOL's standard
\simpset{}s.  This is because it can cause an unappealing increase in
term size (there are two occurrences of $p$ on the right hand
side of the theorem).  Nonetheless, it is clear that this theorem may
be appropriate on occasion:
\begin{session}
\begin{alltt}
>>_ open arithmeticTheory;
>> SIMP_CONV bossLib.arith_ss [LEFT_ADD_DISTRIB] ``p * (n + 1)``;
\end{alltt}
\end{session}
Note how the \ml{arith\_ss} \simpset{} has not only simplified the
intermediate \ml{(p * 1)} term, but also re-ordered the addition to
put the simpler term on the left, and sorted the multiplication's
arguments.


\subsection{Simplification tactics}
\label{sec:simplification-tactics}
\index{simplification!tactics}

The simplifier is implemented around the conversion \ml{SIMP\_CONV},
which is a function for `converting' terms into theorems.  To apply
the simplifier to goals (alternatively, to perform tactic-based proofs
with the simplifier), \HOL{} provides five tactics, all of which are
available in \ml{bossLib}.

\subsubsection{\ml{SIMP\_TAC : simpset -> thm list -> tactic}}
\index{SIMP_TAC@\ml{SIMP\_TAC}}

\ml{SIMP\_TAC} is the simplest simplification tactic: it attempts to
simplify the current goal (ignoring the assumptions) using the given
\simpset{} and the additional theorems.  It is no more than the
lifting of the underlying \ml{SIMP\_CONV} conversion to the tactic
level through the use of the standard function \ml{CONV\_TAC}.

\subsubsection{\ml{ASM\_SIMP\_TAC : simpset -> thm list -> tactic}}
\index{ASM_SIMP_TAC@\ml{ASM\_SIMP\_TAC}}

Like \ml{SIMP\_TAC}, \ml{ASM\_SIMP\_TAC} simplifies the current goal
(leaving the assumptions untouched), but includes the goal's
assumptions as extra rewrite rules.  Thus:
\begin{session}
\begin{alltt}
>>__ g `(x = 3) ==> P x`;
>>- e strip_tac;

>> e (ASM_SIMP_TAC bool_ss []);
\end{alltt}
\end{session}
\noindent
In this example, \ml{ASM\_SIMP\_TAC} used \holtxt{x = 3} as an
additional rewrite rule, and replaced the \holtxt{x} of \holtxt{P x}
with \holtxt{3}.  When an assumption is used by \ml{ASM\_SIMP\_TAC} it
is converted into rewrite rules in the same way as theorems passed in
the list given as the tactic's second argument.  For example, an
assumption \holtxt{\~{}P} will be treated as the rewrite \holtxt{|- P = F}.

\subsubsection{\ml{FULL\_SIMP\_TAC : simpset -> thm list -> tactic}}
\index{FULL_SIMP_TAC@\ml{FULL\_SIMP\_TAC}}

\noindent
The tactic \ml{FULL\_SIMP\_TAC} simplifies not only a goal's
conclusion but its assumptions as well.  It proceeds by simplifying
each assumption in turn, additionally using earlier assumptions in the
simplification of later assumptions.  After being simplified, each
assumption is added back into the goal's assumption list with the
\ml{STRIP\_ASSUME\_TAC} tactic.  This means that assumptions that
become conjunctions will have each conjunct assumed separately.
Assumptions that become disjunctions will cause one new sub-goal to be
created for each disjunct.  If an assumption is simplified to false,
this will solve the goal.

\ml{FULL\_SIMP\_TAC} attacks the assumptions in the order in which
they appear in the list of terms that represent the goal's
assumptions.  Typically then, the first assumption to be simplified
will be the assumption most recently added.  Viewed in the light of
\ml{goalstackLib}'s printing of goals, \ml{FULL\_SIMP\_TAC} works its
way up the list of assumptions, from bottom to top.

The following demonstrates a simple use of \ml{FULL\_SIMP\_TAC}:
\begin{session}
\begin{alltt}
>>__ drop();
>>__ g `f (x:num) < 10  /\ (x = 4) ==> 4 + x < 10`;
>>- e strip_tac;

>> e (FULL_SIMP_TAC bool_ss []);
\end{alltt}
\end{session}
In this example, the assumption \holtxt{x = 4} caused the \holtxt{x}
in the assumption \holtxt{f x < 10} to be replaced by \holtxt{4}.  The
\holtxt{x} in the goal was similarly replaced.  If the assumptions had
appeared in the opposite order, only the \holtxt{x} of the goal would
have changed.

The next session demonstrates more interesting behaviour:
\begin{session}
\begin{alltt}
>>__ drop(); g`x <= 4n ==> f x + 1 < 10`;
>>- e strip_tac;

>> e (FULL_SIMP_TAC bool_ss [arithmeticTheory.LESS_OR_EQ]);
\end{alltt}
\end{session}
In this example, the goal was rewritten with the theorem stating
\[
\vdash x \leq y \iff x < y \lor x = y
\]
Turning the assumption into a disjunction resulted in two sub-goals.
In the second of these, the assumption \holtxt{x = 4} further
simplified the rest of the goal.

\subsubsection{\ml{RW\_TAC : simpset -> thm list -> tactic}}
\index{RW_TAC@\ml{RW\_TAC}}

Though its type is the same as the simplification tactics already
described, \ml{RW\_TAC} is an ``augmented'' tactic.  It is augmented
in two ways:
\begin{itemize}
\item When simplifying the goal, the provided \simpset{} is augmented
  not only with the theorems explicitly passed in the second argument,
  but also with all of the rewrite rules from the \ml{TypeBase}, and
  also with the goal's assumptions.
%
  \index{TypeBase@\ml{TypeBase}}
\item \ml{RW\_TAC} also does more than just perform simplification.
  It also repeatedly ``strips'' the goal.  For example, it moves the
  antecedents of implications into the assumptions, splits
  conjunctions, and case-splits on conditional expressions.  This
  behaviour can rapidly remove a lot of syntactic complexity from
  goals, revealing the kernel of the problem.  On the other hand, this
  aggressive splitting can also result in a large number of
  sub-goals.  \ml{RW\_TAC}'s augmented behaviours are intertwined with
  phases of simplification in a way that is difficult to describe.
\end{itemize}

\subsubsection{\ml{SRW\_TAC : ssfrag list -> thm list -> tactic}}
\index{SRW_TAC@\ml{SRW\_TAC}}

The tactic \ml{SRW\_TAC} has a different type from the other
simplification tactics.  It does not take a \simpset{} as an argument.
Instead its operation always builds on the built-in \simpset{}
\ml{srw\_ss()} (further described in Section~\ref{sec:srw-ss}).  The
theorems provided as \ml{SRW\_TAC}'s second argument are treated in
the same way as by the other simplification tactics.  Finally, the
list of \simpset{} fragments are merged into the underlying
\simpset{}, allowing the user to merge in additional simplification
capabilities if desired.

For example, to include the Presburger decision procedure, one could
write
\begin{hol}
\begin{verbatim}
   SRW_TAC [ARITH_ss][]
\end{verbatim}
\end{hol}
\Simpset{} fragments are described below in
Section~\ref{sec:simpset-fragments}.

The \ml{SRW\_TAC} tactic performs the same mixture of simplification and
goal-splitting as does \ml{RW\_TAC}.  The main differences between the
two tactics lie in the fact that the latter can be inefficient when
working with a large \ml{TypeBase}, and in the fact that working with
\ml{SRW\_TAC} saves one from having to explicitly construct
\simpset{}s that include all of the current context's ``appropriate''
rewrites.  The latter ``advantage'' is based on the assumption that
\ml{(srw\_ss())} never includes inappropriate rewrites.  The presence
of unused rewrites is never a concern: the presence of rewrites that
do the wrong thing can be a major irritation.

\subsubsection{Abbreviated ``Power'' tactics}
\HOL{}'s \ml{bossLib} module (part of the standard interactive environment) comes with a number of powerful simplification tactics with short lower-case names.
These do not require or allow for the specification of a particular \simpset{} because they use \ml{srw_ss()} combined with the arithmetic decision procedure for the natural numbers, as well as a \simpset{} fragment that eliminates \holtxt{let}-terms.
All of the tactics take a list of rewrite theorems as their one argument.
These tactics are:

\begin{center}
\index{simp (simplification tactic)@\ml{simp} (simplification tactic)}%
\index{rw (simplification tactic)@\ml{rw} (simplification tactic)}%
\index{fs (simplification tactic)@ \ml{fs} (simplification tactic)}%
\index{rfs (simplification tactic)@\ml{rfs} (simplification tactic)}%
\begin{tabular}{ll}
\ml{simp} & Corresponds to \ml{ASM\_SIMP\_TAC}\\
\ml{rw} & Corresponds to \ml{SRW\_TAC[]}\\
\ml{fs} & Corresponds to \ml{FULL\_SIMP\_TAC}\\
\ml{rfs} & Corresponds to \ml{REV\_FULL\_SIMP\_TAC}
\end{tabular}
\end{center}
Thus, the following is possible:
\begin{session}
\begin{alltt}
>>_ g `x < 3 /\ P x ==> x < 20 DIV 2`;
>> e (simp[]);
\end{alltt}
\end{session}
In addition, two similar tactics allow special-purpose simplification of propositional structure:
\begin{itemize}
\item%
\index{csimp@\ml{csimp}}%
\ml{csimp} simplifies with \ml{srw\_ss()} and the \ml{CONJ\_ss} \simpset{} fragment.
The latter allows for conjuncts to be used as assumptions when rewriting other conjuncts.
Thus, if simplifying $c_1\land c_2$, $c_2$ will be assumed as a possible source of rewrites while $c_1$ is simplified to $c_1'$.
Then $c_1'$ will be assumed while $c_2$ is simplified.
\item \index{dsimp@\ml{dsimp}}\ml{dsimp} simplifies with \ml{srw\_ss()} and the \ml{DNF\_ss} \simpset{} fragment.
The latter normalises to disjunctive normal form and also moves quantifiers so as to maximise the chance that they can be eliminated over equalities.
When this tactic works well, it can provide a ``pure'' simplification analogue of the repeated stripping and variable elimination done by \ml{rw}.
\end{itemize}

\subsection{The standard \simpset{}s}
\label{sec:standard-simpsets}

\HOL{} comes with a number of standard \simpset{}s.  All of these are
accessible from within \ml{bossLib}, though some originate in other
structures.

\subsubsection{\ml{pure\_ss} and \ml{bool\_ss}}
\label{sec:purebool-ss}
%
\index{pure_ss@\ml{pure\_ss}}
%
The \ml{pure\_ss} \simpset{} (defined in structure \ml{pureSimps})
contains no rewrite theorems at all, and plays the role of a blank
slate within the space of possible \simpset{}s.  When constructing a
completely new \simpset, \ml{pure\_ss} is a possible starting point.
The \ml{pure\_ss} \simpset{} has just two components: congruence rules
for specifying how to traverse terms, and a function that turns
theorems into rewrite rules.  Congruence rules are further described
in Section~\ref{sec:advanced-simplifier}; the generation of rewrite
rules from theorems is described in
Section~\ref{sec:simplifier-rewriting}.

\index{bool_ss (simplification set)@\ml{bool\_ss} (simplification set)}
%
The \ml{bool\_ss} \simpset{} (defined in structure \ml{boolSimps}) is
often used when other \simpset{}s might do too much.  It contains
rewrite rules for the boolean connectives, and little more.  It
contains all of the de~Morgan theorems for moving negations in over
the connectives (conjunction, disjunction, implication and conditional
expressions), including the quantifier rules that have $\neg(\forall
x.\,P(x))$ and $\neg(\exists x.\,P (x))$ on their left-hand sides.  It
also contains the rules specifying the behaviour of the connectives
when the constants \holtxt{T} and \holtxt{F} appear as their
arguments.  (One such rule is \holtxt{|- T /\bs{} p = p}.)

As in the example above, \ml{bool\_ss} also performs
$\beta$-reductions and one-point unwindings.  The latter turns terms
of the form \[
\exists x.\;P(x)\land\dots (x = e) \dots\land Q(x)
\]
into
\[
P(e) \land \dots \land Q(e)
\]
Similarly, unwinding will turn $\forall x.\;(x = e)
\Rightarrow P(x)$ into $P(e)$.

Finally, \ml{bool\_ss} also includes congruence rules that allow
the simplifier to make additional assumptions when simplifying
implications and conditional expressions.  This feature is further
explained in Section~\ref{sec:simplifier-rewriting} below, but can be
illustrated by some examples (the first also demonstrates unwinding
under a universal quantifier):
\begin{session}
\begin{alltt}
>>__ remove_ovl_mapping GrammarSpecials.fromNum_str
       {Name = "int_of_num", Thy = "integer"};
>> SIMP_CONV bool_ss [] ``!x. (x = 3) /\ P x ==> Q x /\ P 3``;

>> SIMP_CONV bool_ss [] ``if x <> 3 then P x else Q x``;
\end{alltt}
\end{session}

\subsubsection{\ml{std\_ss}}
%
\index{std_ss (simplification set)@\ml{std\_ss} (simplification set)}
%
The \ml{std\_ss} \simpset{} is defined in \ml{bossLib}, and adds
rewrite rules pertinent to the types of sums, pairs, options and
natural numbers to \ml{bool\_ss}.
\begin{session}
\begin{alltt}
>> SIMP_CONV std_ss [] ``FST (x,y) + OUTR (INR z)``;

>> SIMP_CONV std_ss [] ``case SOME x of NONE => P | SOME y => f y``;
\end{alltt}
\end{session}

With the natural numbers, the \ml{std\_ss} \simpset{} can calculate
with ground values, and also includes a suite of ``obvious rewrites''
for formulas including variables.
\begin{session}
\begin{alltt}
>> SIMP_CONV std_ss [] ``P (0 <= x) /\ Q (y + x - y)``;

>> SIMP_CONV std_ss [] ``23 * 6 + 7 ** 2 - 31 DIV 3``;
\end{alltt}
\end{session}

\subsubsection{\ml{arith\_ss}}
%
\index{arith_ss (simplification set)@\ml{arith\_ss} (simplification set)}
%
The \ml{arith\_ss} \simpset{} (defined in \ml{bossLib}) extends
\ml{std\_ss} by adding the ability to decide formulas of Presburger
arithmetic, and to normalise arithmetic expressions (collecting
coefficients, and re-ordering summands).  The underlying natural
number decision procedure is that described in
Section~\ref{sec:numLib} below.

These two facets of the \ml{arith\_ss} \simpset{} are demonstrated
here:
\begin{session}
\begin{alltt}
>> SIMP_CONV arith_ss [] ``x < 3 /\ P x ==> x < 20 DIV 2``;

>> SIMP_CONV arith_ss [] ``2 * x + y - x + y``;
\end{alltt}
\end{session}
Note that subtraction over the natural numbers works in ways that can
seem unintuitive.  In particular, coefficient normalisation may not
occur when first expected:
\begin{session}
\begin{alltt}
>>+ SIMP_CONV arith_ss [] ``2 * x + y - z + y``;
\end{alltt}
\end{session}
Over the natural numbers, the expression $2 x + y - z + y$ is not
equal to $2 x + 2 y - z$.  In particular, these expressions are not
equal when $2x + y < z$.

\subsubsection{\ml{list\_ss}}
%
\index{list_ss (simplification set)@\ml{list\_ss} (simplification set)}
%
The last pure \simpset{} value in \ml{bossLib}, \ml{list\_ss} adds
rewrite theorems about the type of lists to \ml{arith\_ss}.  These
rewrites include the obvious facts about the list type's constructors
\holtxt{NIL} and \holtxt{CONS}, such as the fact that \holtxt{CONS} is
injective:
\begin{hol}
\begin{verbatim}
   (h1 :: t1 = h2 :: t2) = (h1 = h2) /\ (t1 = t2)
\end{verbatim}
\end{hol}
Conveniently, \ml{list\_ss} also includes rewrites for the functions
defined by primitive recursion over lists.  Examples include
\holtxt{MAP}, \holtxt{FILTER} and \holtxt{LENGTH}.  Thus:
\begin{session}
\begin{alltt}
>> SIMP_CONV list_ss [] ``MAP (\x. x + 1) [1;2;3;4]``;

>> SIMP_CONV list_ss [] ``FILTER (\x. x < 4) [1;2;y + 4]``;

>> SIMP_CONV list_ss [] ``LENGTH (FILTER ODD [1;2;3;4;5])``;
\end{alltt}
\end{session}
These examples demonstrate how the simplifier can be used as a general
purpose symbolic evaluator for terms that look a great deal like those
that appear in a functional programming language.  Note that
this functionality is also provided by \ml{computeLib} (see
Section~\ref{sec:computeLib} below); \ml{computeLib} is more
efficient, but less general than the simplifier.  For example:
\begin{session}
\begin{alltt}
>> EVAL ``FILTER (\x. x < 4) [1;2;y + 4]``;
\end{alltt}
\end{session}

\subsubsection{The ``stateful'' \simpset---\ml{srw\_ss()}}
\label{sec:srw-ss}
\index{srw_ss (simplification set)@\ml{srw\_ss} (simplification set)}

The last \simpset{} exported by \ml{bossLib} is hidden behind a
function.  The \ml{srw\_ss} value has type \ml{unit -> simpset}, so
that one must type \ml{srw\_ss()} in order to get a \simpset{} value.
This use of a function type allows the underlying \simpset{} to be
stored in an \ML{} reference, and allows it to be updated
dynamically.  In this way, referential transparency is deliberately
broken.  All of the other \simpset{}s will always behave identically:
\ml{SIMP\_CONV~bool\_ss} is the same simplification routine wherever
and whenever it is called.

In contrast, \ml{srw\_ss} is designed to be updated.  When a theory is
loaded, when a new type is defined, the value behind \ml{srw\_ss()}
changes, and the behaviour of \ml{SIMP\_CONV} applied to
\ml{(srw\_ss())} changes with it.  The design philosophy behind
\ml{srw\_ss} is that it should always be a reasonable first choice in
all situations where the simplifier is used.

This versatility is illustrated in the following example:
\begin{session}
\begin{alltt}
>> Datatype `tree = Leaf | Node num tree tree`;

>> SIMP_CONV (srw_ss()) [] ``Node x Leaf Leaf = Node 3 t1 t2``;

>> load "pred_setTheory";

>> SIMP_CONV (srw_ss()) [] ``x IN { y | y < 6}``;
\end{alltt}
\end{session}
%
Users can augment the stateful \simpset{} themselves with the function
%
\begin{holboxed}
\index{export_rewrites@\ml{export\_rewrites}}
\begin{verbatim}
   BasicProvers.export_rewrites : string list -> unit
\end{verbatim}
\end{holboxed}
The strings passed to \ml{export\_rewrites} are the names of theorems
in the current segment (those that will be exported when
\ml{export\_theory} is called).  Not only are these theorems added to
the underlying \simpset{} in the current session, but they will be
added in future sessions when the current theory is reloaded.
\begin{session}
\begin{alltt}
>> val tsize_def = Define`
     (tsize Leaf = 0) /\
     (tsize (Node n t1 t2) = n + tsize t1 + tsize t2)
   `;

>> val _ = BasicProvers.export_rewrites ["tsize_def"];

>> SIMP_CONV (srw_ss()) [] ``tsize (Node 4 (Node 6 Leaf Leaf) Leaf)``;
\end{alltt}
\end{session}

\index{theorem attributes!simp@\ml{simp}}
Alternatively, the user may also flag theorems directly when using \ml{store\_thm}, \ml{save\_thm}, or the \ml{Theorem} and \ml{Definition} syntaxes by appending the \ml{simp} attribute to the name of the theorem.
Thus:
\begin{session}
\begin{verbatim}
Theorem useful_rwt[simp]:
  ...term...
Proof ...tactic...
QED
\end{verbatim}
\end{session}
is a way of avoiding having to write a call to \ml{export\_rewrites}.
Equally, the example above could be written:
\begin{session}
\begin{alltt}
>>_ Definition tsize_def[simp]:
      (tsize Leaf = 0) /\
      (tsize (Node n t1 t2) = n + tsize t1 + tsize t2)
    End
\end{alltt}
\end{session}

As a general rule, \ml{(srw\_ss())} includes all of its context's
``obvious rewrites'', as well as code to do standard calculations
(such as the arithmetic performed in the above example).  It does not
include decision procedures that may exhibit occasional poor
performance, so the \simpset{} fragments containing these procedures
should be added manually to those simplification invocations that need
them.

\subsection{\Simpset{} fragments}
\label{sec:simpset-fragments}
\index{simplification!simpset fragments}

The \simpset{} fragment is the basic building block that is used to
construct \simpset{} values.  There is one basic function that
performs this construction:
\begin{hol}
\begin{verbatim}
   op ++  : simpset * ssfrag -> simpset
\end{verbatim}
\end{hol}
where \ml{++} is an infix.  In general, it is best to build on top of
the \ml{pure\_ss} \simpset{} or one of its descendants in order to
pick up the default ``filter'' function for converting theorems to
rewrite rules.  (This filtering process is described below in
Section~\ref{sec:generating-rewrite-rules}.)

For major theories (or groups thereof), a collection of relevant
\simpset{} fragments is usually found in the module \ml{<thy>Simps},
with \ml{<thy>} the name of the theory.  For example, \simpset{}
fragments for the theory of natural numbers are found in
\ml{numSimps}, and fragments for lists are found in \ml{listSimps}.

Some of the distribution's standard \simpset{} fragments are described
in Table~\ref{table:ssfrags}.  These and other \simpset{} fragments
are described in more detail in the \REFERENCE.

\begin{table}[htbp]
\begin{center}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{lp{0.65\textwidth}}
\ml{BOOL\_ss} &
Standard rewrites for the boolean operators
(conjunction, negation \&c), as well as a conversion for performing
$\beta$-reduction.  (In \ml{boolSimps}.)
\\
\ml{CONG\_ss} & Congruence rules for implication and conditional
expressions. (In \ml{boolSimps}.)
\\
\ml{ARITH\_ss} &
The natural number decision
procedure for universal Presburger arithmetic. (In \ml{numSimps}.)
\\
\ml{PRED\_SET\_AC\_ss} & AC-normalisation for unions and intersections
over sets. (In \ml{pred\_setSimps}.)
\end{tabular}
\end{center}
\caption{Some of \HOL's standard \simpset{} fragments}
\label{table:ssfrags}
\end{table}

\Simpset{} fragments are ultimately constructed with the \ml{SSFRAG}
constructor:
\begin{hol}
\begin{verbatim}
   SSFRAG : {
     convs  : convdata list,
     rewrs  : thm list,
     ac     : (thm * thm) list,
     filter : (controlled_thm -> controlled_thm list) option,
     dprocs : Traverse.reducer list,
     congs  : thm list,
     name   : string option
   } -> ssfrag
\end{verbatim}
\end{hol}
A complete description of the various fields of the record passed to
\ml{SSFRAG}, and their meaning is given in \REFERENCE.  The
\ml{rewrites} function provides an easy route to constructing a
fragment that just includes a list of rewrites:
\begin{hol}
\begin{verbatim}
   rewrites : thm list -> ssfrag
\end{verbatim}
\end{hol}

\subsubsection{Removing rewrites and conversions from \simpset{}s}
\index{simplification!removing rewrites|(}
The \ml{-*} (infix) function can be used to remove elements from \simpset{}s.
This can be done to temporarily affect the simplifier when it is applied to a particular goal.
For example:
\begin{session}
\begin{alltt}
>> SIMP_CONV (srw_ss()) [] “x ++ (y ++ z)”;

>>+ SIMP_CONV (srw_ss() -* ["APPEND_ASSOC"]) [] “x ++ (y ++ z)”;
\end{alltt}
\end{session}
The second argument to \ml{-*} is a list of strings, naming rewrite theorems or conversions.
The names to use are visible if \simpset{} values are printed out in the interactive session.
The example below demonstrates removing the beta-conversion:
\begin{session}
\begin{alltt}
##assert Exn.capture (SIMP_CONV (bool_ss -* ["BETA_CONV"]) []) “(\x. x + 3) 10” |> Exn.get_exn |> isSome
>>+ SIMP_CONV (bool_ss -* ["BETA_CONV"]) [] “(\x. x + 3) 10”;
\end{alltt}
\end{session}
Further, because a theorem like \ml{AND\_CLAUSES}
\begin{alltt}
>>__ val oldlw = !linewidth; linewidth := 60;
##thm AND_CLAUSES
>>__ linewidth := oldlw;
\end{alltt}
has multiple conjuncts, one theorem can generate multiple different rewrites.
Specific sub-rewrites can be removed from a \simpset{} without affecting  others derived from the same original theorem by appending numbers to the theorem name:
\begin{session}
\begin{alltt}
>>+ SIMP_CONV (bool_ss -* ["AND_CLAUSES"]) [] “(T ∧ p) ∧ (q ∧ T)”;

##assert SIMP_CONV (bool_ss -* ["AND_CLAUSES.1"]) [] “(T ∧ p) ∧ (q ∧ T)” |> concl |> rhs |> aconv “(T ∧ p) ∧ q”
>> SIMP_CONV (bool_ss -* ["AND_CLAUSES.1"]) [] “(T ∧ p) ∧ (q ∧ T)”;
\end{alltt}
\end{session}

If using a ``power tactic'' such as \ml{simp}, there is no \simpset{} value visible to modify with \ml{-*}.
Instead, one must use a special theorem form (see Section~\ref{sec:simp-special-rewrite-forms} below), \ml{Excl} to exclude a rewrite.
For example, sometimes the associativity of list-append can be annoying (here it masks the rewrite defining list-append):
\begin{session}
\begin{alltt}
>>__ proofManagerLib.dropn (case proofManagerLib.status() of
           Manager.PRFS l => List.length l)
>>_ g `f (x ++ (h::t ++ y)) = f (x ++ h::(t ++ y))`;
>> e (simp[]);
\end{alltt}
\end{session}

We can prevent the application of this normalising rewrite with the \ml{Excl} form:
\begin{session}
\begin{alltt}
>>_ b();
>> e (simp[Excl "APPEND_ASSOC"]);
\end{alltt}
\end{session}
\index{simplification!removing rewrites|)}

\subsection{Rewriting with the simplifier}
\label{sec:simplifier-rewriting}

Rewriting is the simplifier's ``core operation''.  This section
describes the action of rewriting in more detail.


\subsubsection{Basic rewriting}
\label{sec:basic-rewriting}

Given a rewrite rule of the form \[
\vdash \ell = r
\]
the simplifier will perform a top-down scan of the input term $t$,
looking for \emph{matches}~(see Section~\ref{sec:simp-homatch} below)
of $\ell$ inside $t$.  This match will occur at a sub-term of $t$
(call it $t_0$) and will return an instantiation.  When this
instantiation is applied to the rewrite rule, the result will be a new
equation of the form \[
\vdash t_0 = r'
\]
Because the system then has a theorem expressing an equivalence for
$t_0$ it can create the new equation \[
  \vdash \underbrace{(\dots t_0\dots)}_t = (\dots r' \dots)
\]
The traversal of the term to be simplified is repeated until no
further matches for the simplifier's rewrite rules are found.  The
traversal strategy is
\begin{enumerate}
\item \label{enum:simp-traverse-toplevel}%
  While there are any matches for stored rewrite rules at this level,
  continue to apply them.  The order in which rewrite rules are
  applied can \emph{not} be relied on, except that when a \simpset{}
  includes two rewrites with exactly the same left-hand sides, the
  rewrite added later will get matched in preference.  (This allows a
  certain amount of rewrite-overloading in the construction of
  \simpset{}s.)
%% may not wish to own up to above detail
\item \label{enum:simp-traverse-recurse}%
  Recurse into the term's sub-terms.  The way in which terms are
  traversed at this step can be controlled by \emph{congruence
    rules}~(an advanced feature, see Section~\ref{sec:simp-congruences}
  below)
\item If step~\ref{enum:simp-traverse-recurse} changed the term at
  all, try another phase of rewriting at this level.  If this fails,
  or if there was no change from the traversal of the sub-terms, try
  any embedded decision procedures (see
  Section~\ref{sec:simp-embedding-code}).  If the rewriting phase or
  any of the decision procedures altered the term, return to
  step~\ref{enum:simp-traverse-toplevel}.  Otherwise, finish.
\end{enumerate}

\subsubsection{Conditional rewriting}
\index{simplification!conditional rewriting}

The above description is a slight simplification of the true state of
affairs.  One particularly powerful feature of the simplifier is that
it really uses \emph{conditional} rewrite rules.  These are theorems
of the form
\[
\vdash P \Rightarrow (\ell = r)
\]
When the simplifier finds a match for term $\ell$ during its traversal
of the term, it attempts to discharge the condition $P$.  If the
simplifier can simplify the term $P$ to truth, then the instance of
$\ell$ in the term being traversed can be replaced by the appropriate
instantiation of $r$.

When simplifying $P$ (a term that does not necessarily even occur in
the original), the simplifier may find itself applying another
conditional rewrite rule.  In order to stop excessive recursive
applications, the simplifier keeps track of a stack of all the
side-conditions it is working on.  The simplifier will give up on
side-condition proving if it notices a repetition in this stack.
There is also a user-accessible variable, \ml{Cond\_rewr.stack\_limit}
which specifies the maximum size of stack the simplifier is allowed to
use.

Conditional rewrites can be extremely useful.  For example, theorems
about division and modulus are frequently accompanied by conditions
requiring the divisor to be non-zero.  The simplifier can often
discharge these, particularly if it includes an arithmetic decision
procedure.  For example, the theorem \ml{MOD\_MOD} from the theory
\ml{arithmetic} states
\[
\vdash 0 < n \;\Rightarrow \; (k\,\textsf{MOD}\,n)\,\textsf{MOD}\,n = k
\,\textsf{MOD}\,n
\]
The simplifier (specifically, \ml{SIMP\_CONV~arith\_ss~[MOD\_MOD]})
can use this theorem to simplify the term
\holtxt{(k~MOD~(x~+~1))~MOD~(x~+~1)}: the arithmetic decision
procedure can prove that \holtxt{0 < x + 1}, justifying the rewrite.

Though conditional rewrites are powerful, not every theorem of the
form described above is an appropriate choice.  A badly chosen rewrite
may cause the simplifier's performance to degrade considerably, as it
wastes time attempting to prove impossible side-conditions.  For
example, the simplifier is not very good at finding existential
witnesses.  This means that the conditional rewrite \[
\vdash x < y \land y < z \Rightarrow (x < z = \top)
\]
will not work as one might hope.  In general, the simplifier is not a
good tool for performing transitivity reasoning.  (Try first-order
tools such as \ml{PROVE\_TAC} instead.)

\subsubsection{Generating rewrite rules from theorems}
\label{sec:generating-rewrite-rules}
\index{equational theorems, in HOL logic@equational theorems, in \HOL{} logic!use of in the simplifier}

There are two routes by which a theorem for rewriting can be passed to
the simplifier: either as an explicit argument to one of the \ML{}
functions (\ml{SIMP\_CONV}, \ml{ASM\_SIMP\_TAC} etc.) that take theorem
lists as arguments, or by being included in a \simpset{} fragment
which is merged into a \simpset.  In both cases, these theorems are
transformed before being used.  The transformations applied are
designed to make interactive use as convenient as possible.

In particular, it is not necessary to pass the simplifier theorems
that are exactly of the form
\[
\vdash P \Rightarrow (\ell = r)
\]
Instead, the simplifier will transform its input theorems to extract
rewrites of this form itself.  The exact transformation performed is
dependent on the \simpset{} being used: each \simpset{} contains its
own ``filter'' function which is applied to theorems that are added to
it.  Most \simpset{}s use the filter function from the \ml{pure\_ss}
\simpset{} (see Section~\ref{sec:purebool-ss}).  However, when a
\simpset{} fragment is added to a full simpset, the fragment can
specify an additional filter component.  If specified, this function
is of type \ml{controlled\_thm~->~controlled\_thm~list}, and is applied
to each of the theorems produced by the existing \simpset's filter.
%
\index{simplification!guaranteeing termination}
(A ``controlled'' theorem is one that is accompanied by a piece of
``control'' data expressing the limit (if any) on the number of times
it can be applied.  See Section~\ref{sec:simp-special-rewrite-forms}
for how users can introduce these limits.  The ``control'' type
appears in the \ML{} module \ml{BoundedRewrites}.)

The rewrite-producing filter in \ml{pure\_ss} strips away
conjunctions, implications and universal quantifications until it has
either an equality theorem, or some other boolean form.  For example,
the theorem \ml{ADD\_MODULUS} states
\[
\vdash
\begin{array}[t]{l}
(\forall n\;x.\;\;0 < n \Rightarrow ((x + n)\,\textsf{MOD}\,n =
 x\,\textsf{MOD}\,n)) \;\;\land\\
(\forall n\;x.\;\;0 < n \Rightarrow ((n + x)\,\textsf{MOD}\,n =
 x\,\textsf{MOD}\,n))
\end{array}
\]
This theorem becomes two rewrite rules \[
\begin{array}{l}
\vdash 0 < n \Rightarrow ((x + n)\,\textsf{MOD}\,n = x\,\textsf{MOD}\,n)\\
\vdash 0 < n \Rightarrow ((n + x)\,\textsf{MOD}\,n = x\,\textsf{MOD}\,n)
\end{array}
\]

If looking at an equality where there are variables on the
right-hand side that do not occur on the left-hand side, the
simplifier transforms this to the rule \[
\vdash (\ell = r) = \top
\]
Similarly, if a boolean negation $\neg P$, becomes the rule \[
\vdash P = \bot
\]
and other boolean formulas $P$ become \[
\vdash P = \top
\]

Finally, if looking at an equality whose left-hand side is itself an
equality, and where the right-hand side is not an equality as well,
the simplifier transforms $(x = y) = P$ into the two rules
\[
\begin{array}{l}
\vdash (x = y) = P\\
\vdash (y = x) = P
\end{array}
\]
This is generally useful.  For example, a theorem such as
\[
\vdash \neg(\textsf{SUC}\,n = 0)
\]
will cause the simplifier to rewrite both $(\textsf{SUC}\,n = 0)$ and
$(0 = \textsf{SUC}\,n)$ to false.

The restriction that the right-hand side of such a rule not itself be
an equality is a simple heuristic that prevents some forms of looping.


\subsubsection{Matching rewrite rules}
\label{sec:simp-homatch}

Given a rewrite theorem $\vdash \ell = r$, the first stage of
performing a rewrite is determining whether or not $\ell$ can be
instantiated so as to make it equal to the term that is being
rewritten.  This process is known as matching.  For example, if $\ell$
is the term $\textsf{SUC}(n)$, then matching it against the term
$\textsf{SUC}(3)$ will succeed, and find the instantiation $n\mapsto
3$. In contrast with unification, matching is not symmetrical: a
pattern $\textsf{SUC}(3)$ will not match the term $\textsf{SUC}(n)$.

\index{higher-order matching} \index{matching!higher-order} The
simplifier uses a special form of higher-order matching.  If a pattern
includes a variable of some function type ($f$ say), and that variable
is applied to an argument $a$ that includes no variables except those
that are bound by an abstraction at a higher scope, then the combined
term $f(a)$ will match any term of the appropriate type as long as the
only occurrences of the bound variables in $a$ are in sub-terms
matching $a$.

Assume for the following examples that the variable $x$ is bound at a
higher scope.  Then, if $f(x)$ is to match $x + 4$, the variable $f$
will be instantiated to $(\lambda x.\; x + 4)$.  If $f(x)$ is to match
$3 + z$, then $f$ will be instantiated to $(\lambda x.\;3 + z)$.
Further $f(x + 1)$ matches $x + 1 < 7$, but does not match $x + 2 <
7$.

Higher-order matching of this sort makes it easy to express quantifier
movement results as rewrite rules, and have these rules applied by the
simplifier.  For example, the theorem
\[
\vdash (\exists x. \;P(x)\lor Q(x)) = (\exists x.\;P(x)) \lor (\exists
x.\;Q(x))
\]
has two variables of a function-type ($P$ and $Q$), and both are
applied to the bound variable $x$.  This means that when applied to
the input \[
\exists z. \;z < 4 \lor z + x = 5 * z
\]
the matcher will find the instantiation \[
\begin{array}{l}
P \mapsto (\lambda z.\;z < 4)\\
Q \mapsto (\lambda z.\;z + x = 5 * z)
\end{array}
\]

Performing this instantiation, and then doing some $\beta$-reduction
on the rewrite rule, produces the theorem\[
\vdash (\exists z. \;z < 4 \lor z + x = 5 * z) =
(\exists z. \;z < 4) \lor (\exists z.\;z + x = 5 * z)
\]
as required.

Another example of a rule that the simplifier will use successfully is
\[
\vdash f \circ (\lambda x.\; g(x)) = (\lambda x.\;f(g(x)))
\]
The presence of the abstraction on the left-hand side of the rule
requires an abstraction to appear in the term to be matched, so this
rule can be seen as an implementation of a method to move abstractions
up over function compositions.

An example of a possible left-hand side that will \emph{not} match as
generally as might be liked is $(\exists x.\;P(x + y))$.  This is
because the predicate $P$ is applied to an argument that includes the
free variable $y$.

\subsection{Advanced features}
\label{sec:advanced-simplifier}

This section describes some of the simplifier's advanced features.

\subsubsection{Congruence rules}
\label{sec:simp-congruences}
\index{simplification!congruence rules}
\index{congruence rules!in simplification}
Congruence rules control the way the simplifier traverses a term.
They also provide a mechanism by which additional assumptions can be
added to the simplifier's context, representing information about the
containing context.  The simplest congruence rules are built into the
\ml{pure\_ss} simpset.  They specify how to traverse application and
abstraction terms.  At this fundamental level, these congruence rules
are little more than the rules of inference \ml{ABS}
\[
\frac{\Gamma \turn t_1 = t_2}
{\Gamma \turn (\lquant{x}t_1) = (\lquant{x}t_2)}
\]
(where $x\not\in\Gamma$) and \ml{MK\_COMB}
\[
\frac{\Gamma \turn f = g \qquad \qquad \Delta \turn x = y}
{\Gamma \cup \Delta \turn f(x) = g(y)}
\]
When specifying the action of the simplifier, these rules should be
read upwards.  With \ml{ABS}, for example, the rule says ``when
simplifying an abstraction, simplify the body $t_1$ to some new $t_2$,
and then the result is formed by re-abstracting with the bound
variable~$x$.''

Further congruence rules should be added to the simplifier in the form
of theorems, via the \ml{congs} field of the records passed to the
\ml{SSFRAG} constructor.  Such congruence rules should be of the form
\[
\mathit{cond_1} \Rightarrow \mathit{cond_2} \Rightarrow \dots (E_1 =
E_2)
\]
where $E_1$ is the form to be rewritten.  Each $\mathit{cond}_i$ can
either be an arbitrary boolean formula (in which case it is treated as
a side-condition to be discharged) or an equation of the general form
\[
\forall \vec{v}. \;\mathit{ctxt}_1 \Rightarrow \mathit{ctxt}_2
\Rightarrow \dots (V_1(\vec{v}) = V_2(\vec{v}))
\]
where the variable $V_2$ must occur free in $E_2$.

For example, the theorem form of \ml{MK\_COMB} would be
\[
\vdash (f = g) \Rightarrow (x = y) \Rightarrow (f(x) = g(y))
\]
and the theorem form of \ml{ABS} would be
\[
\vdash (\forall x. \;f (x) = g (x)) \Rightarrow (\lambda x. \;f(x)) = (\lambda
x.\;g(x))
\]
The form for \ml{ABS} demonstrates how it is possible for congruence
rules to handle bound variables.  Because the congruence rules are
matched with the higher-order match of Section~\ref{sec:simp-homatch},
this rule will match all possible abstraction terms.

These simple examples have not yet demonstrated the use of
$\mathit{ctxt}$ conditions on sub-equations.  An example of this is
the congruence rule (found in \ml{CONG\_ss}) for implications.  This
states
\[
\vdash (P = P') \Rightarrow (P' \Rightarrow (Q = Q')) \Rightarrow
(P \Rightarrow Q = P' \Rightarrow Q')
\]
This rule should be read: ``When simplifying $P\Rightarrow Q$, first
simplify $P$ to $P'$.  Then assume $P'$, and simplify $Q$ to $Q'$.
Then the result is $P' \Rightarrow Q'$.''

The rule for conditional expressions is
\[
\vdash \begin{array}[t]{l}
  (P = P') \Rightarrow (P' \Rightarrow (x = x')) \Rightarrow
  (\neg P' \Rightarrow (y = y')) \;\Rightarrow\\
       (\textsf{if}\;P\;\textsf{then}\;x\;\textsf{else}\;y =
       \textsf{if}\;P'\;\textsf{then}\;x'\;\textsf{else}\;y')
\end{array}
\]
This rule allows the guard to be assumed when simplifying the
true-branch of the conditional, and its negation to be assumed when
simplifying the false-branch.

The contextual assumptions from congruence rules are turned into
rewrites using the mechanisms described in
Section~\ref{sec:generating-rewrite-rules}.

Congruence rules can be used to achieve a number of interesting
effects.  For example, a congruence can specify that sub-terms
\emph{not} be simplified if desired.  This might be used to prevent
simplification of the branches of conditional expressions:
\[
\vdash (P = P') \Rightarrow
       (\textsf{if}\;P\;\textsf{then}\;x\;\textsf{else}\;y =
       \textsf{if}\;P'\;\textsf{then}\;x\;\textsf{else}\;y)
\]
If added to the simplifier, this rule will take precedence over any
other rules for conditional expressions (masking the one above from
\ml{CONG\_ss}, say), and will cause the simplifier to only descend
into the guard.  With the standard rewrites (from \ml{BOOL\_ss}):
\[
\begin{array}{l}
\vdash \;\textsf{if}\;\top\;\textsf{then}\;x\;\textsf{else}\;y \,\;=\,\; x\\
\vdash \;\textsf{if}\;\bot\;\textsf{then}\;x\;\textsf{else}\;y \,\;=\,\; y
\end{array}
\]
users can choose to have the simplifier completely ignore
a conditional's branches until that conditional's guard is simplified
to either true or false.


\subsubsection{AC-normalisation}
\index{simplification!AC-normalisation}

The simplifier can be used to normalise terms involving associative
and commutative constants.  This process is known as
\emph{AC-normalisation}.  The simplifier will perform AC-normalisation
for those constants which have their associativity and commutativity
theorems provided in a constituent \simpset{} fragment's \ml{ac}
field.

For example, the following \simpset{} fragment will cause
AC-normalisation of disjunctions
\begin{session}
\begin{alltt}
>>__ load "simpLib";
>>__ open simpLib;
##eval[DISJ_ss] SSFRAG { name = NONE,
           convs = [], rewrs = [], congs = [],
           filter = NONE, ac = [(DISJ_ASSOC, DISJ_COMM)],
           dprocs = [] }
\end{alltt}
\end{session}
The pair of provided theorems must state
\begin{eqnarray*}
x \oplus y &=& y \oplus x\\
x \oplus (y \oplus z) &=& (x \oplus y) \oplus z
\end{eqnarray*}
for a constant $\oplus$.  The theorems may be universally quantified,
and the associativity theorem may be oriented either way.  Further,
either the associativity theorem or the commutativity theorem may be
the first component of the pair.  Assuming the \simpset{} fragment
above is bound to the \ML{} identifier \ml{DISJ\_ss}, its behaviour is
demonstrated in the following example:
\begin{session}
\begin{alltt}
>> SIMP_CONV (bool_ss ++ DISJ_ss) [] ``p /\ q \/ r \/ P z``;
\end{alltt}
\end{session}

\index{arith_ss (simplification set)@\ml{arith\_ss} (simplification set)}
The order of operands in the AC-normal form that the simplifer's
AC-normalisation works toward is unspecified.  However, the normal
form is always right-associated.  Note also that the \ml{arith\_ss}
\simpset, and the \ml{ARITH\_ss} fragment which is its basis, have
their own bespoke normalisation procedures for addition over the
natural numbers.  Mixing AC-normalisation, as described here, with
\ml{arith\_ss} can cause the simplifier to go into an infinite loop.

AC theorems can also be added to \simpset{}s via the theorem-list part
of the tactic and conversion interface, using the special rewrite form
\ml{AC}:
\begin{session}
\begin{alltt}
>> SIMP_CONV bool_ss [AC DISJ_ASSOC DISJ_COMM] ``p /\ q \/ r \/ P z``;
\end{alltt}
\end{session}
See Section~\ref{sec:simp-special-rewrite-forms} for more on special
rewrite forms.

\subsubsection{Embedding code}
\label{sec:simp-embedding-code}

The simplifier features two different ways in which user-code can be
embedded into its traversal and simplification of input terms.  By
embedding their own code, users can customise the behaviour of the
simplifier to a significant extent.

\paragraph{User conversions}
The simpler of the two methods allows the simplifier to include
user-supplied conversions.  These are added to \simpset{}s in the
{convs} field of \simpset{} fragments.  This field takes lists of
values of type
\begin{hol}
\begin{verbatim}
   { name: string,
    trace: int,
      key: (term list * term) option,
     conv: (term list -> term -> thm) -> term list -> term -> thm}
\end{verbatim}
\end{hol}

The \ml{name} and \ml{trace} fields are used when simplifier tracing
is turned on.  If the conversion is applied, and if the simplifier
trace level is greater than or equal to the \ml{trace} field, then a
message about the conversion's application (including its \ml{name})
will be emitted.

The \ml{key} field of the above record is used to specify the
sub-terms to which the conversion should be applied.  If the value is
\ml{NONE}, then the conversion will be tried at every position.
Otherwise, the conversion is applied at term positions matching the
provided pattern.  The first component of the pattern is a list of
variables that should be treated as constants when finding pattern
matches.  The second component is the term pattern itself.  Matching
against this component is \emph{not} done by the higher-order match of
Section~\ref{sec:simp-homatch}, but by a higher-order ``term-net''.
This form of matching does not aim to be precise; it is used to
efficiently eliminate clearly impossible matches.  It does not check
types, and does not check multiple bindings.  This means that the
conversion will not only be applied to terms that are exact matches
for the supplied pattern.

Finally, the conversion itself.  Most uses of this facility are to add
normal \HOL{} conversions (of type \ml{term->thm}), and this can be
done by ignoring the \ml{conv} field's first two parameters.  For a
conversion \ml{myconv}, the standard idiom is to write
\ml{K~(K~myconv)}.  If the user desires, however, their code
\emph{can} refer to the first two parameters.  The second parameter is
the stack of side-conditions that have been attempted so far.  The
first enables the user's code to call back to the simplifier, passing
the stack of side-conditions, and a new side-condition to solve.  The
\ml{term} argument must be of type \holtxt{:bool}, and the recursive
call will simplify it to true (and call \ml{EQT\_ELIM} to turn a term
$t$ into the theorem $\vdash t$).  This restriction is lifted for
decision procedures (see below), but for conversions the recursive call can
\emph{only} be used for side-condition discharge. Note also that it
is the user's responsibility to pass an appropriately updated stack of
side-conditions to the recursive invocation of the simplifier.

A user-supplied conversion should never return the reflexive identity
(an instance of $\vdash t = t$).  This will cause the simplifier to
loop.  Rather than return such a result, raise a \ml{HOL\_ERR} or
\ml{Conv.UNCHANGED} exception.  (Both are treated the same by the simplifier.)



\paragraph{Context-aware decision procedures}
Another, more involved, method for embedding user code into the
simplifier is \emph{via} the \ml{dprocs} field of the \simpset{}
fragment structure.  This method is more general than adding
conversions, and also allows user code to construct and maintain its
own bespoke logical contexts.

The \ml{dprocs} field requires lists of values of the type
\ml{Traverse.reducer}.  These values are constructed with the
constructor \ml{REDUCER}:
\begin{hol}
\begin{verbatim}
   REDUCER : {initial : context,
              addcontext : context * thm list -> context,
              apply : {solver : term list -> term -> thm,
                       conv : term list -> term -> thm,
                       context : context,
                       stack : term list} -> term -> thm}
          -> reducer
\end{verbatim}
\end{hol}
The \ml{context} type is an alias for the built-in \ML{} type
\ml{exn}, that of exceptions.  The exceptions here are used as a
``universal type'', capable of storing data of any type.  For example,
if the desired data is a pair of an integer and a boolean, then the
following declaration could be made:
\begin{hol}
\begin{verbatim}
   exception my_data of int * bool
\end{verbatim}
\end{hol}
It is not necessary to make this declaration visible with a wide
scope.  Indeed, only functions accessing and creating contexts of this
form need to see it. For example:
\begin{hol}
\begin{verbatim}
  fun get_data c = (raise c) handle my_data (i,b) => (i,b)
  fun mk_ctxt (i,b) = my_data(i,b)
\end{verbatim}
\end{hol}

When creating a value of \ml{reducer} type, the user must provide an
initial context, and two functions.  The first, \ml{addcontext}, is
called by the simplifier's traversal mechanism to give every embedded
decision procedure access to theorems representing new context
information.  For example, this function is called with theorems from
the current assumptions in \ml{ASM\_SIMP\_TAC}, and with the theorems
from the theorem-list arguments to all of the various simplification
functions.  As a term is traversed, the congruence rules governing
this traversal may also provide additional theorems; these will also
be passed to the \ml{addcontext} function.  (Of course, it is entirely
up to the \ml{addcontext} function as to how these theorems will be
handled; they may even be ignored entirely.)

When an embedded reducer is applied to a term, the provided \ml{apply}
function is called.  As well as the term to be transformed, the
\ml{apply} function is also passed a record containing a
side-condition solver, a more general call-back to the simplifier,
the decision procedure's current context, and the
stack of side-conditions attempted so far.  The stack and solver are
the same as the additional arguments provided to user-supplied
conversions. The \ml{conv} argument is call-back to the simplifier,
which given a term $t$ returns a theorem of the form $\vdash t = t'$
or fails. In contrast, the \ml{solver} either returns the theorem
$\vdash t$ or fails. The power of the reducer abstraction is having
access to a context that can be built appropriately for each decision
procedure.

Decision procedures are applied last when a term is encountered by the
simplifier.  More, they are applied \emph{after} the simplifier has
already recursed into any sub-terms and tried to do as much rewriting
as possible.  This means that although simplifier rewriting occurs in
a top-down fashion, decision procedures will be applied bottom-up and
only as a last resort.

As with user-conversions, decision procedures must raise an exception
rather than return instances of reflexivity.

\subsubsection{Special rewrite forms}
\label{sec:simp-special-rewrite-forms}

Some of the simplifier's features can be accessed in a relatively
simple way by using \ML{} functions to construct special theorem
forms.  These special theorems can then be passed in the
simplification tactics' theorem-list arguments.

Two of the simplifier's advanced features, AC-normalisation and
congruence rules can be accessed in this way.  Rather than construct a
custom \simpset{} fragment including the required AC or congruence
rules, the user can instead use the functions \ml{AC} or \ml{Cong}:
\begin{hol}
\begin{verbatim}
   AC : thm -> thm -> thm
   Cong : thm -> thm
\end{verbatim}
\end{hol}
For example, if the theorem value
\begin{hol}
\begin{verbatim}
   AC DISJ_ASSOC DISJ_COMM
\end{verbatim}
\end{hol}
appears amongst the theorems passed to a simplification tactic, then
the simplifier will perform AC-normalisation of disjunctions.  The
\ml{Cong} function provides a similar interface for the addition of
new congruence rules.

\index{simplification!guaranteeing termination}
\index{Once (controlling rewrite applications)@\ml{Once} (controlling rewrite applications)|pin}
\index{Ntimes (controlling rewrite applications)@\ml{Ntimes} (controlling rewrite applications)|pin}
Two other functions provide a crude mechanism for controlling the
number of times an individual rewrite will be applied.
\begin{hol}
\begin{verbatim}
   Once : thm -> thm
   Ntimes : thm -> int -> thm
\end{verbatim}
\end{hol}
A theorem ``wrapped'' in the \ml{Once} function will only be applied
once when the simplifier is applied to a given term.  A theorem
wrapped in \ml{Ntimes} will be applied as many times as given in the
integer parameter.

\index{simplification!requiring rewrite application}
Another pair of special forms allow the user to \emph{require} that certain rewrites are applied.
Both forms check the count of instances of rewrite-redexes appearing in the goal that results after simplification has happened.
If the requirement is not satisfied, the relevant tactic fails.
In this context, a rewrite redex is the LHS of a theorem being used as a rewrite, so that, for example, the redex of the theorem $\vdash x + 0 = x$ is $x + 0$.
\index{Req0 (simplification theorem modifier)@\ml{Req0} (simplification theorem modifier)}
The \ml{Req0} form checks that the number of redexes of the corresponding rewrite is zero in the resulting goal.
For unconditional rewrites, such a requirement is usually redundant, but this form can be useful when rewrites are conditional and the simplifier may have failed to discharge side-conditions.
For example:
\begin{session}
\begin{alltt}
>> val th = arithmeticTheory.ZERO_MOD;
>>+ simp[Req0 th] ([], ``0 MOD z``);

>> simp[Req0 th] ([], ``0 MOD (z + 1)``)
    (* succeeds because arithmetic d.p. knows z + 1 is nonzero *);
\end{alltt}
\end{session}

\index{ReqD (simplification theorem modifier)@\ml{ReqD} (simplification theorem modifier)}
The \ml{ReqD} modifier requires that the redex count should have decreased.
This is implicitly a check on the original goal as well: it must have a non-zero count of redexes itself.

Both \ml{Req0} and \ml{ReqD} can be combined with \ml{Once} and \ml{Ntimes}.

\paragraph{Simplifying at particular sub-terms}
\index{simplification!at particular sub-terms}
We have already seen (Section~\ref{sec:simp-congruences} above) that
the simplifier's congruence technology can be used to force the
simplifier to ignore particular terms.  The example in the section
above discussed how a congruence rule might be used to ensure that
only the guards of conditional expressions should be simplified.

In many proofs, it is common to want to rewrite only on one side or
the other of a binary connective (often, this connective is an
equality).  For example, this occurs when rewriting with equations
from complicated recursive definitions that are not just structural
recursions.  In such definitions, the left-hand side of the equation
will have a function symbol attached to a sequence of variables, e.g.:
\begin{hol}
\begin{verbatim}
   |- f x y = ... f (g x y) z ...
\end{verbatim}
\end{hol}
Theorems of a similar shape are also returned as the ``cases''
theorems from inductive definitions.

Whatever their origin, such theorems are the classic example of
something to which one would want to attach the \ml{Once} qualifier.
However, this may not be enough:  one may wish to prove a result such
as
\begin{hol}
\begin{verbatim}
   f (constructor x) y = ... f (h x y) z ...
\end{verbatim}
\end{hol}
(With relations, the goal may often feature an implication instead of
an equality.)  In this situation, one often wants to expand just the
instance of \holtxt{f} on the left, leaving the other occurrence
alone.  Using \ml{Once} will expand only one of them, but without
specifying which one is to be expanded.

The solution to this problem is to use special congruence rules,
constructed as special forms that can be passed as theorems like
\ml{Once}.  The functions
\begin{hol}
\begin{verbatim}
   SimpL : term -> thm
   SimpR : term -> thm
\end{verbatim}
\end{hol}
construct congruence rules to force rewriting to the left or right of
particular terms.  For example, if \holtxt{opn} is a binary operator,
\ml{SimpL~\holquote{(opn)}} returns \ml{Cong} applied to the theorem
\begin{hol}
\begin{verbatim}
   |- (x = x') ==> (opn x y = opn x' y)
\end{verbatim}
\end{hol}
\index{SimpLHS@\ml{SimpLHS}|pin}\index{SimpRHS@\ml{SimpRHS}|pin}
Because the equality case is so common, the special values
\ml{SimpLHS} and \ml{SimpRHS} are provided to force
simplification on the left or right of an equality respectively.
These are just defined to be applications of \ml{SimpL} and \ml{SimpR}
to equality.

Note that these rules apply throughout a term, not just to the
uppermost occurrence of an operator.  Also, the topmost operator in
the term need not be that of the congruence rule.  This behaviour is
an automatic consequence of the implementation in terms of congruence
rules.

\subsubsection{Limiting simplification}
\label{sec:limit-simpl}

\index{simplification!guaranteeing termination}
In addition to the \ml{Once} and \ml{Ntimes} theorem-forms just
discussed, which limit the number of times a particular rewrite is
applied, the simplifier can also be limited in the total number of
rewrites it performs. The \ml{limit} function (in \ml{simpLib} and
\ml{bossLib})
\begin{hol}
\begin{verbatim}
   limit : int -> simpset -> simpset
\end{verbatim}
\end{hol}
records a numeric limit in a \simpset{}.  When a limited \simpset{}
then works over a term, it will never apply more than the given number
of rewrites to that term.  When conditional rewrites are used, the
rewriting done in the discharge of side-conditions counts against the
limit, as long as the rewrite is ultimately applied.  The application
of user-provided congruence rules, user-provided conversions and
decision procedures also all count against the limit.

When the simplifier yields control to a user-provided conversion or
decision procedure it cannot guarantee that these functions will ever
return (and they may also take arbitrarily long to work, often a worry
with arithmetic decision procedures), but use of \ml{limit} is
otherwise a good method for ensuring that simplification terminates.

\subsubsection{Rewriting with arbitrary pre-orders}
\label{sec:preorder-rewriting}
\index{simplification!with pre-orders}

In addition to simplifying with respect to equality, it is also
possible to use the simplifier to ``rewrite'' with respect to a relation
that is reflexive and transitive (a \emph{preorder}).  This can be a
very powerful way of working with transition relations in operational
semantics.

{\newcommand{\bred}{\ensuremath{\rightarrow^*_\beta}}

  Imagine, for example, that one has set up a ``deep embedding'' of the
  $\lambda$-calculus.  This will entail the definition of a new type
  (\texttt{lamterm}, say) within the logic, as well as definitions of
  appropriate functions (\eg, substitution) and relations over
  \texttt{lamterm}.  One is likely to work with the reflexive and
  transitive closure of $\beta$-reduction (\bred).  This relation has
  congruence rules such as
\[
\begin{array}{c@{\qquad\qquad}c}
\infer{M_1 \,N\;\bred\;M_2\,N}{M_1 \;\bred\;M_2} &
\infer{M \,N_1\;\bred\;M\,N_2}{N_1 \;\bred\;N_2}\\[3mm]
\multicolumn{2}{c}{\infer{(\lambda v.M_1)\;\bred\;(\lambda v.M_2)}{M_1\;\bred M_2}}
\end{array}
\] and one important rewrite
\[
\infer{(\lambda v. M)\,N \;\bred\; M[v := N]}{}
\]
Having to apply these rules manually in order to show that a
given starting term can reduce to particular destination is usually
very painful, involving many applications, not only of the theorems
above, but also of the theorems describing reflexive and transitive
closure (see Section~\ref{relation}).

Though the $\lambda$-calculus is non-deterministic, it is also confluent, so
the following theorem holds:
\[
\infer{
  M_1 \;\bred\;N\;\;=\;\;M_2\;\bred\; N
}{
  \beta\textrm{-nf}\;N & M_1 \;\bred\;M_2
}
\]
This is the critical theorem that justifies the switch from rewriting
with equality to rewriting with \bred.  It says that if one has a term
$M_1\bred N$, with $N$ a $\beta$-normal form, and if $M_1$ rewrites to
$M_2$ under \bred, then the original term is equal to $M_2\bred N$.
With luck, $M_2$ will actually be syntactically identical to $N$, and
the reflexivity of \bred{} will prove the desired result.  Theorems
such as these, that justify the switch from one rewriting relation to
another are known as \emph{weakening congruences}.

When adjusted appropriately, the simplifier can be modified to exploit
the five theorems above, and automatically prove results such as
\[
u ((\lambda f\,x. f (f\,x)) v) \bred u (\lambda x. v(v\,x))
\]
(on the assumption that the terms $u$ and $v$ are $\lambda$-calculus
variables, making the result a $\beta$-normal form).

In addition, one will quite probably have various rewrite theorems
that one will want to use in addition to those specified above.  For
example, if one has earlier proved a theorem such as
\[
K\,x\,y \bred x
\]
then the simplifier can take this into account as well.

The function achieving all this is
\index{add_relsimp@\ml{add\_relsimp}}
\begin{verbatim}
   simpLib.add_relsimp  : {trans: thm, refl: thm, weakenings: thm list,
                           subsets: thm list, rewrs : thm list} ->
                          simpset -> simpset
\end{verbatim}
The fields of the record that is the first argument are:
\begin{description}
\item[\texttt{trans}] The theorem stating that the relation is
  transitive, in the form $\forall x y z. R\,x\,y \land R\,y\,z \Rightarrow R x z$.
\item[\texttt{refl}] The theorem stating that the relation is
  reflexive, in the form $\forall x. R\,x\,x$.
\item[\texttt{weakenings}] A list of weakening congruences, of the
  general form $P_1 \Rightarrow P_2 \Rightarrow \cdots (t_1 = t_2)$, where at least one of the
  $P_i$ will presumably mention the new relation $R$ applied to a
  variable that appears in $t_1$.  Other
  antecedents may be side-conditions such as the requirement in the
  example above that the term $N$ be in $\beta$-normal form.
\item[\texttt{subsets}] Theorems of the form $R'\,x\,y \Rightarrow R\,x\,y$.
  These are used to augment the resulting \simpset's ``filter'' so that
  theorems in the context mentioning $R'$ will derive useful rewrites
  involving $R$.  In the example of $\beta$-reduction, one might also have
  a relation $\rightarrow_{wh}^*$ for weak-head reduction.  Any weak-head
  reduction is also a $\beta$-reduction, so it can be useful to have the
  simplifier automatically ``promote'' facts about weak-head reduction
  to facts about $\beta$-reduction, and to then use them as rewrites.
\item[\texttt{rewrs}] Possibly conditional rewrites, presumably mostly
  of the form $P \Rightarrow R\,t_1\,t_2$.  Rewrites over equality can also be
  included here, allowing useful additional facts to be included.  For
  example, when working with the $\lambda$-calculus, one might include both
  the rewrite for $K$ above, as well as the definition of
  substitution.
\end{description}
} % end of block defining \bred

The application of this function to a \simpset{} \texttt{ss} will
produce an augmented \texttt{ss} that has all of \texttt{ss}'s
existing behaviours, as well as the ability to rewrite with the given
relation.


\index{simplification|)}

\section{Efficient Applicative Order Reduction---\texttt{computeLib}}
\label{sec:computeLib}

Section \ref{sec:datatype} and Section \ref{TFL} show the ability of
\HOL{} to represent many of the standard constructs of functional
programming. If one then wants to `run' functional programs on
arguments, there are several choices. First, one could apply the
simplifier, as demonstrated in Section \ref{sec:simpLib}. This allows
all the power of the rewriting process to be brought to bear,
including, for example, the application of decision procedures to
prove constraints on conditional rewrite rules.  Second, one could
write the program, and all the programs it transitively depends on,
out to a file in a suitable concrete syntax, and invoke a compiler or
interpreter. This functionality is available in \HOL{} via use of
\ml{EmitML.exportML}.

Third, \ml{computeLib} can be used. This library supports call-by-value
evaluation of \HOL{} functions by deductive steps. In other words, it
is quite similar to having an \ML{} interpreter inside the \HOL{} logic,
working by forward inference. When used in this way, functional
programs can be executed more quickly than by using the simplifier.

The most accessible entry-points for using the \ml{computeLib} library
are the conversion \ml{EVAL} and its tactic counterpart
\ml{EVAL\_TAC}.  These depend on an internal database that stores
function definitions. In the following example, loading \ml{sortingTheory}
augments this database with relevant definitions, that of Quicksort
(\holtxt{QSORT}) in particular, and then we can evaluate
\holtxt{QSORT} on a concrete list.
%
\setcounter{sessioncount}{0}
\begin{session}
\begin{alltt}
>> load "sortingTheory";

>> EVAL ``QSORT (<=) [76;34;102;3;4]``;
\end{alltt}
\end{session}
Often, the argument to a function has no variables: in that case
application of \ml{EVAL} ought to return a ground result,
as in the above example. However, \ml{EVAL} can also evaluate functions on
arguments with variables---so-called \emph{symbolic} evaluation---and
in that case, the behaviour of \ml{EVAL} depends on the structure of the
recursion equations. For example, in the following session, if there is
sufficient information in the input, symbolic execution can deliver
an interesting result. However, if there is not enough information
in the input to allow the algorithm any traction, no expansion will
take place.
%
\begin{session}
\begin{alltt}
>> EVAL ``REVERSE [u;v;w;x;y;z]``;

>> EVAL ``REVERSE alist``;
\end{alltt}
\end{session}
%

\subsection{Dealing with divergence}

The major difficulty with using \ml{EVAL} is termination. All too
often, symbolic evaluation with \ml{EVAL} will diverge, or generate
enormous terms. The usual cause is conditionals with variables in the
test. For example, the following definition is provably equal to \holtxt{FACT},
%
\begin{session}
\begin{alltt}
>> Define `fact n = if n=0 then 1 else n * fact (n-1)`;
\end{alltt}
\end{session}
%
But the two definitions evaluate completely differently.
%
\begin{session}
\begin{alltt}
>> EVAL ``FACT n``;

> EVAL ``fact n``;
  <.... interrupt key struck ...>
Interrupted.
\end{alltt}
\end{session}
%
The primitive-recursive definition of \holtxt{FACT} does not expand
at all, while the destructor-style recursion of \holtxt{fact} never stops
expanding. A rudimentary monitoring facility shows the behaviour, first
on a ground argument, then on a symbolic argument.
%
\begin{session}
\begin{alltt}
>> val [fact] = decls "fact";
>> computeLib.monitoring := SOME (same_const fact);

>> EVAL ``fact 4``;

> EVAL ``fact n``;
fact n = (if n = 0 then 1 else n * fact (n - 1))
fact (n - 1) = (if n - 1 = 0 then 1 else (n - 1) * fact (n - 1 - 1))
fact (n - 1 - 1) =
(if n - 1 - 1 = 0 then 1 else (n - 1 - 1) * fact (n - 1 - 1 - 1))
fact (n - 1 - 1 - 1) =
(if n - 1 - 1 - 1 = 0 then
   1
 else
   (n - 1 - 1 - 1) * fact (n - 1 - 1 - 1 - 1))
   .
   .
   .
\end{alltt}
\end{session}
%
In each recursive expansion, the test involves a variable, and hence
cannot be reduced to either \holtxt{T} or \holtxt{F}. Thus, expansion
never stops.

Some simple remedies can be adopted in trying to deal with
non-terminating symbolic evaluation.
\begin{itemize}
\item \ml{RESTR\_EVAL\_CONV} behaves like \ml{EVAL} except
  it takes an extra list of constants. During
  evaluation, if one of the supplied constants is encountered, it will
  not be expanded. This allows evaluation down to a specified level,
  and can be used to cut-off some looping evaluations.
\item \ml{set\_skip} can also be used to control
 evaluation. See the \REFERENCE{} entry for \ml{CBV\_CONV} for
 discussion of \ml{set\_skip}.

\end{itemize}

\paragraph{Custom evaluators}

For some problems, it is desirable to construct a customized
evaluator, specialized to a fixed set of definitions. The \ml{compset}
type found in \ml{computeLib} is the type of definition databases. The
functions \ml{new\_compset}, \ml{bool\_compset}, \ml{add\_funs}, and
\ml{add\_convs} provide the standard way to build up such
databases. Another quite useful \holtxt{compset} is
\ml{reduceLib.num\_compset}, which may be used for evaluating
terms with numbers and booleans.  Given a \ml{compset}, the function
\ml{CBV\_CONV} generates an evaluator: it is used to implement \ml{EVAL}.
See \REFERENCE{} for more details.

\paragraph{Dealing with Functions over Peano Numbers}

Functions defined by pattern-matching over Peano-style numbers are not
in the right format for \ml{EVAL}, since the calculations will be
asymptotically inefficient. Instead, the same definition should be
used over numerals, which is a positional notation described in
Section~\ref{sec:numerals}. However, it is preferable for proofs to
work over Peano numbers. In order to bridge this gap, the function
\ml{numLib.SUC\_TO\_NUMERAL\_DEFN\_CONV} is used to convert a function
over Peano numbers to one over numerals, which is the format that
\ml{EVAL} prefers. \ml{Define} will automatically call
\ml{SUC\_TO\_NUMERAL\_DEFN\_CONV} on its result.

\paragraph{Storing definitions}

\HOL{}'s top-level definition facilities (\ie, the \ml{Define} function and the \ml{Definition} syntax) automatically add definitions to the global compset
used by \ml{EVAL} and \ml{EVAL\_TAC}.
However, when \ml{Hol\_defn} is used to define a function, its defining equations are not added to the global compset until \ml{tprove} is used to prove the termination constraints.
Moreover, \ml{tprove} does not add the definition persistently into the global compset.
Therefore, one must use \ml{add\_persistent\_funs} in a theory to be sure that definitions made by \ml{Hol\_defn} are available to \ml{EVAL} in descendant theories.
Another point: one must call \ml{add\_persistent\_funs} before \ml{export\_theory} is called.

Occasionally, one does \emph{not} want a definition automatically added to the global compset.
\index{theorem attributes!nocompute@\ml{nocompute}}
The easiest way to achieve this is to use the \ml{nocompute} ``pseudo-attribute'':
\begin{session}
\begin{alltt}
>> Definition f_def[nocompute]: f x = x + 10
   End
>> EVAL ``f 6``;
\end{alltt}
\end{session}


\section{Arithmetic Libraries---\texttt{numLib}, \texttt{intLib} and \texttt{realLib}}
\label{sec:numLib}
\index{decision procedures!Presburger arithmetic over natural numbers}

Each of the arithmetic libraries of \HOL{} provide a
suite of definitions and theorems as well as automated inference support.

\paragraph{numLib}

The most basic numbers in \HOL{} are the natural numbers. The
\ml{numLib} library encompasses the theories \ml{numTheory},
\ml{prim\_recTheory}, \ml{arithmeticTheory}, and \ml{numeralTheory}.
This library also incorporates an evaluator for numeric expression
from \ml{reduceLib} and a decision procedure for linear arithmetic
\ml{ARITH\_CONV}. The evaluator and the decision procedure are
integrated into the simpset \ml{arith\_ss} used by the simplifier.
As well, the linear arithmetic decision procedure can be directly
invoked through \ml{DECIDE} and \ml{DECIDE\_TAC}, both found in
\ml{bossLib}.


\index{decision procedures!Presburger arithmetic over integers}
\paragraph{intLib}

The \ml{intLib} library comprises \ml{integerTheory}, an extensive
theory of the integers, plus two decision procedures
for full Presburger arithmetic. These are available as
\ml{intLib.COOPER\_CONV} and \ml{intLib.ARITH\_CONV}. These
decision procedures are able to deal with linear arithmetic
over the integers and the natural numbers, as well as dealing
with arbitrary alternation of quantifiers.  The \ml{ARITH\_CONV}
procedure is an implementation of the Omega Test, and seems to
generally perform better than Cooper's algorithm.  There are problems
for which this is not true however, so it is useful to have both
procedures available.

\paragraph{realLib}

The \ml{realLib} library provides a foundational development
of the real numbers and analysis. See Section \ref{reals}
for a quick description of the theories.
Also provided is a theory of polynomials, in \theoryimp{polyTheory}.
A decision procedure for linear arithmetic on the real numbers
is also provided by \ml{realLib}, under the name \ml{REAL\_ARITH\_CONV}
and \ml{REAL\_ARITH\_TAC}.

\section{Pattern Matches Library---\texttt{patternMatchesLib}}\label{sec:PatternMatchesLib}
\input{PatternMatchesLib.tex}


\section{Bit Vector Library---\texttt{wordsLib}}

The library \theoryimp{wordsLib} provides tool support for bit-vectors, this includes facilities for: evaluation, parsing, pretty-printing and simplification.

\subsection{Evaluation}

The library \theoryimp{wordsLib} should be loaded when evaluating ground bit-vector terms.  This library provides a \emph{compset} \ml{words\_compset}, which
can be used in the construction of custom \emph{compsets} and conversions.
\setcounter{sessioncount}{0}
\begin{session}
\begin{alltt}
>> load "wordsLib";

>> EVAL ``8w + 9w:word4``;
\end{alltt}
\end{session}
Note that a type annotation is used here to designate the word size.  When the word size is represented by a type variable (\ie{} for arbitrary length words), evaluation may give partial or unsatisfactory results.

\subsection{Parsing and pretty-printing}

Words can be parsed in binary, decimal and hexadecimal.   For example:
\begin{session}
\begin{alltt}
>> ``0b111010w : word8``;

>> ``0x3Aw : word8``;
\end{alltt}
\end{session}
It is possible to parse octal numbers, but this must be enabled first by setting the reference \ml{base\_tokens.allow\_octal\_input} to true.  For example:
\begin{session}
\begin{alltt}
>> ``072w : word8``;

>> base_tokens.allow_octal_input:=true;

>> ``072w : word8``;
\end{alltt}
\end{session}

Words can be pretty-printed using the standard number bases. For example, the function
\ml{wordsLib.output\_words\_as\_bin} will select binary format:
\begin{session}
\begin{alltt}
>> wordsLib.output_words_as_bin();

>> EVAL ``($FCP ODD):word16``;
\end{alltt}
\end{session}
The function \ml{output\_words\_as} is more flexible and allows the number base to vary depending on
the word length and numeric value.  The default pretty-printer (installed when loading \theoryimp{wordsLib}) prints small values in decimal and large values in hexadecimal.
The function \ml{output\_words\_as\_oct} will automatically enable the parsing of octal numbers.

The trace variable \ml{"word printing"} provides an alternative method for changing the output number base --- it is particularly suited to temporarily selecting a number base, for example:
\begin{session}
\begin{alltt}
>> Feedback.trace ("word printing", 1) Parse.term_to_string ``32w``;
\end{alltt}
\end{session}
The choices are as follows: 0 (default) -- small numbers decimal, large numbers hexadecimal; 1 -- binary; 2 -- octal; 3 -- decimal; and 4 -- hexadecimal.

\subsubsection{Types}

You may have noticed that \ty{:word4} and \ty{:word8} have been used as convenient parsing abbreviations for \ty{:\bool[4]} and \ty{:\bool[8]} --- this facility is available for many standard word sizes.  Users wishing to use this notation for non-standard word sizes can use the function \ml{wordsLib.mk\_word\_size}:
\begin{session}
\begin{alltt}
>>+ Lib.try Parse.Type `:word15` handle _ => bool;

>> wordsLib.mk_word_size 15;

>> ``:word15``;
\end{alltt}
\end{session}

\subsubsection{Operator overloading}

The symbols for the standard arithmetic operations (addition, subtraction and multiplication) are overloaded with operators from other standard theories, \ie{} for the natural, integer, rational and real numbers.  In many cases type inference will resolve overloading, however, in some cases this is not possible.  The choice of operator will then depend upon the order in which theories are loaded.  To change this behaviour the functions \ml{wordsLib.deprecate\_word} and \ml{wordsLib.prefer\_word} are provided.  For example, in the following session, the selection of word operators is deprecated:
\begin{session}
\begin{alltt}
>> type_of ``a + b``;

>> wordsLib.deprecate_word();

>> type_of ``a + b``;
\end{alltt}
\end{session}
In the above, natural number addition is chosen in preference to word addition.  Conversely, words are preferred over the integers below:
\begin{session}
\begin{alltt}
>>_ load "intLib";
>>__ temp_overload_on("+", ``int_add``);

>> type_of ``a + b``;

>> wordsLib.prefer_word();
>> type_of ``a + b``;
\end{alltt}
\end{session}
Of course, type annotations could have been added to avoid this problem entirely.

\subsubsection{Guessing word lengths}

It can be a nuisance to add type annotations when specifying the return type for operations such as: \holtxt{word\_extract}, \holtxt{word\_concat}, \holtxt{concat\_word\_list} and \holtxt{word\_replicate}.  This is because there is often a ``standard'' length that could be guessed, \eg{} concatenation usually sums the constituent word lengths.  A facility for word length guessing is controlled by the reference \ml{wordsLib.guessing\_word\_lengths}, which is false by default.  The guesses are made during a post-processing step that occurs after the application of \ml{Parse.Term}.  This is demonstrated below.
\begin{session}
\begin{alltt}
>> wordsLib.guessing_word_lengths:=true;

>> ``concat_word_list [(4 >< 1) (w:word32); w2; w3]``;
\end{alltt}
\end{session}
In the example above, word length guessing is turned on.  Two guesses are made: the extraction is expected to give a four bit word, and the concatenation gives a twelve bit word ($3 \times 4$).  If non-standard numeric lengths are required then type annotations can be added to avoid guesses being made.  With guessing turned off the result types would remain as invented type variables, \ie{} as alpha and beta above.

\subsection{Simplification and conversions}

The following \emph{simpset} fragments are provided:
\begin{description}
\item[\ml{SIZES\_ss}] evaluates a group of functions that operate over numeric types, such as \holtxt{dimindex} and \holtxt{dimword}.
\item[\ml{BIT\_ss}] tries to simplify occurrences of the function \holtxt{BIT}.
\item[\ml{WORD\_LOGIC\_ss}] simplifies bitwise logic operations.
\item[\ml{WORD\_ARITH\_ss}] simplifies word arithmetic operations.  Subtraction is replaced with multiplication by -1.
\item[\ml{WORD\_SHIFT\_ss}] simplifies shift operations.
\item[\ml{WORD\_ss}] contains all of the above fragments, and also does some extra ground term evaluation.  This fragment is added to \ml{srw\_ss}.
\item[\ml{WORD\_ARITH\_EQ\_ss}] simplifies \holtxt{``a = b``} to \holtxt{``a - b = 0w``}.
\item[\ml{WORD\_BIT\_EQ\_ss}] aggressively expands non-arithmetic bit-vector operations into Boolean expressions.  (Should be used with care -- it includes \ml{fcpLib.FCP\_ss}.)
\item[\ml{WORD\_EXTRACT\_ss}] simplification for a variety of operations: word-to-word conversions; concatenation; shifts and bit-field extraction.  Can be used in situations where \ml{WORD\_BIT\_EQ\_ss} is unsuitable.
\item[\ml{WORD\_MUL\_LSL\_ss}] simplifies multiplication by a word literal into a sum of partial products.
\end{description}
Many of these \emph{simpset} fragments have corresponding conversions.  For example, the conversion \ml{WORD\_ARITH\_CONV} is based on \ml{WORD\_ARITH\_EQ\_ss}, however, it does some extra work to ensure that \holtxt{``a = b``} and \holtxt{``b = a``} convert into the same expression.  Therefore, this conversion is suited to reasoning about the equality of arithmetic word expressions.

The behaviour of the fragments listed above are demonstrated using the following function:
\begin{session}
\begin{alltt}
>> fun conv ss = SIMP_CONV (pure_ss++ss) [];
\end{alltt}
\end{session}
The following session demonstrates \ml{SIZES\_ss}:
\begin{session}
\begin{alltt}
>> conv wordsLib.SIZES_ss ``dimindex(:12)``;

>> conv wordsLib.SIZES_ss ``FINITE univ(:32)``;
\end{alltt}
\end{session}
The fragment \ml{BIT\_ss} converts \holtxt{BIT} into membership test over a set of (high) bit positions:
\begin{session}
\begin{alltt}
>> conv wordsLib.BIT_ss ``BIT 3 5``;

>> conv wordsLib.BIT_ss ``BIT i 123``;
\end{alltt}
\end{session}
This simplification provides some support for reasoning about bitwise operations over arbitrary word lengths.  The arithmetic, logic and shift fragments help tidy up basic word expressions:
\begin{session}
\begin{alltt}
>> conv wordsLib.WORD_LOGIC_ss ``a && 12w || 11w && a``;

>> conv wordsLib.WORD_ARITH_ss ``3w * b + a + 2w * b - a * 4w:word2``;

>> conv wordsLib.WORD_SHIFT_ss ``0w << 12 + a >>> 0 + b << 2 << 3``;
\end{alltt}
\end{session}

The remaining fragments are not included in \ml{wordsLib.WORD\_ss} or \ml{srw\_ss}.  The bit equality fragment is demonstrated below.
\begin{session}
\begin{alltt}
>> SIMP_CONV (std_ss++wordsLib.WORD_BIT_EQ_ss) [] ``a && b = ~0w : word2``;
\end{alltt}
\end{session}
The extract fragment is useful for reasoning about bit-field operations and is best used in combination with \ml{wordsLib.SIZES\_ss} or \ml{wordsLib.WORD\_ss}, for example:
\begin{session}
\begin{alltt}
>> SIMP_CONV (std_ss++wordsLib.SIZES_ss++wordsLib.WORD_EXTRACT_ss) []
     ``(4 -- 1) ((a:word3) @@ (b:word2)) : word5``;
\end{alltt}
\end{session}
Finally, the fragment \ml{WORD\_MUL\_LSL\_ss} is demonstrated below.
\begin{session}
\begin{alltt}
>> conv wordsLib.WORD_MUL_LSL_ss ``5w * a : word8``;
\end{alltt}
\end{session}
Rewriting with the theorem \ml{wordsTheory.WORD\_MUL\_LSL} provides an means to undo this simplification, for example:
\begin{session}
\begin{alltt}
>> SIMP_CONV (std_ss++wordsLib.WORD_ARITH_ss) [wordsTheory.WORD_MUL_LSL]
     ``a << 2 + a : word8``;
\end{alltt}
\end{session}
Obviously, without adding safeguards, this rewrite theorem cannot be deployed when used in combination with the \ml{WORD\_MUL\_LSL\_ss} fragment.

\subsubsection{Decision procedures}

A decision procedure for words is provided in the form of
\ml{blastLib.BBLAST\_PROVE}.  This procedure uses \emph{bit-blasting} ---
converting word expressions into propositions and then using a SAT solver to
decide the goal.\footnote{This approach enables counter-examples to be given
when a goal's negation is satisfiable.} This approach is reasonably general and
can tackle a wide range of bit-vector problems.  However, there are some
limitations: the approach only works for constant word lengths, linear
arithmetic (multiplication by literals) and for shifts and bit-field
extractions with respect to literal values.  Also note that some problems will
be potentially slow to prove, e.g.\ when word sizes are large and/or when
there are many nested additions (perhaps through multiplication).

The following examples show \ml{BBLAST\_PROVE} in use:
\begin{session}
\begin{alltt}
>>_ load "blastLib";
>> blastLib.BBLAST_PROVE ``a + 2w <+ 4w <=> a <+ 2w \/ 13w <+ a :word4``;

>> blastLib.BBLAST_PROVE ``w2w (a:word8) <+ 256w : word16``;
\end{alltt}
\end{session}
The decision procedure \ml{BBLAST\_PROVE} is based on the conversion
\ml{BBLAST\_CONV}. This conversion can be used to convert bit-vector problems
into a propositional form; for example:
\begin{session}
\begin{alltt}
>> blastLib.BBLAST_CONV ``(((a : word16) + 5w) << 3) ' 5``;
\end{alltt}
\end{session}
There are also bit-blasting tactics: \ml{BBLAST\_TAC} and \ml{FULL\_BBLAST\_TAC}; with only the latter making use of goal assumptions.

\section{The \texttt{HolSat} Library}\label{sec:HolSatLib}
\input{HolSat.tex}


\section{The \texttt{HolQbf} Library}\label{sec:HolQbfLib}
\input{HolQbf.tex}


\section{The \texttt{HolSmt} library}\label{sec:HolSmtLib}
\input{HolSmt.tex}

\section{The \texttt{Quantifier Heuristics} library}\label{sec:QuantHeuristicsLib}
\input{QuantHeuristics.tex}

\section{Tree-Structured Finite Sets and Finite Maps}\label{sec:enumfset}
\input{enumfset.tex}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "description"
%%% End:
